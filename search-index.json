{
  "210": "\n\nprocessing unit (cpu)\ncontrol unit\nmemory\ninput/output\nregisters\nalu\nmultiplexers\ngeneral-purpose\nsr1, sr2\ndr\nstore intermediate data and results during \ncomputation.\n\n\nsource registers providing operands for the alu \noperations.\ndestination register to store the result from the alu\nperforms arithmetic operations (addition, subtraction) and logic operations (and, or, not).\nsr2mux\nmarmux\npcmux\nselects between a register value (sr2) and an \nimmediate value (from sext).\nselects the source for the memory address register \n(mar).\nselects the source for the next pc value (pc + 1, alu \noutput, etc.).\npc\nir\nld.pc, ld.ir, ld.reg, ld.cc, \nld.mar, ld.mdr\ncondition codes (n, z, p)\nholds the address of the next instruction to be executed.\nholds the current instruction fetched from memory.\ncontrol signals for loading values into respective registers.\nflags indicating the result of the last operation (negative, zero, positive).\nmar\nmdr\nmemory\nholds the address in memory to read from or write to.\nholds the data to be written to or read from memory.\nstorage for instructions and data. controlled by signals like mem.en (memory enable) and \nw.r (write/read).\ninterfaces for external devices to input data to or output data from the cpu.\n\nadd r6, r3, r7add (+) whatever is in r3 with whatever is in r7 and store this in r6\nand r4, r0, r5bitwise and () whatever is in r0 and r5, store the result in r4\nadd r6, r3, #-1add (+) whatever is in r3 with #number, store the result in r6\nand r4, r0, #3bitwise and () whatever is in r0 with #number, store the result in r4\nnot r0, r2bitwise not whatever is in r2, store the result in r0\nld r1, #2pc + #number = address, load content of address in r1\nst r1, #15content of address in r1 is stored in address pc + #number\nldi r1, #13pc + #number = address, read whatever is stored in this address, and use this as the address to load the data in r1.\nsti r2, #13whatever is stored in r2, use this address to access the next address. store this in pc + #number\nldr r4, r1, #3r1 +  #3 to get an address. content of this address will be loaded into r4.\nstr r2, r1, #3get whatever is in r2, and store this in r1 + #3\nlea r1, #9get pc + #9, store in r1\nbrz x0d9get pc + x0d9 = x4104. if z set to 1, then pc=x4104, else, pc=pc\n\nmcq 2\n\nx3000\nx3001\nx3002\nx3003\nx3004\nx3005\nx3006\nx3007\nx3008\nx3009\nx300a\nx300b\nx300c\nx300d\nx300e\nx300f\nx3011\nx3012\nx3013\nx3014\nx3015\nx3016\nx3017\nx3018\nx3019\nx301a\nx301b\n...\nx3020\nx3021\nx3022\n...\nwhat should be the value of x3012 be if the following program is executed \nprogram:\n.origx3000 ; the first instruction is at x3000\nldr1, #2address = 3001+2 = x3003:x340fr1=x340f\nldr2, #0address = 3002+0 = x3002:x320fr2=x320f\nstr1, #15address = 3003+f = x3012x3012: x340f\nstr2, #15address = 3004+f = x3013x3013: x320f\ninitial memory status:\naddress: content\nx3000: x2202\nx3001: x2400\nx3002: x320f\nx3003: x340f\nstarting from x3004, all of them are x0000\n\nwhat is the machine code of the instruction \"ld r1, 9\"?\n0010 0010 0000 1001 = x2209\nwhat is the machine code of the instruction \"add r3, r3, r2\" in hexadecimal?\n0001 0110 1100 0010 = x16c2\nwhat is the effect of executing instruction 0101100101101111? it should be assumed that the instruction is at address x300c.\n0101 100 101 1 01111 = and r4 r5 #15\nr5 = x5678 and x000f = x0008. the value in r4 is set to x0008\nwhat is the operation of the ld instruction?\ndr = m[pc + sext(pcoffset9)]\n\nwhat should be the value of r1 if the following program is executed (based on the initial register status)?\nprogram:\nand r3, r3, #0r3 = x0012 and x0000 r3 = x0000\nnot r0, r2r2 = x0003 notr0 = xfffc\nadd r1, r0, r0r0 = xfff8 add xfff8r1 = xfff8\nand r3, r3, r2r2 = x0003 and x0000r3 = x0000\nadd r1, r1, #-1r1 = xfff8 - x0001r1 = fff7\n\ninitial registers status:\nr0: x0028, xfffc\nr1: x0006, fff8, fff7\nr2: x0003\nr3: x0012, x0000, x0000\n\nwhat should be the value at register r2 if the following program is executed (based on the initial memory status)?\nprogram:\n.origx3000 ; the first instruction is at x3000\nldr1, #2address = 3001+2 = x3003:x340fr1 = x340f\nldr2, #0address = 3002+0 = x3002:x320fr2 = x320f\nstr1, #15address = 3003+15 = x3012x3012: x340f\nstr2, #15address = 3004+15 = x3013x3013: x320f\nldir1, #13address = 3005+13 = x3012:x340fr1 = x0000\nldir2, #13address = 3006+13 = x3013:x320fr2 = x0000\ninitial memory status:\naddress: content\nx3000: x2202\nx3001: x2400\nx3002: x320f\nx3003: x340f\nx3004: xa20d\nx3005: xa40d\nstarting from x3006, all of them are x0000\nld: pc + #number = address, load content of address in r1\nst: content of address in r1 is stored in address pc + #number\nldi: pc + #number = address, read whatever is stored in this address, and use this as the address to load the data in r1.\nsti: whatever is stored in r2, use this address to access the next address. store this in pc + #number\n\nmcq 3\n\nwhat should be the value of r3 if the following program is executed (based on the initial register status)?\n\nprogram:\n.orig x3000 ; the first instruction is at x3000\nand r1, r1, #0r1 = x0000\nand r0, r0, #0r0 = x0000 \nand r2, r2, #0r2 = x0000\nlea r4, #5address = 3004 + 5 = x3009r1 = x3009\nlea r0, #-4address = 3005 - 4 = x3001r0 = x3001\nlea r2, #8address = 3006 + 8 = x300er2 = x300e\nlea r3, #-9address = 3007 - 9 = x2ffer3 = x2ffe\nlea r1, #9address = 3008 + 9 = x3011r1 = x3011\ninitial registers status:\nr0: x0000, x0000, x3001\nr1: x0000, x0000, x3009, x3011\nr2: x0000, x0000, x300e\nr3: x0000, x30f8\nr4: x0000\n\nwhich one of the following statements is correct regarding the execution of instruction 1100000101000000? it should be assumed that the \ninstruction is at address x3008.\n1100 000 101 000000 = jmp r5 (r5 is at x3001), the address of the instruction that will be executed after this instruction is x3001.\nlea: get pc + #9, store in r1\nldi: pc + #number = address, read whatever is stored in this address, and use this as the address to load the data in r1.\n\nfor the program above, which one of the following statements is correct?\n\"ldi r3, c\"\n\nx3008: x1234\nx3009: x2345\nx300a: \nx300b\nx300c: x3008\nx300d: x300b\n\naddress of 'c' = x300c, so x300c stores x3008, which stores x1234. hence x1234 is stored \ninto r3.\n\ninstruction \"ldi r3, c\" sets the value of r3 to x1234.\nwhat should be the value of r3 if the following program is executed?\n\nprogram:\n.orig x3000\nx3000lea r1, inte; r1 is the location of the integer\nx3001and r2, r2, 0; r2 is the count of the integers\nx3002add r2, r2, 3\nx3003loopadd r4, r2, 0\nx3004str r4, r1, 0\nx3005add r1, r1, 1\nx3006add r2, r2, -1\nx3007brp loop\nx3008lea r1, inte; r1 is the location of the integer\nx3009and r3, r3, 0; r3 is the sum\nx300aand r2, r2, 0; r2 is the count of the integers\nx300badd r2, r2, 3\nx300cloop1ldr r4, r1, 0\nx300dadd r3, r3, r4\nx300eadd r1, r1, 1\nx300fadd r2, r2, -1\nx30aabrp loop1\nx3012exit trap 37 ; halt\nx3013inte.blkw 12\n.end\n\nr1: x3016\nr2: x0000\nr3: x0006\nr4: x0001\n\nx3013: x0003\nx3014: x0002\nx3015: x0001\nldr r4, r1, #3r1 +  #3 to get an address. content of this address will be loaded into r4.\nstr r2, r1, #3get whatever is in r2, and store this in r1 + #3\nwhat is the machine code of the instruction \"brnzp loop\" according to the following program?\n\nprogram:\n.orig x3000\nlea r1, inte; r1 is the location of the integer\nand r3, r3, 0; r3 is the sum\nand r2, r2, 0; r2 is the count of the integers\nadd r2, r2, 12\nloopbrz exit\nldr r4, r1, 0\nadd r3, r3, r4\nadd r1, r1, 1\nadd r2, r2, -1\nbrnzploop\nexit trap x25 ; halt\ninte.blkw 12\n.end\n\nbrnzp is at x3009, so pc = x300a\nloop is at x3004\nso offset value = x300a - x3004 = #-6 (b1010)\n0000 1111 1111 1010 = x0ffa\n\nwhat is the condition code after the execution of \"not r4, r2\" based on the following program?\n\nprogram:\n.orig x3000\nld r3, v3 ; load v3 to r3\nld r2, v2 ; load v2 to r2\nld r1, v1; load v1 to r1\nld r0, v0 ; load v0 to r0\nadd r4, r1, r3 \nnot r4, r2 1111 1111 1111 1110 → 0000 0000 0000 0001 → x0001 (positive)\nand r4, r2, r1\nldr r4, r0, 5\nexit trap x25 ; halt\nv0 .fill x3000 ; m[v0] = x3000\nv1 .fill x0001 ; m[v1] = 1\nv2 .fill xfffe ; m[v2] = -2\nv3 .fill xffff ; m[v3] = -1\n.end\n\nr0: x3000\nr1: x0001\nr2: xfffe\nr3: xffff\nr4: x0001\n\nwhat is the machine code of the instruction \"lea r0, -8\"?\n1110 0001 1111 1000 = xe1f8\n\nwhat should be the value of r1 if the following program is executed (based on the initial register \nstatus)?\n\nprogram:\n.origx3000 ; the first instruction is at x3000\nand r0, r0, #0\nadd r2, r0, #2\nadd r3, r0, #3\nlea r1, #9pc = x3004 + #9 = x300d\nstr r3, r1, #3x300d + 3 = x3010\nstr r2, r1, #4x300d + 4 = x3011\nldr r2, r1, #3x3010: x0003 into r2\nldr r3, r1, #4x3011: x0002 into r3\ninitial registers status:\nr0: x0000\nr1: x0000\nr2: x0000\nr3: x0000\nr4: x0000\n\nr0: x0000\nr1: x300d\nr2: x0003\nr3: x0002\n\nx3010: x0003\nx3011: x0002\n\nmcq 4\n\nwhat will be printed out after the execution of the following program if the values of \"vala\" (mem[vala]) and \"valb\" \n(mem[valb]) are set to x0004 and 0x0005 respectively at the beginning?\nprogram:\n.orig x3000\nand r0, r0, #0\nld r1, vala\nld r2, valb\nnot r2, r21111 1111 1111 1010 = xfffa\nadd r2, r2, #1\nadd r3, r1, r2x0004 + fffb = xffff\nbrn bigb ; if valb > vala\nld r0, valb\nbrnzp disp\nbigbld r0, vala\ndispld r1, out0\nadd r0, r0, r1\nout\nexithalt\nvala.blkw1x0004\nvalb.blkw1x0005\nout0.fillx30 ;ascii of '0'\n.end\nr0: '4'\nr1: '0'\nr2: x0005, xfffa, xfffb\nr3: xffff\noutput: 4\nwhat is the output of the program if '1' is inputted? given that the ascii code of 'a' is x61 and '0' is x30.\nprogram:\n.orig x3000\nld r2, term ;\nld r3, ascii ; load ascii difference\nagain trap x23 ; input character\nadd r1, r2, r0 ; test for terminate\nbrz exit ; exit if done\nadd r0, r0, r3\ntrap x21 ; output to monitor...\nbrnzp again\nterm .fill xffc9\nascii .fill x0031\nexit trap x25 ; halt\n.end\nr0: x0061\nr1: xfffa\nr2: xffc9\nr3: x0031\noutput: b\nwhat is the output of the program if ‘z’ is inputted? given that the ascii code of 'a' is x61 and 'a' is x41.\nprogram:\n.orig x3000\nld r2, term\nld r3, ascii ; load ascii difference\nagain trap x23 ; input character\nadd r1, r2, r0 ; test for terminate\nbrz exit ; exit if done\nadd r0, r0, r3\ntrap x21 ; output to monitor...\nbrnzp again\nterm .fill xffc9\nascii .fill x0020\nexit trap x25 ; halt\n.end\n\nr0: z (x005a), x007a\nr1: x0023\nr2: xffc9\nr3: x0020\noutput = x007a (z)\n\nwhat will be printed out after the execution of the following program?\nprogram:\n.orig x3000\nlea r1, hello; r1 points to the character\nld r0, val\nadd r0, r1, r0;\nldr r0, r0, #0; r0 holds the character\ntrap x21; or just out prints r0[7:0]\ntrap x25; or halt\nhello.stringz \"hello world\"\nval.fill #7\n.end\nr1: 'o'\nr0: x0007\n\nwhat is the value of r3 after the execution of the following program if the value of \"num\" (mem[num]) is set to x0002 at the beginning?\nprogram:\n.origx3000\nld r1, six\nld r2, num\nand r3, r3, #0\n; the inner loop\n;\nagainadd r3, r3, r2\nadd r1, r1, #-1\nbrzp again\n;\nhalt\n;\nnum.blkw #1 x0002\nsix.fill x0006\n.end\nr1: xffff\nr2: x0002\nr3: x000e\nr3 holds x000e.\nfor the program above, which one of the statements below is correct?\n\na.the value stored at memory location with label a is the ascii code of \nthe string \"xyz\".\nthe memory location a points to the first letter in the string 'x' represented by its ascii \nvalue: x0058.\nb.the value stored at memory location with label b must be 0.\nb reserves five blank memory locations all set to x0000\nc.the value stored at memory location with label b is 5.\nb reserves five blank memory locations all set to x0000\nd.the value stored at memory location with label a is the ascii code of \ncharacter \"x\".\nthe memory location a points to the first letter in the string 'x' represented by its ascii \nvalue: x0058.\n\nmcq 5\n\nwhat is the value of the register r3 after the execution of the following program when \n\"valn\" is 5?\n\nprogram:\n.orig x3000\nld r2, valn\nadd  r4, r2, #0\nadd  r1, r2, #-1\n; outer loop\noloopand r3, r3, #0\n; inner loop\niloopadd r3, r3, r4\nadd r1, r1, #-1\nbrp iloop\n; end of inner loop\nadd r4, r3, #0\nadd r2, r2, #-1\nadd r1, r2, #-1\nbrp oloop\n; end of outer loop\nhalt\nvaln.blkw 1 #5\n.end\nr1: x0000\nr2: x0001\nr3: x0078\nr4: x0078\nat the end of the program, r3 = x0078\n\nfor the program above, what is the machine code of instruction \"ldr r0, r2, #2\"?\n0110 0000 1000 0010 = x6082\nwhat is the machine code of the instruction \"brz \nfinish\"?\nprogram:\n.orig x3000\nlea r1, hello\nloop ldr r0, r1, #0\nbrz finish\ntrap x21\nadd r1, r1, #1\nbrnzp loop\nfinishtrap x25\nhello.stringz \"hello world\"\n.end\n\nbrz = 0000 010\npc = x3003, finish = x3006, offset = 3\n0000 0100 0000 0011 = x0403\n\n\nwhat is the value of r3 after the execution of the \nfollowing program if the values of \"vala\" (mem[vala]) \nand \"valb\" (mem[valb]) are set to x0000 and 0x0005 \nrespectively at the beginning?\nprogram:\n.orig x3000\nand r3, r3, #0\nld r2, valb\nld r1, vala\n; loop\nagainbrnz exit\nadd r3, r3, r2\nadd r1, r1, #-1 \nbrnzp again\n;\nexithalt\nvala.blkw 1 x0000\nvalb.blkw 1; x0005\n.end\nr3: x0000\nr2: x0005\nr1: x0000\n\nthe value stored in r3 will be x000 after the execution of \nthis program.\nthe hexadecimal representation of a machine instruction is x5543. the address \nof the instruction is x4000. which one of the assembly instructions below \ncorresponds to the machine instruction?\na. and r2, r5, r3\nb. and r2, r5, #3\nc. add r2, r5, #3\nd. add r2, r5, r3\ne. and r2, r5, r4\n\nx5543 = 0101 010 101 000 011 = and r2, r5, r3\nwhat is the value passed to the os stack during the execution of the instruction \n\"out\" based on the following program?\nprogram:\n\n.orig x3000 \nld r2, term\nld r3, ascii\nagaintrap x20\nadd r1, r2, r0\nbrz exit\nout\nadd r0, r0, r3\ntrap x21\nbrnzp again\nterm.fill xffc9\nascii.fill x0020\nexittrap x25; halt\n.end\n\nr0: x3003\nr2: xffc9\nr3: x0020\n\nos stack = [x3003]\n\nwhen a trap is called, the incremented pc is put onto the os stack. then the pc will \nperform the trap. when it is done, it pops off the value on the os stack and puts that into \nthe pc to get back to the original program. this is stored in r0.\n\ntrap and out both use the os stack.\n\nmcq 7\n\nd = 3 (offset -1)\nc = 2 (offset 0)\ndynamic link\nret address\nret value\nb = -1 (offset 4)\na = 0 (offset 5)\nr5\nr6\nwhich one of the following lc-3 instruction sequences will be generated when \nconverting statement “int a=0;” to lc-3 assembly language instructions?\n\nand rg, rg, 0\nstr rg, rg, a\n\nand rl, r0, 0\nstr rl, rl, a\n\nand r0, r0, 0\nstr r0, rl, a\n\nand r0, r0, 0\nstr r0, rg, a\nwhich one of the following lc-3 instruction sequences will be generated when \nconverting statement “int c=2;” to lc-3 assembly language instructions?\n\nand r0, r0, 0\nadd r0, r1, 2\nstr r0, rl, c\n\nand r0, r0, 0\nadd r0, r0, 2\nstr r0, rg, c\n\nand r0, r1, 0\nadd r0, r0, 2\nstr r0, rl, c\n\nstr #2, rl, c\nwhich one of the following lc-3 instruction sequences will be generated when \nconverting statement “int d=3;” to lc-3 assembly language instructions?\n\nand r0, r0, 0\nadd r0, r0, 3\nstr r0, rg, d\n\nadd r0, r0, 3\nstr r0, rl, d\n\nstr #3, rl, d\n\nand r0, r0, 0\nadd r0, r0, 3\nstr r0, rl, d\nwhich one of the following lc-3 instruction sequences will be generated when \nconverting statement “a = c++;” (i.e., the if branch of the “if ... else ...” statement) to \nlc-3 assembly language instructions? label “next” should be regarded as the label \nof the statement that follows the “if ... else ...” statement.\n\nldr r0, rl, c\nadd r0, r0, 1\nstr r0, rl, c\nstr r0, rg, a\nbrnzp next\n\ndr r0, rg, c\nstr r0, rk, a\nadd r0, r0, 1\nstr r0, rg, c\n\nldr r0, rl, c\nstr r0, rg, a\nadd r0, r0, 1\nstr r0, rl, c\nbrnzp next\n\nldr r0, rg, c\nstr r0, rk, a\nadd r0, r0, 1\nstr r0, rg, c\nbrnzp next\n\nwhich one of the following lc-3 instruction sequences will be generated when \nconverting statement “d = d - b;” (i.e. the else branch of the “if ... else ...” statement) \nto lc-3 assembly language instructions?\n\nldr r0, rl, d\nldr r1, rg, b\nnot r1, r1\nadd r1, r1, -1\nadd r0, r0, r1\nstr r0, rl, d\n\nldr r0, rl, d\nldr r1, rg, b\nadd r0, r0, -r1\nstr r0, rl, d\n\nldr r0, rl, d\nldr r1, rl, b\nnot r1, r1\nadd r1, r1, -1\nadd r0, r0, r1\nstr r0, rl, d\n\nldr r0, rl, d\nldr r1, rg, b\nnot r1, r1\nadd r1, r1, 1\nadd r0, r0, r1\nstr r0, rl, d\n\n\n\n\n\n\nd = 3 (offset -1)\nc = 2 (offset 0)\ndynamic link\nret address\nret value\nb = -1 (offset 4)\na = 0 (offset 5)\nr5\nr6\nwhich one of the following lc-3 instruction sequences will be generated when \nconverting condition “d < c” of the “if ... else ...” statement to lc-3 assembly \nlanguage instructions? label “else” should be regarded as the label of the statement \nin the “else” branch.\n\nldr r0, rl, d\nldr r1, rl, c\nnot r1, r1\nadd r1, r1, 1\nadd r0, r0, r1\nbrnp else\n\nldr r0, rl, d\nldr r1, rl, c\nnot r1, r1\nadd r1, r1, 1\nadd r0, r0, r1\nbrn else\n\nldr r0, rl, d\nldr r1, rl, c\nnot r1, r1\nadd r1, r1, 1\nadd r0, r0, r1\nbrzp else\n\n\nldr r0, rg, d\nldr r1, rg, c\nnot r1, r1\nadd r1, r1, 1\nadd r0, r0, r1\nbrn else\n\nif d < c, after making c negative, when adding d and c, the result should be negative. the if \nstatement runs when n is set to 1, p and z are set to 0 (else).\n\n2023 s2 midterm\n\nconvert the given decimal numbers into 9-bit unsigned or signed binary integers. if \nthe binary integers are signed, provide your answer using the specified \nrepresentation: sign magnitude, 1's complement or 2’s complement signed integers. \n\nrepresent 147 as a 9-bit unsigned binary integer010010011\nrepresent 106 as a 9-bit sign magnitude binary integer001101010\nrepresent -53 as a 9-bit 1's complement binary integer111000010\nrepresent -157 as a 9-bit 2's complement binary integer101100001\n\nwhich of the following additions result in overflow? you can assume that both \noperands and the result are 8-bit 2's complement signed binary integers. \n\nselect one or more of the following options.\n\n00100101 + 01001111 = 37 + 79 = 116\n10111111 + 11000000 = -65 + -64 = -129 (overflow)\n11100111 + 11110101= -25 - 11 = -36\n01000011 + 00111111 = 67 + 63 = 130 (overflow)\n\nfor an 8-bit 2's complement, -2\nn-1 \nto 2\nn-1 \n- 1 values are represented (-2\n8 \nto 2\n8 \n- 1)\nrange = -128 to 127\n\nthe lc-3 architecture uses 16-bit addresses and has an addressability of 16 bits. \ngiven this information, answer the following questions:\n\nhow much memory, in bytes, does the lc-3 architecture have available? \n\nif the memory is arranged as a one dimensional array of memory locations, how many \noutputs would the memory address decoder need? \n\nif the memory is arranged as a two dimensional grid of memory locations, requiring a row \nand column address decoder, what would the total number of outputs be (row decoder \noutputs + column decoder outputs)? \n\nwhat is the assembly language code for the instruction represented by x56ba?\n0101 011 010 1 11010 = and r3 r2 #-26\n\nthe initial value of the registers are:\nr0: x0001; r1: xaaaa; r2: x5555; r3: x00f7;\nr4: x0000; r5: xffc1; r6: x003f; r7: x0000;\n\n(a) what is the value of r0 after and r0, r1, r2 is executed?\nxaaaa and x5555\n0101 0101 0101 0101 and 1010 1010 1010 1010 = x0000\n\n(b) what is the value of r3 after not r3, r3 is executed?\nnot x00f7\n0000 0000 1111 0111\n1111 1111 0000 1000 = xff08\n\n(c) what is the value of r4 after add r4, r5, r6 is executed?\nffc1 add 003f = x0000\n\n\n\nanswer the following questions. all of your answers must be in hexadecimal, in the \nformat \"x\" followed by 4 hexadecimal digits. for example, x1234 or xffff, are in the \nrequired format. answers are not case sensitive.\n\nwhat will be the value in register r5 when this program is executed?  \n\n\n\n\n\n\n\nafter the execution, r5 = x000c\n\nwhat is the machine code for the instruction ld r0, stop? \n0010 0000 0000 0101 = x2005\nwhat is the machine code for the instruction brnzp loop?\noffset = #-6\n0000 1111 1111 1010 = x0ffa\nr0r1r2r3r4r5\nxfffb000500000005000c000c\ncomplete the symbol table for this program\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwhat is the output of this program?\nr1: x0004\nr2: null\nr3: z\n\n\nx300d: t\nx300e: e\nx300f: s\nx3010: t\nx3011: null\nx3012: x0000, z\nx3013: x0000, k\nx3014: x0000, y\nx3015: x0000, z\nloopx3003\nexitx300a\ntextx300d\nnew_\ntext\nx3012\nendx3016\n\nwhat is the value at memory location x3ffe after the execution of the third jsr push \ninstruction?  \nr0: x0000\nr1: x0007\nr6: x3ffd\n\nx4000: x0006\nx3fff: x0003\nx3ffe: x0007after the third jsr push, x3ffe = x0007\n\nwhat is the value in register r1 after the execution of the second jsr pop \ninstruction? \nr0: x000c\nr1: x0001\nr6: x3ffd\n\nx4000: x0006\nx3fff: x0003\nx3ffe: x0001\nx3ffd: x0001 after the second jsp pop, r1= x0001\n\nwhat is the value in register r6 after the execution of the brnzp exit instruction? \nr0: x0001\nr1: x0001\nr6: x3ffd\n\nx4000: x0006\nx3fff: x0003\nx3ffe: x0001\nx3ffd: x0001 after the program, r6 = x3ffd\nwhat would the output of this program be? \nr1: 1\nr0: 2\nr2: xfe04\nr3: 0001\n\n\noutput: 123\nxfe06: 2\n\nwhat will be the value in register r7 after executing the instruction jsr print? \nx300b\n\n\nmcq 8\n\n7ff1k-2\n7ff2j-1\n7ff3i0\n7ff4dyn link1\n7ff5ret address2\n7ff6ret val3\n7ff7a4\n7ff8b5\n7ff9c6\n7ffad7\n7ffbz-2\n7ffcy-1\n7ffdx0\n7ffedyn link1\n7fffret address2\n8000ret val3\nwhich one of the following lc-3 instruction \nsequences will be generated for passing variable x \nto function foo?\n\nldr  r0, rl, x\nadd  r6, r6, #x\nstr  r0, r6, #0\n\nldr  r1, rl, x\nadd  r6, r6, #-1\nstr  r1, r6, #0\n\nldr  r0, rl, x\nadd  r6, r6, #1\nstr  r0, r6, #0\n\nldr  r0, rg, x\nadd  r6, r6, #-1\nstr  r0, r6, #0\n\nwhich one of the following lc-3 instruction \nsequences will be generated for passing value 1 to \nfunction foo?\n\nand  r0, r0, #0\nadd  r0, r0, #1\nadd  r6, r6, #1\nstr  r0, r6, #0\n\nadd  r0, r0, #1\nadd  r6, r6, #1\nstr  r0, r6, #0\n\nand  r0, r0, #1\nadd  r6, r6, #-1\nstr  r0, r6, #0\n\nand  r0, r0, #0\nadd  r0, r0, #1\nadd  r6, r6, #-1\nstr  r0, r6, #0\nwhich one of the following lc-3 instruction \nsequences will be generated for storing the return \naddress in the activation record of function foo?\n\nadd  r6, r6, #-1\nstr  r7, r6, #0\n\nadd  r6, r6, #1\nstr  r5, r6, #0\n\nadd  r6, r6, #-1\nstr  r5, r6, #0\n\nstr  r7, r6, #0\nadd  r6, r6, #-1\n\nwhich one of the following lc-3 instruction \nsequences will be generated for moving the value of \nvariable j to the slot reserved for storing the return \nvalue in the activation record of function foo?\n\nldr  r0, r6, #j\nstr  r0, rl, #3\n\nldr  r1, rl, #j\nstr  r1, rl, #3\n\nldr  r1, r6, #j\nstr  r1, rg, #j\n\nldr  r0, rg, #j\nstr  r0, r6, #3\n\nwhich one of the following lc-3 instruction \nsequences will be generated for restoring the stack \nframe pointer before function foo terminates?\n\nadd  r6, r6, #1\nldr  r5, r6, #0\n\nldr  r5, r5, #0\nadd  r6, r6, #-1\n\nldr  r5, r6, #0\nadd  r6, r6, #-1\n\nldr  r5, r6, #0\nadd  r6, r6, #1\nwhich one of the following lc-3 instruction sequences \nwill be generated for assigning the value returned by \nfunction foo to variable z and removing the remaining \nelements of the activation record of function foo from the \nruntime stack?\n\nldr  r1, r6, #0\nstr  r1, rl, z\nadd  r6, r6, #5\n\nldr  r0, r6, j\nstr  r0, rl, z\nadd  r6, r6, #4\n\nldr  r1, rl, j\nstr  r1, rl, z\nadd  r6, r6, #5\n\nldr  r0, r6, #0\nstr  r0, rl, z\nadd  r6, r6, #4\n\nmcq 9\n\nx2ff5int i0\nx2ff6dyn link1\nx2ff7ret address2\nx2ff8ret val3\nx2ff9int x4\nx2ffaint* y5\nx2ffbb=3-1\nx2ffca=20\nx2ffddyn link1\nx2fferet address2\nx3000ret val3\nwhich one of the following lc-3 instruction sequences will be \ngenerated for passing &b to function foo?\n\nldr  r1, rl, b\nadd  r6, r6, -1\nstr  r1, r6, 0\n\nadd  r1, rl, b\nadd  r6, r6, -1\nstr  r1, r6, 0\n\nadd  r1, rg, b\nadd  r6, r6, 1\nstr  r1, r6, 0\n\nadd  r1, rl, b\nadd  r6, r6, -1\nstr  r1, r6, b\n\nwhich one of the following lc-3 instruction sequences will be \ngenerated when converting “i = x + *y” to lc-3 assembly \nlanguage instructions?\n\nadd  r0, rl, yr0 = x2ffb (y points to x2ffb)\nldr  r0, r0, 0r0 = y\nldr  r1, rl, xr1 = x\nadd  r0, r1, r0r0 = y+x\nstr  r0, rl, ii = y+x\n\nldr  r0, rl, yr0 = y\nldr  r0, r0, 0r0 = mem[y]\nldr  r1, rl, xr1 = x\nadd  r0, r1, r0r1 = x +y\nstr  r0, rl, ii = y+x\n\nadd  r0, rl, yr0 = x2ffb \nldr  r0, r0, yr0 = y\nldr  r1, rl, xr1 = x\nadd  r0, r1, r0r0 = x + y\nstr  r0, rl, ii = = x + y\n\nldr  r0, rl, y\nldr  r1, rl, x\nadd  r0, r1, r0\nstr  r0, rl, i\nwhich one of the following lc-3 instruction sequences will be generated when converting \n“*y = x” to lc-3 assembly language instructions?\n\nldr  r1, rl, x\nadd  r0, rl, y\nstr  r1, r0, 0\n\nldr  r1, rl, x\nldr  r0, rl, y\nstr  r1, r0, y\n\nldr  r1, rl, x\nldr  r0, rl, y\nstr  r1, r0, 0\n\nldr  r1, rl, x\nadd  r0, rl, y\nstr  r1, r0, y\n\nx2ff1x[0]-11\nx2ff2x[1]-10\nx2ff3x[2]-9\nx2ff4x[3]-8\nx2ff5x[4]-7\nx2ff6x[5]-6\nx2ff7x[6]-5\nx2ff8x[7]-4\nx2ff9x[8]-3\nx2ffax[9]-2\nx2ffbb-1\nx2ffca0\nx2ffddyn link1\nx2fferet address2\nx3000ret value3\nwhich one of the following lc-3 instruction sequences will \nbe generated when converting “x[2] = 3” to lc-3 assembly \nlanguage instructions?\n\nand  r0, r0, 0\nadd  r0, r0, 3\nadd  r1, rl, xr1 = x2ff1\nstr  r0, r1, 2x2ff3: 3\n\nand  r0, r0, 0\nadd  r0, r0, 3\nstr  r0, rl, 2\n\nand  r0, r0, 0\nadd  r0, r0, 3\nadd  r1, rl, x\nldr  r1, r1, 0\nstr  r0, r1, 2\n\nand  r0, r0, 0\nadd  r0, r0, 3\nldr  r1, rl, xr1 = x[0]\nstr  r0, r1, 2x[0] + 2 = 3\nwhich one of the following lc-3 instruction \nsequences will be generated when converting \n“b = x[a]” to lc-3 assembly language \ninstructions?\n\n\nadd  r1, rl, xr1 = x2ff1\nldr  r1, r1, ax2ff1: x[a]\nstr  r1, rl, bb: x[a]\n\nldr  r0, rl, ar0 = a\nadd  r1, r0, xr1 = \nldr  r1, r1, 0\nstr  r1, rl, b\n\nldr  r0, rl, xr0 = x[0]\nadd  r1, rl, ar1 = x2ffc\nldr  r1, r1, 0x2ffc: a\nstr  r1, rl, bb = x2ffc\n\nldr  r0, rl, ar0 = a\nadd  r1, rl, xr1 = x2ff1\nadd  r1, r1, r0r1 = x2ff1\nldr  r1, r1, 0\nstr  r1, rl, b\n\nwhich one of the following lc-3 instruction \nsequences will be generated when converting \n“*(x+1) = 0” to lc-3 assembly language \ninstructions?\n\nand  r0, r0, 0r0 = 0000\nadd  r1, rl, xr1 = 2ff1\nadd  r1, r1, 1r1 = 2ff2\nstr  r0, r1, 0x[1] = 0\n\nand  r0, r0, 0r0 = 0000\nadd  r1, rl, xr1 = 2ff1\nadd  r1, r1, 1r1 = 2ff2\nldr  r1, r1, 02ff2 = x[1]\nstr  r0, r1, 0x[1] = 0\n\nand  r0, r0, 0r0 = 0\nldr  r1, rl, xr1 = x[0]\nadd  r1, r1, 1r1 = x[0] + 1\nstr  r0, r1, 0x[0] + 1 = 0\n\nand  r0, r0, 0r0 = 0\nldr  r1, rl, xr1 = x[0]\nadd  r1, r1, 1r1 = x[0] + 1\nldr  r1, r1, 0r1 = x[0] + 1\nstr  r0, r1, 0\nxaff1n=a\nxaff2m[0]\nxaff3m[1]\nxaff4m[2]\nxaff5dyn link\nxaff6ret address\nxaff7ret value\nxaff8a\nxaff9*b\nxaffaz=4\nxaffby=3\nxaffcx=2\nxaffddyn link\nxafferet address\nxafffret value\nwhich one of the following statements is correct?\n\nafter statement “m[1] = *b;” in function foo is executed, the \nvalue stored in location 0xaff2 is 0xaff9.\n\nafter statement “m[1] = *b;” in function foo is executed, the \nvalue stored in location 0xaff2 is 3.\n\nafter statement “m[1] = *b;” in function foo is executed, the \nvalue stored in location 0xaff3 is 3.\n\nafter statement “m[1] = *b;” in function foo is executed, the \nvalue stored in location 0xaff2 is 0xaffd.\n\nwhich one of the following statements is correct?\n\nafter statement “m[2] = &a;” in function foo is executed, the \nvalue stored in location 0xaff4 is 0xaffc.\n\nafter statement “m[2] = &a;” in function foo is executed, the \nvalue stored in location 0xaff1 is 0xaff9.\n\nafter statement “m[2] = &a;” in function foo is executed, the \nvalue stored in location 0xaff2 is 0xaffc.\n\nafter statement “m[2] = &a;” in function foo is executed, the \nvalue stored in location 0xaff4 is 0xaff8.\n\nmcq 10\n\nif a cache line consists of 32 bytes and we access the byte at memory address \n0x4567, what is the range of the addresses of the bytes that are loaded into the \ncache? assume only one line of the cache is loaded at a time.\n\n\noffset = (2\nn\n = 32 bits for offset) = 5\n\nso, x4560 = 0100 0101 0110 0000 has offset 0\nso, x457f = 0100 0101 0111 1111 has offset 31\n\nrange = x4560 to x457f \n\n\na direct mapped cache has 8 cache lines. each cache line consists of 2 words, and \neach word is one byte. the address bus consists of 7 bits.\n\n8 lines so 8 = 2^3, we have 3 bits for the index. for 2 words holding one byte each, we \nhave 2 = 2^1, so 1 bit for the offset. hence the tag = 7 - (3+1) = 3\n\nthe tag field of the cache consists of 2 bits.\n\nthe tag field of the cache consists of 3 bits.\n\nthe index field consists of 2 bits.\n\nthe index field consists of 1 bit.\n\na direct mapped cache has 8 cache lines. each cache line consists of 2 words, and \neach word is one byte. the address bus consists of 7 bits. which one of the \nfollowing statements is correct?\n\nthe total number of bytes for storing data in the cache is 8k bytes.\n\nthe total number of bytes for storing data in the cache is 8 bytes.\n\nthe total number of bytes for storing data in the cache is 16 bytes.\n\nthe total number of bytes for storing data in the cache is 32 bytes.\n\nindex = 3 bits\noffset = 1 bits\ntag = 3 bits\n\nthere are 3 cache lines and each line has 1 bytes, there are 2^4bytes in the cache.\n= 16 bytes\n\n\n\nindextagword[0]word[1]\n0 (000)0102021\n1 (001)\n2 (010)\n3 (011)0011617\n4 (100)0103929\n5 (101)\n6 (110)\n7 (111)0113e3f\ntag = 3 bits\nindex = 3 bits\noffset = 1 bit\n\naccess 1,2,3,4\n16 = 0001 0110 (miss)\n20 = 0010 0000 (miss)\n19 = 0001 1001 (miss)\n18 = 0001 1000 (hit)\n\naccess 5, 6, 7\n11 = 0001 0001 (miss)\n21 = 0010 0001 (miss)\n3e = 0011 1110 (miss)\n\naccess 8, 9, 10, 11, 12\n17 = 0001 0111 (hit)\n11 = 0001 0001 (mis)\n16 = 0001 0110 (hit)\n29 = 0010 1001 (miss)\n\naccess 13, 14, 15\n10 = 0001 0000 = (hit)\n17 = 0001 0111 = (hit)\n21 = 0010 0001 = (miss)\n\n\n\n\n\n\nregister files\n100s bytes\n<1 cycle access\nl1 cache\nseveral kb\n1-3 cycle access\nl2 cache\n32mb\n5-15 cycle access\nmemory\n128mb - few gb\n50-300 cycle access\ndisk\nmany gb - few tb\n1,000,000+ cycle access\ndata retrieval very fast, keeps us with \nspeed of cpu, but only holds a few \nhundred bytes of data.\nstill fast access speed, slightly slower \nthan registers, but it stores more data\naccess speed is lowered, but not too \nbad, and stores several tens of mb\nreally slow access time, by the time we access \ndata from memory, the cpu is able to run \nhundreds of instructions, but holds so much \nmore data\ndisks allow us to store any amounts of data. \nslowest.\nmade from same low density \nmaterial, and expensive.\nthe larger a memory device is, the speed of the clock needs to be reduced in order to ensure every memory cell receives the signal at the same time. hence, speed on a larger device would be slower than the speed on a \nsmaller device.\ntemporal locality: a recently accessed memory has a higher probability that in the near future the same data will be accessed again.\nspatial locality: if we are accessing a data item, it is highly likely that the data items stored around the currently accessed item, will be accessed in the near future.\n-so when loading list[1], we might also want to load list[2], list[3] ... etc\n-hence, we will pre-fetch this data item list into memory so it's available in cache\n\n\ncache look-up\nin a cache with 4 slots, each slot holds one item. when accessing data items, \ninsert them into an empty cache slot. if no slots are available, evict the data item in \nthe slot where the data item has not been accessed for the longest time.\n\nproblem: searching for a data item must be done in linear time complexity, \nhence it is not very efficient.\ndirect-mapped caches\nnow an address can go to just one predefined slot in the cache using slot = \naddress % cachesize. so addresses that have a remainder of 1 go into slot 1, \nremainders of 2 into slot 2 etc.\n\nproblem 1: searching for a data item requires a division operation, which \nconsumes a lot of clock cycles\nproblem 2: cache thrashing: two items go in and out all the time even though \nthere is plenty of space available.\nsolution: direct-mapped caches\nsolution 1: replace division operation with a \nbitwise and operation, provided the denominator \nis a power of 2.\nsolution 2: fully-associative cache\nfully-associative cache\nfully associative mapping is that it solves the conflict miss problem, thereby \nincreasing the hit rate. instead of one cache box, we have two cache boxes, which \nis the original split into two smaller cache lines.\n\nproblem: inefficient data lookup operations\nsolution: set-associative cache\n\nset-associative cache\naddressitem\n\n\naddressitem\n\n\n{1, 3, 5, 7, 9, ...}\n{0, 2, 4, 6, 8, ...}\n2 way set-associative cache\nwe can't fix all thrashing problem with a set-associative cache. for example, in a 2 way set-associative cache, the sequence {1,3,5,1,3,5} will cause a thrashing problem.\n\ncache calculations\ncache is organised as a collection of lines.\neach line consists of one or several bytes.\na cache with 2 lines where each line has four words and each word has four bytes, how many bytes are there in the cache?\n4 * 4 * 2 = 32\nin general, if there are 2\nm\n cache lines and each line has 2\nn\n bytes, there are 2\nm+n\n bytes in the cache.\n0122\nn\n-1\n\n\n...\n\n2\nm\n\n2\nn\n\nthe position of a byte in a cache line is given as an offset value. for a line with \n2\nn\n bytes, there are 2\nn\n offset values.\n\neach cache line is given an index. for 2\nm\n line, there are 2\nm\n index values.\noffset0offset1offset2offset2\nn\n-1\nindex 0\nindex 1\nindex 2\nindex 2\nm\n\nthe offset determines position in a cache line\noffset = log(bytes in a line)\nthe index determines the cache line\nindex = log(bytes of cache size)\ntagindex (m bits)offset (n bits)\nthe tag field distinguishes different memory \nlocations that are mapped to the same cache \nlocation.\n\ntagindexoffset\nwe have 2 lines.\n2 = 2\n1\n\nso, 1 bits is used for the index field.\n\neach word holds 4 bytes\nso 4 words = 16 bytes\n\n16 = 2\n4\nso, 4 bits are used for the offset field.\n\n\nfor a cache with 2 cache lines and 4 words per cache line...\nthe remaining bits are used for the tag \nfield.\n\n1 bits for index, 4 bits for offset.\n\neach line has 4 words, 4 bits each, so \neach line has 16 bits in total.\n\n16 - 5 = 11 bits for the tag field.\n\nfor a direct mapped cache of size 32 and each slot holds one byte, where should the data items at memory addresses 0x0 and 0x20 be?\ntag field = 27 bits index = 5 bits offset = 0 bits \n0x0 = 00000000\nlast five bits = x0000, hence x0 will be mapped to line 0\n\nx20 = 0010 0000\nlast five bits = x0000, hence x20 will be mapped to line 0\n\nhence, why we need a tag value.\nif a cache line consists of 16 bytes and we access the byte at address 0x1231, what is the range of the addresses of the bytes that are loaded into the cache? \nassume only one line of the cache is loaded at a time.\noffset = 4\nbecause the cache line holds 16 bytes, so log(16) = 4\n\nminimum offset value = 0\nmaximum offset value = #15 (0xf)\n\nso, x1230 is the first address with offset 0, x1231 is the second address with offset 1, and so x123f would be the last address with offset f.\n\nhence, the range of address would be from x1230 to x123f\n\nif a cache line consists of 32 bytes and we access the byte at memory address 0x4567, what is the range of the addresses of the bytes that are loaded into \nthe cache? assume only one line of the cache is loaded at a time.\n\noffset = 5\nx4567 = 0100 0101 0110 0111\n\nminimum = 0100 0101 0110 0000 = x4560\nmaximum = 0100 0101 0111 1111 = x457f\n\n\n\na direct mapped cache has 8 cache lines. each cache line consists of 2 words, and each word is one byte. the address bus consists of 7 bits. which one of \nthe following statements is correct?\n\na.the index field consists of 1 bit.\nb.the index field consists of 2 bits.\nc.the tag field of the cache consists of 2 bits.\nd.the tag field of the cache consists of 3 bits\n\nindex = 3\noffset = 1\ntag = 7 - 4 = 3\n\n\ntagword[0]word[1]\n0010x20x21\n1010x28x29\n2\n3001x16x17\n4001x18x19\n5\n6\n7011x3ex3f\neach cache line has 2 words, 1 byte each, so each cache line \nconsists of 2 bytes.\n\nthe address is 7 bits. \n\neach cache line has 2 bytes, so 2^1 = 2, so offset field has 1 \nbit.\n\nwe have 8 lines, so 2^3 = 8, so we have 3 bit for index field.\n\nremaining bits makes up tag field: 7 - 4 = 3\naddress 1,2,3,4\nx16 = 0001 0110 (miss)\nx20 = 0010 0000 (miss)\nx19 = 0001 1001 (miss)\nx18 = 0001 1000 (hit)\n\naddress 5,6,7\nx11 = 0001 0001 (miss)\nx21 = 0010 0001 (miss)\nx3e = 0011 1110 (miss)\nx17 = 0001 0111 (hit)\n\naddress 8, 9, 10, 11, 12\nx11 = 0001 0001 (miss)\nx28 = 0010 1000 (miss)\nx16 = 0001 0110 (hit)\nx29 = 0010 1001 (hit)\n\naddress 13, 14, 15, 16\nx10 = 0001 0000 (hit)\nx17 = 0001 0111 (hit)\nx21 = 0010 0001 (miss)\n\n\nhow to write data\ncpu → cache → memory\n\nif data is already in the cache\n\nwrite-through - write go to main memory and cache\nwhen we write data item we update item in cache as well as in main memory (completes only after memory write has completed, which takes a long time)\n-keeps data in cache and main memory consistent\n-takes a long time\n\n-reads entire block (cache line) from memory on a cache miss\n-writes only the updated item to memory for a store\n-evictions do not need to write to memory\n\nwrite-back\nwhen writing, we only update data item in cache not main memory\n-data in cache and memory is inconsistent (upon evicting, we have to write data in cache back to main memory)\n-only one copy of data\n\n-when we perform updates, we only update data in cache, so value stored in cache will not be the same as data stored in memory, upon eviction of an updated \ndata item, we have to send this to main memory to ensure the update is reflected in main memory.\n\nif data is not in the cache\n\nwrite-allocate\nfirst fetch data into cache, then compute write computation\n\nno write-allocate\nsend data straight into memory, don't write into cache\n\n\n\n\nvdtagbyte 1byte 2...byte n\n\n\n\n\n\nwrite-back meta-data\nv = 1 means the line has valid data\nd = 1 means the bytes are newer than main memory\n\nwhen allocating line:\n-set v=1, d=0, fill in tag and data\nwhen writing line:\n-set d=1\nwhen evicting line:\n-if d = 0; just set v=0\n-if d = 1; write-back data, then set d=0, v=0\n\nwrite-through vs write-back\nwrite-through\n(keeps data in cache and main memory consistent but takes a long time)\nwrite-back\n(only update data item in cache not main memory)\npros\nin a cache miss, write-through is generally better than write-through.in a cache hit, write-back is faster than write-through on writes\n\ncons\nin a cache hit, write-through is slower than write-back on writes\nin a cache miss, it needs to load a block of data. write-through needs to write \none data item to memory.\nin a cache miss, write-back is slower than write-through if a dirty line has to be \nevicted.\nin a cache miss, it needs to load a block of data. write-back needs to write a \nblock of data to memory (if dirty)\nwrite-through\non a cache hit:\n-data is written to both the cache and the main memory.\n-ensures data consistency between cache and main memory, but it is slower because it requires two writes.\non a cache miss:\n-data is fetched from the main memory and written to both the cache and the main memory.\n-can be slower because it involves fetching data from the main memory and updating both places.\nwrite-back\non a cache hit:\n-data is written only to the cache, and a \"dirty bit\" is set to indicate that the cache has the most recent data.\n-faster because it writes data only once (to the cache), reducing the write operations to the main memory.\non a cache miss:\n-if the cache line to be replaced is dirty (modified), it is written back to the main memory first. then, the new data is fetched from the main memory and updated only in the cache.\n-can be more complex as it requires checking the dirty bit and potentially performing additional write operations to the main memory.\n\n2d-array\na(0,0)a(0,1)a(0,2)a(0,3)\na(1,0)a(1,1)a(1,2)a(1,3)\na(2,0)a(2,1)a(2,2)a(2,3)\na(3,0)a(3,1)a(3,2)a(3,3)\na(0,0)\na(0,1)\na(0,2)\na(0,3)\na(1,0)\na(1,1)\na(1,2)\na(1,3)\na(2,0)\na(2,0)\na(2,1)\n...\na(3,3)\nhow a 2d-array is stored in memory\nlayout: row by row\nusing a 2d-array, we can access data either row by row, or column by column\n\nin a 4x4 array, accessing data in a row y row basis, we would expect 4 cache \nmisses, but using a column to column access, we expect 16 misses.\n\nvirtual memory\n\nshared memory multiprocessors (smp)\neach core can run a program independently as they have their own \nalu and control logic.\neach core runs a program alone, but on a shared interconnect to \nshare the same memory module.\n\nnow, multiple program running at the same time, it might be \nassumed that they have access to all available memory. now the \nissue is, what happens when another program is executed \nconcurrently on another processor?\n\nthe addresses would conflict.\n \ncore 0\ncache\ncore 1\ncache\ncore 2\ncache\ncore 3\ncache\ninterconnect\nmemoryi/o\nhow can the single main memory be shared by multiple \nprocesses?\nprogram aprogram b\n.orig x3000.orig x3000\n\nsolution 1: microsoft and google can sit together and decide on which memory addresses their programs can start at.\n-not a very viable option.\n\nsolution 2: map a virtual address (generated by cpu) to a physical address (in memory) automatically.\n-each programmer thinks they can access any memory location (can start program anywhere)\n-when program loaded in system, the programs are mapped to different regions in memory (a virtual address is given, program can convert its address into the virtual address)\nboth programs would try to access the same address in \nmemory, meaning one program is likely going to \noverwrite the other.\n\nproblem 2: not enough memory\nsolution: only load the instructions and data that are immediately needed into the memory\n\nin virtual memory, it's just an abstraction, meaning we can technically store any amounts of data into virtual memory.\nit appears to exist as main memory, but really, it corresponds to storage for information, without regard to its exact physical location. therefore, vm supports multi-tasking, the ability to \nrun more than one process at a time.\n-each process has its own virtual memory space.\n-a process is an instance of the execution of a program\n-programmers can code as if they own all of memory\n-program/cpu can access any address from 0 to 2^n -1\n\n\nphysical memory refers to the actual physical chip in the computer\n\na\nc\nb\nd\nc\nb\na\norder does not need to be preserved.\nphysical page just finds an available space to hold virtual page.\n\nmmu (memory management unit) allocated virtual pages to physical pages.\na\nb\nc\nd\n\n\na\nb\nc\n\nc\nb\na\n\nx\ny\nz\n\nx\nz\ny\nmmu\nmmu\nx1000\nx1000\nboth program a and program b start at x1000 (virtually)\nwhen the programs are accessed, the addresses in virtual address are \nconverted into a real address in physical memory via translation. this is \nhow both program a and b can start at the same address, but not \noverwrite each other.\nprogram a\nprogram b\n2\n10\n = 1024 = 1k\n1kb = 2\n10\n bytes\n1k of 1kb = 1mb = 2\n10\n x 2\n10\n bytes = 2\n20\n bytes\n1k of 1mb = 1gb = 2\n10\n x 2\n20\n = 2\n30\n bytes\nsize of page table:\n\nhow large is the page table is there is 4gb of virtual memory, adn each page in physical memory is \nrepresented by 16 bits\nsince 4gb = 4 x 2\n30\n = 2\n32\n\n\nwe would have 2 x 2\n32\n = 2\n34\n bytes in the page table. (16 bits = 2 bytes). in gb = 8gb\n\n\nconverting a virtual address into a physical address\n\n\nvirtual page numberpage offset\n1.partitional virtual address into a page number and offset\npage offset = n bits for a address consisting of 2^n bits\neg: if each page is 4k, then 4 x 2\n10\n = 2\n12\n, so offset = 12\n      2.       lookup page table\nconcatenate both to derive the physical address. simply lookup the page number, and \nconcatenate the physical page number with the offset value.\nthe size of a virtual page is the same as the size of a physical page. first byte in virtual page \nis the first byte in the physical page, and the second byte in the virtual page is the same as the \nsecond byte in the physical page. therefore, the position of a byte in the virtual page is the \nsame as the position of the byte in the physical page.\nexample\nread mem[0x00002538], assume each page is 4kb (2\n12 \nbytes) \n4kb = 4 x 2\n10\n = 2\n2 \nx 2\n10\n = 2\n12\n\noffset = 12 bits (3 hex digits)\noffset = x538\nvirtual page number = x00002\n\nptbr = x90000000 (starting address of page table)\n\n\nx90000000 (ptbr)\n\nx412b000\n\n70x10045\n6\n5\n4\n3xc20a3\n2x4123b\n1x10044\n0\n0x000020x538\nx4123b0x538\nphysical address\n\nexercise\nassume that the virtual address consists of 32 bits, the size of each virtual page is 8kb, the physical address consists of 30 bits, and the first five entries of the page \ntable are as below. consider virtual address 0x00004321.\n-what is the offset value? x0321\n-what is the vpn?00002 0321\n-what is the ppn?x1812e 0321 (0001 1000 0001 0010 1110 0 0011 0010 0001)\n-what is the physical address that corresponds to the virtual address? (0 0011 0000 0010 0101 1100 0011 0010 0001 = 0x3025c321\nread mem[0x00004321], assume each page is 8kb (8 x 2\n10 \nbytes = 2\n3\nx 2\n10\n = 2\n13\n) \noffset = 13 bits \n\nx4321 = 0100 0011 0010 0001\noffset = 0 0011 0010 0001 = x0321\nvpn = 0000 0000 0000 0000 0100 0011 0010 0001\n\n\n\n\n2\n10\n = 1024 = 1k\n1kb = 2\n10\n bytes\n1k of 1kb = 1mb = 2\n10\n x 2\n10\n bytes = 2\n20\n bytes\n1k of 1mb = 1gb = 2\n10\n x 2\n20\n = 2\n30\n bytes\nx0000 0000 0000 0000 0100x0321\nx000020x0321\n4x12345\n3x00891\n2x1812e\n1x0891a\n0x16789\nx1812e0x0321\n\ninvalid pages\n\nkerneluser\nvalid bitrwxrwxnumber\n0disk b\n1x10045\n0\n0\n1xc20a3\n1x4123b\n1x10044\n0\npage table also consists of condition bits to represent access permissions for data stored in corresponding virtual \npage.\n\nkernel: os system permission\nuser: permissions for users\n\nsometimes, the physical memory is not large enough to gold all the instructions and the data of the programs.\n\nhence, we should use paging/swapping to counter this issue.\n\npaging/swapping\n\npaging allows us to run processes larger than physical memory. (view memory as a \"cache\" for secondary storage)\nswap: swap memory pages out to disk when not in use\npage: page them back in when needed\n\na page fault occurs when the accessed location is not in the physical memory\n-null entry: illegal address (location is not allocated to the program to used)\n-on disk: bring in the page from disk\n\nuse temporal/spatial locality\n-pages used recently most likely to be used again soon\n\nif a swapped-out page is pages into the physical memory later, it might be stored at a different physical address.\nwhen v=0:\n1.case1: page entry is empty → instruction tries to access a memory location that hasn't been allocated for program to use (illegal address). system needs to terminate \nexecution of program and generate an error message (segmentation fault).\n2.case2: entry for this page is not empty. corresponds to a virtual address that still resides on disk. needs to be brought into main memory.\n\n\n\ndata types\nintinteger (occupies at least 16 bits)\ndoublefloating point (occupies at least 32 bits)\ncharcharacter (occupies at least 8 bits)\n\ndifferent data types occupy different amounts of memory\n\nglobal variables (can access anywhere in the program)\nlocal variables (can access only in a particular region)\n\n\n#include <stdio.h>\nvoid foo();\nint global = 0;\n\nint main()\n{\n  int local = 1; \n  printf(\"main: global %d local %d\\n\", global, local)\n  global = 1;\n  foo();\n  printf(\"main: global %d local %d\\n\", global, local)\n}\n\nvoid foo() {\n  int local; \n  local = 3;\n  printf(\"foo: global %d\\n\", global);\n  global = 2;\n}\nglobal variable\nlocal to main\nglobal is updated\nlocal to foo\nglobal is updated\nmain: global 0 local 1\nfoo: global 1\nmain: global 2 local 1\noutput\n\nsymbol table\nrecords information about variables (name of variables, \ntheir types, their offset and their scope)\n\n\n x0000\n\n\n\n\n\n\n\n\n\nxffff\naddress increases\nos\nos\ninstructions\nglobal variables\nr4\npc\nr4 always points to the starting location where the global variables are stored.\nruntime stack\nholds information on functions that are being executed.\nthe runtime stack only holds information about functions that have been explicitly called.\n\n \naddress increases\nfirst global \nvariable\nsecond global \nvariable\nthird global \nvariable\nr4\naddress = [r4] + offset\noffset 0\noffset 1\noffset 2\n\n \naddress increases\nthird local \nvariable\nsecond local \nvariable\nfirst local \nvariable\nr5\naddress = [r5] + offset\noffset 0\noffset -1\noffset -2\nlocal variables are stored in the stack frame.\n\n x0000\n\n\n\n\n\n\n\n\n\nxffff\nos\nos\ninstructions\nglobal variables\nr4\npc\nmain\nfoo\nint main() {\n  foo()\n}\n\nvoid foo() {\n}\nafter function foo() has terminated, it is \nremoved from the stack.\nr6\nr5\nr6 points to the top of the runtime stack.\nr5 points to the bottom of the stack frame of the current function.\neach function has their own stack frame.\n\n#include <stdio.h>\nint inglobal;\n\nmain()\n{\n int inlocal;\n  int outlocala;\n  int outlocalb\n\n  inlocal = 5;\n  inglobal = 3;\n\n  outlocala = inlocal++ &~inglobal;\n  outlocalb = inlocal - inglobal;\n\n}\nnametypeoffsetscope\ninglobalint0global\ninlocalint0main\noutlocalaint-1main\noutlocalbint-2main\n\nldr/str\n\nldr r2, r5, #-2\n[r5] + -2 → address\nr2 ← [address]\n \nfirst global \nvariable\nsecond global \nvariable\nthird global \nvariable\nr4\nldr r1, r4, #2\n[r4] + 2 → address\nr1 ← [address]\n \nthird local \nvariable\nsecond local \nvariable\nfirst local \nvariable\nr5\nlocalglobal\n\nstr r2, r5, #-3\n[r5] + -3 → address\n[address] ← [r2]\n \nfirst global \nvariable\nsecond global \nvariable\nthird global \nvariable\nr4\nstr r1, r4, #4\n[r4] + 4 → address\n[address] ← [r1]\n \nthird local \nvariable\nsecond local \nvariable\nfirst local \nvariable\nr5\nlocalglobal\n\n#include <stdio.h>\nint inglobal;\n\nmain()\n{\n int inlocal;\n  int outlocala;\n  int outlocalb\n\n  inlocal = 5;\n  inglobal = 3;\n\n  outlocala = inlocal++ &~inglobal;\n  outlocalb = inlocal - inglobal;\n\n}\nand r0, r0, #0\nadd r0, r0, #5storing #5 into r5 local\nstr r0, r5, #0\n\nand r0, r0, #0\nadd r0, r0, #3storing #3 into r4 global\nstr r0, r4, #0\n\n#include <stdio.h>\nint inglobal;\n\nmain()\n{\n  int inlocal;\n  int outlocala;\n  int outlocalb\n\n  inlocal = 5;\n  inglobal = 3;\n\n  outlocala = inlocal++ &~inglobal;\n  outlocalb = inlocal - inglobal;\n\n}\nb=1\na = b++post-increment\nassign current value of b first, then increase the value of b by 1.\na=1\nb=2\na = ++bpre-increment\nfirst increase value of b by one, then add it to a.\na = 2\nb=2\ninlocal and not(inglobal)\n\nldr r0, r5, #0retrieve inlocal into r0\nadd r1, r0, #1increment inlocal\nstr r1, r5, #0store inlocal in r5 / update it\n\nldr r1, r4, #0retrieve inglobal\nnot r1, r1bitwise not of inglobal\nand r2, r0, r1bitwise and with pre-increment inlocal and not inglobal \nstr r2, r5, #-1stores the result in inlocala (offset -1)\n\n#include <stdio.h>\nint inglobal;\n\nmain()\n{\n  int inlocal;\n  int outlocala;\n  int outlocalb\n\n  inlocal = 5;\n  inglobal = 3;\n\n  outlocala = inlocal++ &~inglobal;\n  outlocalb = inlocal - inglobal;\n\n}\ninlocal - inglobal\n\nldr r2, r5, #0retrieve inlocal into r2\nldr r3, r4, #0retrieve inglobal into r3\nnot r3, r3negative inglobal\nadd r3, r3, #1increment inglobal\nadd r2, r2, r3compute inlocal - inglobal into r2\n\nstr r2, r5, #-2store the result into outlocalb\n2's complement conversion \nof positive to negative\n\nconditional statements\n\nint main() {\nint x, y;\nif (x == 2) y = 5;\n}\n \ny\nx\nr5\noffset 0\noffset -1\nldr r0, r5, #0\nadd r0, r0, #-2\nbrnp not_true\n\nand r1, r1, #0\nadd r1, r1, #5\nstr r1, r5, #-1\n\nnot_true\n...\n\nint x, y, z;\nif (x) {\n  y++;\n  z–;\n}\nelse {\n  y–;\n  z++;\n}\nldr r0, r5, #0\nbrz else\n\nldr r1, r5, #-1\nadd r1, r1, #1\nstr r1, r5, #-1\nldr r1, r5, #-2\nadd r1, r1, #-1\nstr r1, r5, #-2\nbrnzp done\n\nelseldr r1, r5, #-1\nadd r1, r1, #-1\nstr r1, r5, #-1\nldr r1, r5, #-2\nadd r1, r1, #1\nstr r1, r5, #-2\ndone\n...\n \ny\nx\nr5\noffset  0\noffset -1\nz\noffset -2\nstack frame of the current function\nif (x)\n●in c, an expression is considered true if it evaluates to a non-zero value.\n●an expression is considered false if it evaluates to zero.\n\nint x;\nx = 0\nwhile (x < 10) {\n  x= x + 1\n}\nand r0, r0, #0\nstr r0, r5, #0\n\nloopldr r0, r5, #0\nadd r0, r0, #-10\nbrzp done\n\nldr r0, r5, #0\nadd r0, r0, #1\nstr r0, r5, #0\nbrnzp loop\ndone\n \n0\nr5\noffset  0\nstack frame of the current function\n012345678910\n-10-9-8-7-6-5-4-3-2-10r0\nr5\nfor (x = 0); x < 10; x++)\n\nfunctions\nzero or multiple arguments are passed in.\n\nsingle or no result is returned.\n\nreturn value is always a particular type, if there is no return value, the return type is void.\n\nr6 always points to the top of the runtime stack.\n\n \nos\nnoname\nr5\n \na\nb\nreturn value\nreturn address\ndynamic link\nw\nx\ny\nlocals\nbookkeeping\nparameters\nint noname (int a, int b)\n{\n  int w, x, y;\n\n  return ;\n}\nos\ninstructions\nglobal variables\nr5\ndynamic link\naddress of stack frame for caller of function, so upon \ntermination we can restore r5.\n\nreturn address\naddress in caller that we need to execute after the \nfunction terminates\n\nreturn value\nreserves location for holding a return value.\n\n \na\nb\nreturn value\nreturn address\ndynamic link\nw\nx\ny\n \noffset  5\noffset  4\noffset  3\noffset  2\noffset  1\noffset  0\noffset -1\noffset -2\naddress increases\n\nint function2 (int a) {\n  int w;\n  ...\n  w = function1(w, 10);\n  ...\n  return w;\n}\n\nint function1 (int q, int r) {\n  int k;\n  int m;\n  ...\n  return k;\n}\nand r0, r0, #0\nadd r0, r0, #10\nadd r6, r6, #-1\nstr r0, r6, #0\n\nldr r0, r5, #0\nadd r6, r6, #-1\nstr r0, r6, #0\n\njsr function1\n\nadd r6, r6, #-1\n\nadd r6, r6, #-1\nstr r7, r6, #0\n\nadd r6, r6, #-1\nstr r5, r6, #0\n\nadd r5, r6, #-1\n\nadd r6, r6, #-1\n\nldr r0, r5, #0\nstr r0, r5, #3\n\nadd r6, r5, #1\n\nldr r5, r6, #0\nadd r6, r6, #1\n\nldr, r7, r6, #0\nadd r6, r6, #1\n\nret\n \n25\n10\nr5\nr6\nxfd00\n \nreturn value\nreturn address\ndynamic link\nw\nr\na\n25\nk\nq\nreturn value\nreturn addressr7\naddress of caller\nk\nm\ndynamic link\nk\nm\n\ncalling the function:\npush second argument\nand r0, r0, #0\nadd r0, r0, #10\nadd r6, r6, #-1\nstr r0, r6, #0\n\npush first argument\nldr r0, r5, #0\nadd r6, r6, #-1\nstr r0, r6, #0\n\ncall subroutine\n\nstarting the callee function:\nleave space for return value\nadd r6, r6, #-1\n\npush return address\nadd r6, r6, #-1\nstr r7, r6, #0\n\npush dynamic link (callers frame ptr)\nadd r6, r6, #-1\nstr r5, r6, #0\n\nset new frame pointer\nadd r5, r6, #-1\n\nallocate space for locals\nadd r6, r6, #-1\nending the callee function\ncopy return into return value\nldr r0, r5, #0\nstr r0, r5, #3\n\npop local variables\nadd r6, r5, #1\n\npop dynamic link\nldr r5, r6, #0\nadd r6, r6, #1\n\npop return address\nldr, r7, r6, #0\nadd r6, r6, #1\n\nreturn control to caller\nret\n\nrecursion\nwhenever a recursive function is invoked, a new activation record is pushed onto the stack.\n\na solution based on a recursive function is not as efficient as a loop-based solution.\n\nint factorial(int n) {\n  int i;\n  int result = 1;\n  for (i = 1; i <= n; i++)\n    result *= i;\n  return result;\n}\n\n\n\n\n\n\n3*fact(2)\nmain\n\n\n\n\n\n3*fact(1)\n3*fact(2)\nmain\n\n\n\n\n1\n3*fact(1)\n3*fact(2)\nmain\nr6\nr6\nr6\n\n\n\n\n\n3*fact(1)\n3*fact(2)\nmain\nr6\n\n\n\n\n\n\n3*fact(2)\nmain\nr6\n\n\n\n\n\n\n\nmain\nr6\n\npointers\nint *p;  (p is a pointer to an integer)\n*p (returns the value pointed to by p\n&z (returns the address of variable z)\n\nint i;\nint *ptr;\n\ni = 4;\nptr = &i;\n*ptr = *ptr + 1\nx3000x3001ptr\nx30014 5i\ni = 4;\nand r0, r0, #0\nadd r0, r0, #4\nstr r0, r5, #0\n\nptr = &i;\nadd r0, r5, #0\nstr r0, r5, #-1\n\n*ptr = *ptr + 1\nldr r0, r5, #-1\nldr r1, r0, #0\nadd r1, r1, #1\nstr r1, r0, #0\nr5\npointer variables: always store addresses of other variables.\ndereferencing pointers (*ptr): accesses or modifies the value at the \naddress the pointer holds.\n\nvoid swap2(int* i, int* j) {\n  int temp;\n  temp = *i;\n  *i = *j:\n  *j = temp;\n}\n\nint main() {\n  int a = 12, b=13;\n  swap2(&a, &b);\n}\nx300113b\nx300012a\nswap2(x3000, x3001)\nswap2(x3000, x3001)\nx300212temp\nx300112b\nx300013a\nin this example, in the main() function, two integers, and and b are declared and \nstored into memory somewhere. \n\nthe addresses of a and b are passed into the swap2() function. hence, \nswap2(x3000, x3001)\n\nin the swap2() function, the parameters are pointers, since x3000 and x3001 are \npassed, the content of x3000 and x3001 are what's passed in. \n\na temp variable is initiated, and it stored what x3000 holds. temp = 12\n\nthen the content of x3000 is assigned to x3001, so i =j\n\nfinally, j is assigned the content of temp, so  j = i\n\n\n\nint main() {\n  int a = 12, b=13;\n  swap2(&a, &b);\n}\nxeff4\nxeff5\nxeff6\nxeff7\nxeff8\nxeff913boffset -1\nxeffa12aoffset 0\nxeffbdynlink\nxeffcreturn ad\nxeffdreturn val\nr5\nr6\n\nxeff1\nxeff2\nxeff3\nxeff4\n\n\nxeff5\n\n\nxeff6\n\n\nxeff7xeffai\nxeff8xeff9j\nxeff913boffset -1\nxeffa12aoffset 0\nxeffbdynlink\nxeffcreturn ad\nxeffdreturn val\nr5\nr6\nvoid swap2(int* i, int* j) {\nadd r0, r5, #-1\nadd r6, r6, #-1\nstr r0, r6, #0\n\nadd r0, r5, #0\nadd r6, r6, #-1\nstr r0, r6, #0\nr0 = xeff9\nr0 = xeffa\n\n\nint main() {\n  int a = 12, b=13;\n  swap2(&a, &b);\n}\n\nxeff1\nxeff2\nxeff312tempoffset 0\nxeff4xeffadynlink\nxeff5return ad\nxeff6return val\nxeff7xeffai\nxeff8xeff9j\nxeff912boffset -1\nxeffa13aoffset 0\nxeffbdynlink\nxeffcreturn ad\nxeffdreturn val\nr5\nr6\n  int temp;\n  temp = *i;\n  *i = *j:\n  *j = temp;\n\nldr r0, r5, #4\nldr r1, r0, #0\nstr r1, r5, #0\n\nldr r1, r5, #5\nldr r2, r1, #0\nstr r2, r0, #0\n\nldr r2, r5, #0\nstr r2, r1, #0\nr0 = xeffa\nr1 = 12\nstore 12 into r5\nr1 = xeff9\nr2 = 13\nstore 13 into r0\nr2 = 12\nstore 12 into r1\n\narrays\n\ndeclaration: typevariable[number_of_elements]\narray reference: variable[index]\n\nthe last element of the array is stored in the stack frame first.\n\nx300axoffset -10\nx3009grid[0]offset -9\nx3008grid[1]offset -8\nx3007grid[2]offset -7\nx3006grid[3]offset -6\nx3005grid[4]offset -5\nx3004grid[5]offset -4\nx3003grid[6]offset -3\nx3002grid[7]offset -2\nx3001grid[8]offset -1\nx3000grid[9]offset 0\nint grid[10], x\nx = grid[3] + 1\ngrid[6] = 5\nadd r0, r5, #-9r0 = &grid[0]\nldr r1, r0, #3r1 = grid[3]\nadd r1, r1, #1r1 = grid[3] + 1\nstr r1, r5, #-10store grid[3] + 1 into x\n\nand r0, r0, #0r0 = 0\nadd r0, r0, #5r0 = 5\nadd r1, r5, #-9r1 = x3009\nstr r0, r1, #6store 5 into x3003\nr5\n\nx300axoffset -10\nx3009grid[0]offset -9\nx3008grid[1]offset -8\nx3007grid[2]offset -7\nx3006grid[3]offset -6\nx3005grid[4]offset -5\nx3004grid[5]offset -4\nx3003grid[6]offset -3\nx3002grid[7]offset -2\nx3001grid[8]offset -1\nx3000grid[9]offset 0\ngrid[x+1] = grid[x] + 2\nldr r0, r5, #-10r0 = x\nadd r1, r5, #-9r1 = x3009\nadd r1, r0, r1r1 = x3009 + x\nldr r2, r1, #0r2 = x\nadd r2, r2, #2r2 = x+2\n\nldr r0, r5, #-10r0 = x\nadd r0, r0, #1r0 = x+1\nadd r1, r5, #-9r1 = x3009\nadd r1, r0, r1r1 = x3009 + x+1\nstr r2, r1, #0store x+2 into x3009 + x+1\nr5\n\nxaff5\nxaff6\nxaff7\nxaff8\nxaff9\nxaffa\nxaffb3y\nxaffc2x\nxaffddyn link\nxafferet add\nxafffret val\nvoid foo(int* a, int b);\nint main() {\n  int x = 2\n  int y = 3\n  foo(&x, y);\n}\n\nvoid foo(int * a, int b) {\n  int m[4], n;\n  n = b;\n  m[0] = a;\n  m[1] = *a\n  m[2] = &b;\n  a = 4;\n  b = 5;\n}\nr5\n\nxaff13n\nxaff2xaffcm[0]\nxaff32m[1]\nxaff4xaffam[2]\nxaff5m[3]\nxaff6xaffc\nxaff7ret add\nxaff8ret val\nxaff94a\nxaffa5b\nxaffb3y\nxaffc2x\nxaffddyn link\nxafferet add\nxafffret val\n   foo(&x, y);\n\nvoid food(int * a, int b) {\n  int m[4], n;\n  n = b;\n  m[0] = a;\n  m[1] = *a\n  m[2] = &b;\n  a = 4;\n  b = 5;\n}\nr5\n\noop\n\nstruct flighttype plane;\n\nstruct flighttype {\n  char flightnum[7]\n  int altitude;\n  int longitude;\n  int latitude;\n  int heading;\n  int airspeed;\n};\nplane.flightnum[0]\nplane.flightnum[1]\nplane.flightnum[2]\nplane.flightnum[3]\nplane.flightnum[4]\nplane.flightnum[5]\nplane.flightnum[6]\nplane.altitude\nplane.longitude\nplane.latitude\nplane.heading\nplane.airspeed\nyou can also use the flighttype name to declare other \nstructs.\n\nstruct flyer {\n  char name[20];\n  struct flighttype flight;\n}\n\nc provides a way to define a data type by giving a new \nname to a predefined type.\n\ntypedef <type> <name>;\n\ntypedef struct flighttype flight;\n\nstruct flighttype plane;\n\nstruct flighttype {\n  char flightnum[7]\n  int altitude;\n  int longitude;\n  int latitude;\n  int heading;\n  int airspeed;\n};\noffset -13y\noffset -12index 0plane.flightnum[0]\noffset -11index 1plane.flightnum[1]\noffset -10index 2plane.flightnum[2]\noffset -9index 3plane.flightnum[3]\noffset -8index 4plane.flightnum[4]\noffset -7index 5plane.flightnum[5]\noffset -6index 6plane.flightnum[6]\noffset -5index 7plane.altitude\noffset -4index 8plane.longitude\noffset -3index 9plane.latitude\noffset -2index 10plane.heading\noffset -1index 11plane.airspeed\noffset 0 (r5)x\nint x;\nflight plane;\nint y;\n\nplane.altitude = 0;\naddress of an element with index i = starting address + i\n\n",
  "220": "\n\ngraph definitions\n\nthe minimum number of arcs in any graph is 0\n1\n23\n1\n23\nthe maximum number of arcs in a digraph or order n is n * (n - 1)\ndigraph of 3 nodes\n3 * (3-1) = 6\nthe maximum number of arcs in an undirected graph is (n * (n - 1)) / 2\ngraph of 3 nodes\n3 * (3-1) = 6 / 2 = 3\nwalk: a sequence of nodes where each node is connected to the next one by an edge.\npath: a walk with no repeated nodes.\ncycle: a walk where the only repeated nodes are the first and last, and the walk has at least 3 different nodes involved.\n\norder: the number of nodes\nsize: the number of arcs (minimum size = 0, maximum size = n(n-1))\n\nsparse: smaller size o(n)\ndense: bigger size o(n\n2\n)\n\nmatrixdetailslistdetails\nchecking if an arc existsө(1)directly access the entry in the matrix at row u and column v to check \nif 1\nө(d)search through the list of neighbors of node u to see if v is present.\noutdegree of nodeө(n)count the number of 1s in the row corresponding to node u.ө(1)directly return the length of the list of neighbors of node u.\nindegree of nodeө(n)count the number of 1s in the column corresponding to node v.ө(n+m)search through all lists to count occurrences of node v. with a maintained \ncount, you can return it directly.\nadding an arcө(1)set the entry in the matrix at row u and column v to 1.ө(1)append node v to the list of neighbors of node u\ndeleting an arcө(1)go to the column i, turn all 1's into 0,ө(d)locate source node, scan through this list to find the target node and remove it.\nadding a nodeө(n)requires creating a new matrix with one additional row and column \nand copying the existing entries\nө(1)add a new list (initially empty) for the new node.\ndeleting a nodeө(n\n2\n)requires creating a new matrix without the row and column of the \ndeleted node and copying the remaining entries.\nө(n+m)remove the list of the node and remove all occurrences of the node from other \nlists. in the worst case, this involves scanning through all lists.\n0\n1\n2\n34\n0: 2\n1: 0,3,4\n2: \n3: 4\n4: 1\n00100\n10011\n00000\n00001\n01000\nspace requirements\nmatrix = ө(n\n2\n)\nlist = ө(n+m logn)\n\nwe need to search for j in list i. the complexity then depends on the length of \nlist i. list i is length d where d is the outdegree of node i so searching for j is \nin θ(d).\n\nhence, answer is b.\n\n\n011100\n010000\n110010\n011005\n110100\n100000\ntherefore, node 0's list is\n0\n0\n1\n0\n1\n1\n\narc definitions\n\nsimple traverse:\n\n-all nodes are white to begin with\n-a staring white node is chosen and turned grey\n-a grey node is chosen and its out-neighbours explored\n-if any out-neighbours is white, it is visited and turned grey. if no out-neighbours are white, the gray node is turned black.\n-the process of choosing grey nodes and exploring neighbours is continued until all nodes reachable from the initial node are black.\n-if any white nodes remain in the digraph, a new starting node is chosen and the process continues until all nodes are black.\npredecessor array:\n\nabcde\n-1-1bca\nab\nc\nd\ne\nsearch forrest:\n\n1. tree arc: all traversed edge{a,e}, {b,c}, {c,d}\n2. forward arc: x appears before y, direct path from x-y (is not tree arc){b,d}\n3. back arc: y appears before x, direct path from y-x (is not tree arc){d,b}\n4. cross arc: x/y appears before y/x, direct path from y-x/x-y (is not tree arc){c,a}\n\nan edge (u,v) is a tree edge if v is a direct descendant of u\nan edge (u,v) is a forward edge if v is a descendant of u but v was already discovered when u explores it.\n-distinction from tree edge: v is not discovered directly by u but rather through another descendant of u.\nan edge (u,v) is a back edge if v is an ancestor of u\nan edge (u,v) is a cross edge if v is neither an ancestor nor a descendant of u.\na\nb\nc\nd\ne\n\ndepth first search\n\ndfs\n●visit the starting node and mark it as visited.\n●from the current node, go to an unvisited neighbor node, mark it as visited, and continue this process.\n●if you reach a node with no unvisited neighbors, backtrack to the previous node and continue the process \nfrom there.\n\nө(n+m) assuming we are using an adjacency list representation.\n0\n12\n34\nseen06172\ndone59483\npred-1-1012\n(v,w) is a tree arc or a forward arc\nseen[v] < seen[w] < done[w] < done[v]\nand v is an ancestor of w\n\n(v,w) is a back arc\nseen[v] < done[v] < seen[w] < done[w]\nand v is not an ancestor of w\n\n(v,w is a cross arc)\nseen[w] < done[w] < seen[v] < done[v]\n0\n1\n2\n4\n3\n\nnode012345\nseen012936\ndone11851047\nfor (1,4)\nseen node 1 at time 1, done node 1 and time 8\nseen node 4 at time 3, done node 4 at time 4\n\nseen[1] < seen[4] < done[4] < done[1]\nsince we saw 1 first, then saw 2, and then 4, 4 is not \ndiscovered directly by 1 but rather through another \ndescendant of 1. hence, (1,4) is a forward arc.\n\nnode01234\nseen02173\ndone95684\nfor (3,4)\nseen node 3 at time 7, done node 1 and time 8\nseen node 4 at time 3, done node 4 at time 4\n\nseen[4] < done[4] < seen[3] < done[3]\nhence, (3,4) is a back arc\n\nbreadth first search\n\nbfs does not produce forward arcs.\n\n(v,w) is a tree arc\ncolour[w] = white and d[w] = d[v] + 1\nthe levels of w and v differ by 1\n\n(v,w) is a back arc\ncolour[w] = black and d[w] <= d[v] - 1\n\n(v,w) is a cross arc\nd[w] < d[v] - 1 and colour[w] = black\nd[w] = d[v] and colour[w] = black or grey\nd[w] = d[v] - 1 and color[w] = black or grey\nthe levels of w and v are the same or differ by 1\n01\n2\n4\n3\n56\nnode0123456\ndepth0111222\npred-1000231\nqueue: 0 1 2 3 6 4 5\nfind a tree arc, cross arc, forward arc, and back arc.\ntree arc = {0,1} {0,2} {0,3} {1,6} {2,4}\ncross arc = {0, 2} {5,6} {2,3} {1,2}\nback arc = {2,0}\nno fowad arcs exist in bfs\n0\n12\n3\n456\nbfs runs by processing nodes at distance 1, then all nodes at distance 2, etc from the root.\n\nnode01234\ndepth210\npred24-1\nqueue: 4 2 1\n\n0\n1\n2\n3\n4\n5\ndfs starting at node 0:\nnode012345\nseen012348\ndone11107659\npred-101231\n0\n1\n2\n3\n4\n5\ntree arcs:\n{0,1}, {1,2} {2,3} {3,4} {1,5}\n\nback arcs:\n{4,2} {2,1}\n\nforward arcs:\n{0,2} {0,5}\n0\n1\n2\n3\n4\n5\ndfs vs bfs\nnode012345\ndepth012341\npred-100230\nbfs starting at node 0:  \n0\n12\n3\n4\n5\ntree arcs:\n{0,1}, {1,2} {2,3} {3,4} {0,5}\n\ncross arcs:\n{1,5} {1,2} {2,1}\n\nback arcs:\n{4,2}\n\npriority first search\n\neach grey node is associated with an integer key. the key represents the priority for the node, were a lower key represents a higher priority, and a higher key represents a \nlower priority. think of a key as the amount of time the node is willing to wait, a lower key means its impatient, and a higher key means its patient.\n\ntakes ω(n\n2\n)\n54\n03\n12\nuse setkey(v) = -index(v)\nqueue013245\nkey0-1–3-2-4-5\n0\n1\n3\n24\n5\n\ntopological sort\n\na topological sort of a digraph g is a linear ordering of the nodes of g, such that if u comes before v in the ordering, (v,u) != e(g)\na graph cannot be topologically sorted if it contains a cycle.\nrunning time: o(n+m)\n01\n23\n4\n1.find a source node\nnode 4 is the only source node in g\n2.find nodes that 4 points to (2, 0, 1 or 3)\nlet's pick node 3\n3.finds nodes that 4 or 3 point to (2, 0, 1)\nlets pick node 1\n4.finds nodes that 4, 3 or 1 points to\nlets pick node 2\n5.only one node is left, hence add node 0\n43120\na digraph with no cycles is a dag for a directed acyclic graph.\na digraph has a topological order if and only if it is a dag.\nif g is a dag, listing nodes in reverse order of dfs finishing times is a topological sort.\n01\n23\n4\nnode01234\nseen21507\ndone34568\npred133-1-1\nhence, listing the 'done' times in descending order, we get 4, 3, 2, 1, 0\n43201\n\n4\n03\n21\nnode01234\nseen07136\ndone58249\npred-1400-1\nsorting the 'done' times in descending order, we get: 4, 1, 0, 3, 2\n\ngirth & connectivity\n\ngirth\ngirth: length of shortest cycle in a undirected graph\ndirected girth: length of shorted cycle in a directed graph\n-if a graph is acyclic, the girth of g is undefined (0 or ∞)\nwe can use bfs to find the shortest cycle (directed girth) in a cyclic digraph \nn(n+m).\n\nsimply run bfs, and the first time a back arc is found, we have found a cycle of \nlength (depth +1). we run this for all the nodes and return the minimum length.\nconnectivity\na graph is connected if for each pair of nodes, there is a path from u to v.\nwe can find the number of connected components by running bfs or dfs. run it \nagain for every starting node and counting how many trees we have.\n\nrunning time = ө(n(n+m))\n\n\nstrong connectivity\na digraph g is strongly connected if for each pair of nodes, there is a path from u to \nv and from v to u.\nstrong components:\n-if there is a cycle of 2 (u→ v and v→u)\n-if you start at a node and cannot get back\n\n{1,2} and {2,1} are strongly connected.\nnode 4: no way to get back to node 3\nnode 0: no way to get back to node 3\nnode 3: no way to get back to node 3\n\nhence, there are 4 strongly connected components in g.\n\ntarjan's algorithm\nfinding the number of strongly connected components in a digraph\n\ntarjan's algorithm\n1.run dfs on g\n2.list nodes of g in reverse order of done times (last finished to first)\n3.run dfs on gr choosing root nodes in order of list from step 2\nthis produces a forest where trees correspond to strong components of g.\n01\n234\nnode01234\nseen01723\ndone96854\ndone times = 0, 2, 1, 3, 4\n01\n234\nnode01234\nseen0145\ndone3276\n0\n1\n2\n3\n4\nhence there is 3 strongly connected components.\n\ntarjan's algorithm – extra example\n024\n1\n3\nnode01234\nseen01324\ndone \ntime\n98675\ndone order: 0, 1, 3, 2, 4\n024\n1\n3\nnode01234\nseen03728\ndone \ntime\n641059\n0\n3\n1\n2\n4\nfinal number of strongly connected components = 2\n\nbipartite graphs\n\na graph is bipartite if the nodes can be partitioned into two non-empty, disjoint sets, such that each edge has one endpoint.\nproperties of bipartite graphs\ng does not contain an odd length cycle\n\nafter running bfs, we get a tree-like structure with:\ntree arcs (on the same level or differ by one)\ncross arcs on the different level\n\n\n\n\nweighted digraphs\n\nadjacency matrix for weighted digraphs\n0\n12\n3\n2\n1\n6\n1\n3\n0123\n00003\n11000\n20200\n30610\nmatrix modification:\nthe out neighbours are now \nrepresented by their weights instead \nof a binary variable 1.\n0: 2 3\n\n1: 0 1\n\n2: 1 2\n\n3: 1 6 2 1\nlist modification:\nthe out neighbours are now represented by \nthe node they points too followed by the \nweight.\ndistance:\n\nthe distance from node 1 to node 2 = 1 + 3 = 4\nthe distance from node 2 to node 1 = 2\n\nthe distance from node 3 to node 0\n-path 13 → 1 → 0 = 6 + 1 = 7\n-path 23 → 2 → 1 → 0 = 1 + 2 + 1 = 4\n\ndistance matrix\n0123\n0053∞\n1104∞\n2320∞\n34310\n\ndijkstra's algorithm\n\nfinding the shortest path in a cyclic weighted digraph with no negative weights, from a source node\n\n1.get source node, and calculate its distance matrix row.\n2.find the minimum distance node, and find its distance matrix row\n3.repeat\n2\n04\n3\n1\n1\n1\n1\n6\n5\n3\n2\n2\n2\nnode01234\ndist0315∞\nnode01234\ndist02157\nnode 0\nnode 0 → 2\nnode01234\ndist02144\nnode 0 → 2 → 1\nnode 0 → 2 → 1 → 3 node01234\ndist02144\ndiameter: the diameter is the maximum length of the \nshortest paths between any pair of vertices in the graph.\n\nrunning time: n\n2\nusing a priority queue = ө(n+m logn)\n\nbellman-ford algorithm\ndealing with negative weights\nrunning time = ө(n*m)\n\n3\n2\n4\n1\n0\n-2\n2\n1\n32\n1\n3\n0\nset node 4 as source\nnode01234\ndist∞∞∞∞0\nnode01234\ndist220-20\nif there is a negative weight cycle present, there does not exist a minimum \npath (undefined).\n\nrun bellman ford run n+1 iterations to indicate the presence or absence of a \nnegative weight cycle. any change in the dist array will indicate the presence \nof a negative weight cycle.\n\n0\n1\n2\n3\n4\n5\n4\n-1\n5\n1\n2\n7\n2\n1\n2\nnode012345\ndist0∞∞∞∞∞\nnode012345\ndist03-12-10\nafter the second interaction, nothing changed, hence, this must be the \nshortest possible traversal.\n\nfloyd's algorithm\n\n01\n23\n45\n4\n1\n4\n3\n2\n2\n1\n4\n3\n012345\n0041∞4∞\n140∞234\n21∞0∞3∞\n3∞2∞0∞1\n4433∞02\n5∞4∞120\ncost matrix\nall pairs shortest paths problem: finding the shortest path in-between every pair of vertices in a weighted graph. this means determining the minimum \nweight or cost required to travel from any node to any other node in the graph.\n\ndijkstra: n m log n\nbellman ford:n\n2\nm\nfloyd's:n\n3\n\n012345\n0041648 6\n1405234\n21507 639\n3627 605 31\n44335 302\n5849120\ndistance matrix\n1.build the cost matrix\n2.compare for x=0, x=1, x=2, x=3, x=4, and x=5\n○whether or not the box is greater than x + column of interest\n○for example, in first iteration, ∞ is greater than 4 + 1, hence that box is updated to \n5. then ∞ is greater than 1 + 4, hence that box is updated to 5\n○in the next iteration we use x=1 as the benchmark. (column 1)\n○in other words: is there a path from node (row) to (column), using node x\n\nextra example for floyd's algorithm for the all pairs problem\n01\n4\n23\n4\n1\n5\n31\n2\n2\n01234\n0041∞2\n140∞52\n21∞0∞3\n3∞5∞01\n422310\n01234\n004132\n140532\n215043\n333401\n422310\ncost matrixafter floyd's algorithm\n\nalgorithm summary\nssspapspnegative weights\nbfsfor weighted graphs no. but for \nunweighted graphs, yes with a \ntime of n+m\nfor weighted graphs no. but for \nunweighted graphs, yes with a \ntime of n(n+m)\nno\ndijkstrayes, (m log n) with a primary \nqueue utilising a heap structure.\nyes. n*m lognno\nbellman fordyes. slower runtime than \ndijkstra. n*m\nyes. n\n2\n*myes\nfloydyes. n\n3\nyes. n\n3\nyes\ngenerally, unweighted digraph (dense) = dijkstra\nfor sssp or apsp, use bellman ford on a sparse digraph, but for dense digraphs use floyd.\n\n\nprim's algorithm\nkruskal algorithm\n\na spanning tree of a graph g is a subgraph of g that contains all does of g, and is a connected acyclic graph.\na minimum spanning tree is a spanning tree that which has the minimum total weight (sum of all edge weights)\n\nprim's algorithm (m log n):\n-choose a starting node\n-find it's neighbours and traverse the arc with the lowest weight as long as connecting this node does not produce a cycle\n-find the neighbours and traverse the arc with the lowest weight of the last node visited and continue\n\nkruskal algorithm (m log n):\n-start with an empty set of edges\n-at each step choose an edge of minimum weight from the remaining edges ensuring that adding the edge does not create a cycle in the subgraph built for far\n-stop when the subgraph is a spanning tree.\n\n54\n0\n1\n3\n2\n2\n41\n3\n2\n3\n3\n4\n1\nprim's algorithm\n54\n0\n1\n3\n2\n1\n1\n2\n4\n3\n5\n2\n3\nkruskal's algorithm\n54\n0\n1\n3\n2\n1\n1\n2\n4\n3\n5\n2\n3\n54\n0\n1\n3\n2\n2\n41\n3\n2\n3\n3\n4\n1\n\nsemester one 2023\n\nsearch through all lists to count occurrences of \nnode v. with a maintained count, you can return \nit directly. (n+m)\n\nanswer = d\n011100\n010000\n110010\n011001\n110100\n100000\nthe first column corresponding to node 0 will be \n0,0,1,0,1,1\n\nanswer = b\n\nqueue: 4 2 1\n\nnode0  1  2  3  4\ndepth           2   1      0\n\nanswer = a\ntopological order = done times in reverse = 4, 1, 0, 3, 2\nnode01234\nseen06138\ndone57249\n\n(1,4)\nnode 4: seen at 3, done at 4\nnode 1: seen at 1, done at 8\n\nseen node 1 < seen node 4 < done node 4 < done node 8\ntree arc or foward arc.\n\naccording to the search tree, node 4 is not a direct ancestor of node 1, hence this must \nbe a forward arc.\nanswer = d\nnode\n012345\nseen\n012936\ndone\n11851047\n0\n12\n4\n3\n5\ngirth is defined by the shortest cycle in a graph.\nsince this is a digraph, we just ignore the directions, and view the underlying graph. hence \nthe shortest cycle would be of length 3.\n\nanswer = c\n\nnode\n01234\nseen\n04561\ndone\n39872\npred\n-1-1120\nreverse done times = 1, 2, 3, 0, 4\nnode\n01234\nseen\n60148\ndone\n73259\npred\n-1-11-1-1\n0\n41\n32\n1\n2\n304\ntotal trees io search forrest = 4, hence there are 4 strongly connected \ncomponents.\nanswer = d\nnode\n01234\nseen\n02173\ndone\n95684\npred\n-12001\n0\n23\n1\n4\n\n0\n1\n2\n4\n3\nby definition, a ccle is a pat (no nodes are repeated) with \nthe exception that the first and last node can be repeated.\nanswer = d\nnode\n012345\nseen\n046581\ndone\n31171092\nreversed done time order = 1, 3, 4, 2, 0, 5\nnode\n012345\nseen\n801269\ndone\n11543710\n0\n1\n2\n3\n4\n5\n1 4 0\nanswer = d\n\nrunning bfs can determine whether or not an odd cycle exists, \nbfs running time of (m+n), hence = (m+n)\n\nanswer = b\nminimum number of connected components = 1\nmaximum number of connected components = 4\ndifference = 4-1 = 3\n\nanswer = a\n\nif a graph has 4 strongly connected components, this means there \nare four nodes that cannot get back to themselves.\nif we add one arc, this would create a cycle, as one node would \nhave to point to an ancestor.\nhence, the minimum number of connected components it can have \nis 1.\nanswer = b\n\n0\n1\n2\n2\n1\n-3\n\n\nprim's algorithm (m log n):\n-choose a starting node\n-find it's neighbours and traverse the arc with the lowest weight as long as \nconnecting this node does not produce a cycle\n-find the neighbours and traverse the arc with the lowest weight of the last node \nvisited and continue\n01\n32\n4\n3\n2\n4\n2\n1\n2\n3\n1\n3\n01234\n-12041\n\n0\n15\n3\n6\n2\n4\n4\n3\n3\n1\n2\n2\n4\n5\n4\n4\n5\n1\n5\n01234\n3 →\n3∞401\n3→4\n35300\n3→4→0\n35300\n3→4→0→2\n\n\n0:\n1: 2 1 3 1 4 3\n2:\n3:\n4:\n\n2024 s1 exam\n\nq1:\nquickselect average runtime = ө(n)\nt(1000) = c * 1000 = 1 second\nc = 1 / 1000\nt(1000000) = 1 / 1000 * 1000000 = 1000000 / 1000 = 1000 seconds\n\nq2:\ni ← 1\nwhile i < n:\n  i ← 3 * i\n  if i % 2 == 1:\n    {c elementary operations}\n\ni increases by a factor of 3 in each iteration while it is less than n, so the while loop runs log\n3\n(n) \ntimes. in each loop, the if statement always runs as i is initially set to 1, being multiplied by 3 wil \nalways yield a multiple of 3, which is always odd. therefore, every if statement is true, and also \nruns c elementary operations.\n\noverall runtime = log\n3\n(n) * c\n\n\ni = 1i = 3i = 9i = 81i = 243i = 729i = 2187...log\n3\n(n)\ni = 3 * 1 = 3\n3 % 2 == 1\ni = 3 * 3 = 9\n9 % 2 == 1\ni = 3 * 9 = 27\n27 % 2 == 1\ni = 3 * 27 = 81\n81 % 2 == 1\ni = 3 * 81 = 243\n243 % 2 == 1\ni = 3 * 243 = 729\n729 % 2 == 1\ni = 3 * 729= 2187\n2187% 2 == 1\n...\nq31\nseen[v] < done[v] < seen[w] < done[w]\n\nseen[v] < seen[w] < done[v] < done[w]\n10 90 150 200\n\nseen[v] < seen[w] < done[w] < done[v]\n\nseen[v] < seen[w] < done[w] < done[v]\n\n\n\nq11.\nfunction f(n):\n    if n < 1: return 0\n    return g(n+1)\nfunction g(n):\n    return f(n-2)\n\nruntime = n\n1234\nfunction f(1)\n   if 1 < 1: return 0\n   return g(2)\nfunction g(2)\n   return f(0)\n\nfunction f(0)\n   if 0 < 1: return 0\n0\nfunction f(2)\n   if 2 < 1: return 0\n   return g(3)\nfunction g(3)\n   return f(1)\n\n\n\n0\nfunction f(3)\n   if 3 < 1: return 0\n   return g(4)\nfunction g(4)\n   return f(2)\n\n\n\n0\nfunction f(4)\n   if 4 < 1: return 0\n   return g(5)\nfunction g(5)\n   return f(3)\n\n\n\n0\n0\n4\n1\n3\n2\nq20.\ntopological order: 2,0,1,4,3\n01234\nseen01824\ndone76935\nq32.\npredecessor array = [3, -1, 1, -1, 3]\n0\n4\n1\n3\n2\n01234\ndepth10101\npred3-11-13\nq26:\nweight of spanning tree = 13\n0\n15\n3\n4\n2\n7\n6\n2\n2\n2\n2\n2\n1\n3\n1\n3\n3\n5\n4\n3\n4\n\n\n\n \n \n \n \n \n \n \n \n best average worst stable in-place swaps comparisons \nselection \nsort \no(n\n2\n) o(n\n2\n) o(n\n2\n) \nno \n(linked-lists \nyes) \nyes depends comparisons same for any list \nn\n2\n \ninsertion \nsort \no(n) \no(n\n2\n) o(n\n2\n) \nyes yes swaps = #inversions \nsorted-list = 0 inversions \nreverse = inversions \n푛(푛−1)\n2\nn-1 + #inversions \nmerge sort o(nlogn) o(nlogn) o(nlogn) yes no \n(linked-lists \nyes) \n*the minimum possible number of comparisons occurs when all the \nelements in l1 are smaller than the first element  \n* the maximum possible number of comparisons occurs when we \nsort of have to zig-zag between l1 and l2 \nquicksort o(nlogn) o(nlogn) \no(n\n2\n) \nno almost hash load factor n/m \noccupied entries  / capacity \nif median = pivot then good \n \nheapsort o(nlogn) o(nlogn) o(nlogn) no yes use quickselect to find the 3rd largest element of the list 25, 65, \n50, 21, 2, 67, 70, 31, 15, 8.  \n• pivot = 25, partition into [21,2,15,8] 25 [65 50 67 70 31]; \n• move into the right hand side; \n• pivot = 65, partition into [50 31] 65 [67 70]; \n• 3rd largest element is the pivot; \n• stop. average = θ(n) \ntreesort o(nlogn) o(nlogn) \no(n\n2\n)  \n  \nhashing add/find/delete in time o(1) avg / o(n) worst \n1024! ≪ 100\n1024\n log\n1024\n(n) ≪ 10\n1024\n log\n3\n(n) ≪ 10\n1/2\n n ≪ 1/5 n log\n5\n(n\n3\n) ≪ 12n\n5/4 \n≪ 8n\n2\n ≪ 6n\n7 \n≪ 10\nn−4\n ≪ 1024n! \nlog n ≺ (log n)\n2\n ≺ √n ≺ n ≺ n log n ≺ n(log n)\n2\n ≺ n\n2\n ≺ n\n2\n ≺ · · · ≺(1.5)\n2\n ≺ 2\n2\n≺ n! ≺ n\n2\n. \n \n \n  \n \n \nss -running time on a linked list is not much different from performance on an array. the differences involve swap versus insertion, but comparisons dominate the running time \nanyway. \n-there is no better way to find the maximum using a list than what we have done above. except for heapsort.  \n-the algorithm is very insensitive to the input. its best and worst case running time are very similar. the best case is when the list is already sorted; the worst is when \nevery swap is needed, and this occurs when the input permutation has no fixed point. however the number of comparisons is the same for every input. \n-it makes the smallest possible number of swaps of any comparison-based sorting algorithm, so may be useful if data moves are very expensive \nis \n-we can reduce the number of data moves if we use a linked list. however, searching to find the right insertion point still takes time in θ(n\n2\n) in the worst case. insertion sort \nhas running time that is very sensitive to the input. the best case occurs when the input is already sorted, and the worst when the input is in reverse sorted order. \nms -splitting is very easy, most of the work is in combining \n-if linked lists are used, it can be done in place. in any case the number of comparisons is in θ(n) \n-mergesort is not very sensitive to the input order. if the input is already sorted, the merge operation does the fewest possible comparisons. the worst case occurs when the \ninput looks like 5,1,7,3,6,2,8,4 (every possible comparison is made at every level of the merge).  \nqs -most of the work is in the splitting, combining is very easy. \n-quicksort is almost never used on a linked list. it is too difficult to quickly find a good pivot element. \n-the worst case number of comparisons occurs when the pivot is always at the end of the sublist. for example, if we always choose the first element as the pivot and the \ninput is in sorted order, this will happen. the running time is then θ(n\n2\n). the \nbest case occurs when the pivot turns out to be the median element, so that the left and right subarrays are balanced at each level of the recursion, and this gives running \ntime in θ(n log n) as with mergesort. \nd.tree -this is a binary tree whose leaves are the outputs of the algorithm. there are n! possible outputs or leaves \n-the number of comparisons required to obtain the output at a leaf is the length of the path from the root to that leaf (the depth of the leaf node). \n-the worst case number of comparisons is the maximum depth (the height) of the tree. \n- going from the root, the number of nodes at each level at most doubles. if the maximum depth (height) is h then the number of leaves is at most 2\nh\n \nq -if we implement q using an unsorted list, we obtain a selection sort. insertion takes θ(1) time but deletion time is θ(n). the total is quadratic. -if we use a sorted list to \nimplement q, we obtain insertion sort. insertion takes θ(n) time but deletion is θ(1). the total is quadratic.  \n-we can do better with an implementation in which insertion and deletion each take time o(log n). the simplest is the binary heap. \nheap -is left-complete (every level except perhaps the last is full and the last level is left-filled) \n-has the partial order property (on every path from the root, the keys decrease) \n-building a heap using n successive insertions takes time in o(n log n) since the tree has height in o(log n) \n-deleting the root n times, restoring the heap property each time, takes time in o(n log n) \n-most efficient implementation: build a complete binary tree without the heap property, then recursively heapify the left and right subtrees, and then let the root swap down to \nthe right position. \n\nbst -the number of comparisons required to find the key is the depth of the leaf containing that key. worst-case run-time: \nθ(logn) \n-the best case is when we find the element on the first try (it is in the middle position) \n- each node is ≥ every node in the left subtree and ≤ every node in the right subtree \n-the running time of all basic operations is proportional to the number of nodes visited \n-in the worst case, finding/removing/inserting take time in θ(height)  \n- the average search cost in a bst built by random insertions is θ(log n) . \nbst vs quicksort \nthe cost of constructing the tree is the same \nas the number of comparisons used by \nquicksort in sorting the file. this is equal to the \ninternal path length, the sum of all depths of \nnodes \n \n matrix details list details traversals \nchecking if \nan arc exists \nө(1) directly access the entry in the \nmatrix at row u and column v to \ncheck if 1 \nө(d) search through the list of neighbors of \nnode u to see if v is present. \ndfs  sparse list: ө(n+m), dense list / matrix: ө(n\n2\n) \n(v,w) is a tree arc or a forward arc \n seen[v] < seen[w] < done[w] < done[v] \n and v is an ancestor of w \n(v,w) is a back arc \n seen[v] < done[v] < seen[w] < done[w] \n and v is not an ancestor of w \n(v,w is a cross arc) \n seen[w] < done[w] < seen[v] < done[v] \noutdegree of \nnode \nө(n) count the number of 1s in the \nrow corresponding to node u. \nө(1) directly return the length of the list of \nneighbors of node u. \nindegree of \nnode \nө(n) count the number of 1s in the \ncolumn corresponding to node \nv. \nө(n+\nm) \nsearch through all lists to count \noccurrences of node v. with a \nmaintained count, you can return it \ndirectly. \nbfs  sparse list: ө(n+m), dense list / matrix: ө(n\n2\n) \nbfs does not produce forward arcs. \n(v,w) is a tree arc \n colour[w] = white and d[w] = d[v] + 1 \n the levels of w and v differ by 1 \n(v,w) is a back arc \n colour[w] = black and d[w] <= d[v] - 1 \n(v,w) is a cross arc \n d[w] < d[v] - 1 and colour[w] = black \nadding an arc ө(1) set the entry in the matrix at \nrow u and column v to 1. \nө(1) append node v to the list of neighbors \nof node u \ndeleting an \narc \nө(1) go to the column i, turn all 1's \ninto 0, \nө(d) locate source node, scan through this \nlist to find the target node and remove \nit. \npfs = ω(n\n2\n) \ntarjans  o(n+m) \ntopological o(n+m) dag \n(a graph of order n with a topological order has n connected \ncomponents) \n \n \ndijkstra o(m log n) \nqueue = ө(n+m logn) \nbellman ford ө(n*m) \nfloyds o(m log n) \nprims o(m log n) \nkruskal o(m log n) \nadding a \nnode \nө(n) requires creating a new matrix \nwith one additional row and \ncolumn and copying the existing \nentries \nө(1) add a new list (initially empty) for the \nnew node. \ndeleting a \nnode \nө(n\n2\n) \nrequires creating a new matrix \nwithout the row and column of \nthe deleted node and copying \nthe remaining entries. \nө(n+\nm) \nremove the list of the node and \nremove all occurrences of the node \nfrom other lists. in the worst case, this \ninvolves scanning through all lists. \n \n ssps apsp -ve weights dfs can be used to determine whether or not a digraph is a dag. if \nthe traversal finds no back arcs, g is a dag \n \nthe diameter of a strongly connected digraph g is the maximum \nof d(u, v) over all nodes u, v ∈ v(g). if the digraph is not strongly \nconnected the diameter is undefined though can be set to ∞. \nbfs for unweighted graphs:  n+m for unweighted graphs: n(n+m) no \ndijkstra queue = ө(n+m logn) n*m logn no \nbellman  slower runtime than dijkstra. n*m n2*m yes \nfloys yes. n3 yes. n3 yes \ngenerally, unweighted digraph (dense) = dijkstra \nfor sssp or apsp, use bellman ford on a sparse digraph, but for dense digraphs use floyd. \n \n \n\n",
  "223": "\n\n \nmgmt223 | summer semester 2025 - lecture notes \n \nmodule one: evolution of work 2 \nmodule two: vocational preferences and the university-to-work transition 3 \nmodule three: job satisfaction and alienation 4 \nmodule five (b) (sts and dc model) and module six (eri and jdr models) 6 \nmodule seven: work in standardised and simple services 7 \nmodule eight: work in knowledge-intensive services 8 \nmodule nine (globalisation and mncs) and module ten (employee voice) 9 \nmodule eleven: the future of work and of the workforce 12 \ncourse review & exam preparation 13 \nconcepts | module seven* 13 \nconcepts | module eight 15 \nconcepts | module nine 16 \nconcepts | module ten 16 \nconcepts | module eleven 17 \ntheories 18 \ntrends 18 \ncontroversies 18 \n \n \n11-feb-2025 module one: evolution of work \nmodule two: vocational preferences and the university-to-work transition \n \n  \n \n \n12-feb-2025 module three: job satisfaction and alienation \n \n \n13-feb-2025 module four (taylorism and fordism) and module five (sts and the dc model) part (a) \nmodule five (b) (sts and dc model) and module six (eri and jdr models) \n \n  \n \n  \n14-feb-2025 module seven: work in standardized and simple services \n \n \n15-feb-2025 module eight: work in knowledge-intensive services \n \n \n16-feb-2025 module nine (globalisation and mncs) and module ten (employee voice) \n \n \n17-feb-2025 module eleven: the future of work and of the workforce \ncourse review & exam preparation \n \n  \n \n  \n18-feb-2025 revision \n \n19-feb-2025 final exam - remote online non-invigilated exam on inspera \n \n1 \n\n \nmodule one: evolution of work \n(1) hunting and gathering societies \n- strong division on labour: men hunting together to catch large game, while women and children gathered seeds \nand plants. \n- ability to make tools from stone to create food and shelter \n- relatively egalitarian societies [all people are equal and deserve equal rights and opportunities] \nlegacies \n- ability to cooperate in teams to tackle larger challenges \n- fashion implements from stone into usable tools \n- learning by doing, by imitating others, learning by directly undertaking new work experiences \n  \n(2) agricultural societies \n- wild plant and animal could be domesticated and stored more effectively \n- greater specialisation developed \nlegacies \n- domestication of crops and animals as a key part of human societies \n- increasing specialisations in work \n- skills in metallurgy (production / purification of metals) leads to more sophisticated tools \n    \n(3) imperial societies \n- ancient rome; societies dominated weaker ones and created the first large cities \n- slavery became prevalent \n- development of craft workers, weaving, carpentry, masonry, gold smithery, etc \n- precursors to trade unions due to working in specialised groups \nlegacies \n- a sad tradition of slavery, where it has not yet been completely exterminated from forced labor \n- growth of self-employed craft workers \n- on-going growth of job specialisation \n    \n(4) feudal societies \n- agricultural societies organised around monarchies, feudal estates, cities and towns \n- landlords extracted shares of crops and required work on land, restricting freedom of movement \n- slavery declined and guilds grew (masons, bakers, tailors, carpenters, blacksmiths) \nlegacies \n- apprenticeship system became a key part of society \n- combination of learning on the job and learning academic / theory based \n- guids (organisation) that regulate who practices a particular profession \n- apprentice → journeyman → master = basic career structure \n1) apprentice: a novice, needing experience and supervision \n2) journeyman: slightly more independent and proficient, completed trade of apprenticeship \n3) master: runs a group of apprentices and journeymen, teachers, owners, managers \n    \n(5) merchant capitalism \n- where individuals use their personal capital to build power in trade and markets \n- the \"putting-out\" of raw materials into workers' homes in towns and the countryside \nlegacies \n- subcontracting and outsourcing, putting our work through a supply chain \n- piece rate system still applies to this day, creating potentials for conflicts of interest (difference in interest \nbetween contractor and employer) \n    \n(6) industrial revolution \n- explosion of scientific findings and research which developed quickly, transferred into making of goods, especially \ngeographically near a major power source. beginnings of a mass production society \n- high levels of specialisation and high levels of job simplification so children can undertake jobs \n- wages were related to output rather than a piece, involves repetitive tasks, physical stamina \n- disturbances to public peace, governments responded by minimum wages, safety and health, maximum working \nhours, protection of children, in the late 19th century. hr specialists, etc \nlegacies \n- extensive science and engineering evolution: transforming old forms of work into new industries \n- expansion on job simplification, dehumanising a job experience, affecting quality of work \n- growing technical trades in specialised fields, and growth of management jobs (hr, hierarchies), creating internal \nlabour markets; a horizontal array of jobs \n    \n(7) advanced industrial revolution \n- individuals face a wide array of occupational options \n- extensive participation in secondary and higher education for both males and females \n- large multidivisional and multinational corporations with extensive career ladders/networks \n- great variation in job quality in services, from standardised and simple jobs to highly specialised \nknowledge-intensive jobs \n- increasing application of computers, ict, robotisation, and ai \n2 \n\n \nthe various stages of social and economic development have left a range of legacies including... \n❖ various forms of labour: slavery; waged and salaried employment; self-employment; apprenticeships \n❖ an ever-present impact of technology and science on work \n❖ teamwork and organisations to coordinate human work but conflicts of interest over wage levels and payment \nsystems and the control of work \n❖ extensive job specialisation but also job simplification \n❖ long periods of educational preparation for work \n❖ on-the-job learning and career structures ('internal labour markets') but also 'dead-end' jobs \n❖ institutions to extend and protect worker rights (trade unions, employment laws) but variable impacts within and \nacross societies \n❖ a massive service sector ranging in job quality from low-paid, repetitive jobs to well-paid professional and expert \nwork \nmodule two: vocational preferences and the university-to-work transition \nholland's vocational typology: expresses six major \"vocational personalities (riasec model)\" as (1) realistic, (2) \ninvestigative, (3) artistic, (4) social, (5) enterprising, and (6) conventional \n \n(1) realistic type \noutdoorsy person, using operating equipment such as tools and machinery, and generally likes to build, repair and create. \nthey're into concrete and practical solutions and opposed to imaginative or scholarly activities. they may be farmers, \ncarpenters, civil engineers, mechanics, electricians, cooks, etc \n(2) investigative type \nobserves, asks questions, likes to solve challenging problems, think analytically, deal with data, and are less social. they \nmay be computer programmers, analysts, professors, lab technicians, etc \n(3) artistic type \ninto art, music, performance, and drawn naturally to these talents. they perceive problems in an artistic manner, intuition \nand inner creativity. they may be architect, musicians, actors, photographers, etc \n(4) social type \nthe people who love other people, helping, healing, communicating, meeting and greeting. these people may perceive \nproblems in a social context, asking other people to solve problems as opposed to researching an article. they may be \nbartenders, counsellors, nurses, hr directors, teachers, etc \n(5) enterprising type \nmeeting, leading and influencing people, starting businesses, selling, promoting and using enterprising beliefs to solve \nproblems. they may be real estate agents, insurance managers, sales managers, etc \n(6) conventional type \nlikes working indoors and at tasks that involve organising and being accurate. solve problems by following established rules \nand practices, not keen on ambiguity. they may be a bank teller, accountant, secretary, etc \n \naccording to holland's key theoretical arguments, in a study of 989 men, 79% of job transitions occurred within a single \nmajor vocational preference category, demonstrating that people tend to move among similar jobs. people tend to act on \ntheir dominant interests and seek occupations in which their interests can be expressed. \n \nyou are more likely to feel more fulfilled if you work in an environment consistent with your vocational personality due to a \nhigher intrinsic job satisfaction. hence, a clearer sense of identity means increased stability in career life, and fewer job \nchanges. \n \nexperimental phase of careers: employee turnover rates are highly correlated with age \ndata on employee turnover by age suggests that people often experiment with job choices in their late teens / twenties. \nalthough, there is natural turnover around the age of 30-40 due to changes in preference to job choices made in their 20s. \n \nthe university-to-work transition \na good graduate job can be defined by training and experience one gets. where a company decides to take a chance on \nsomeone, and sees if they are any good. \n \nchange in culture [university-to-work] \n- work may feel like starting from the bottom as a beginner \n- work may have less feedback \n- work may have less autonomy \n- a mismatch between university skills and work requirements \n- work may require behavioural skills (presenting, oral speaking, selling a product) \n- students' inflated expectations due to unrealistic job previews and marketers \n \n3 \n\n \nmodule three: job satisfaction and alienation  \nalienation: karl marx argued that workers under capitalism were alienated from the product and process of their work, \nwere unable to use their inherent creativity, and were dominated economically and socially by the capitalist class. \n- \n(1) craftworkers had a high level of control, they owned the products that they made and sold, and controlled \nhow they went about the job, hence were not alienated. \n- \n(2) factory workers have a low level of control, work long hours in difficult unsafe environments, are heavily \nsupervised and therefore cannot express creativity, hence become subservient to a machine and as a result, \nbecome alienated. (the capitalist owns the means of production and products, pays you a wage, and hopes you'll \nsurvive) \nconditions that cause alienation \n- less common in an a democratic advanced revolutional society where working people vote for parties that \nimprove working conditions \n- more common in earlier stages of industrialisation: \n- in factories where conditions are poor \n- in societies where there is little protective labour legislation or poorly enforced \n- in conditions of chronic unemployment or loose labour markets. too many workers chasing too few \njobs, so there is little pressure on employers to improve working conditions \nloose labour market: employer is not under pressure to improve working conditions \ntight labour market: employer is under pressure to improve working conditions \n \nalienation is a continuum. on average in nz, people express a fair level of job satisfaction. those who are employed in the \nadvanced revolutional society with a high level of job satisfaction have high \nlife satisfaction. \n \njob satisfaction: appraising yourself, about the extent of liking or disliking a job you're doing. \nwhat drives job satisfaction? \n(1) intrinsic sources of satisfaction: how the job's tasks make you feel. eg: social environment \n(2) extrinsic sources of satisfaction: pay, level of security, social recognition, prospects of advancement \n \nintrinsic job satisfaction \nfinding a job that actually interests them. to find a job that is intrinsically interesting, even if it isn't one's life passion, work \nstill should be intrinsically interesting as well. \nskill utilisation. if someone has just finished a bachelors of computer science, they want to use these acquired skills in a \njob. the work needs to be complex enough to demand those skills, hence if too repetitive, people tend to have lower js \ndue to deskilling. hence complexity and variety is required to supplement skill utilisation. whatever level of education, can \nyou use the skills you have? meaning people of lower levels of education can be happy in their work. \ncontrol, autonomy, voice. how much control do you have over working methods and decisions? the higher level of control, \nthe more creativity comes out. \nworkload and working time. work intensification means workload is increasing for the time you have available. \ninstantiation reduces wellbeing by increasing stress, exhaustion and overload. or, are people intensifying their work \nenvironment themselves by listening to loud music in an understimulating environment. work extensification means work \ninvades personal life. \nrelations at work. supervised support (1) and (2) collegial support. are co-workers covering for you effectively? are \nsupervisors supporting you effectively? \n \nextrinsic job satisfaction \njob security. the reliability of money in your life. hence we can make commitments, support a spouse for example. \nfairness of pay \n(1) external equity: what are you worth in the labour market? \n(2) internal equity: all hierarchies of a position, how much more is the next level up worth? is that gap fair? \n(3) performance equity: are you rewarded appropriately if you perform better than colleagues doing the same? \nfringe benefits. kiwisaver, healthcare insurance benefits, superannuation, the extent you feel your job brings you esteem or \nstatus in society, advancement prospects. \n \none size does not fit all \n- people vary in the importance they place on the range of intrinsic and extrinsic factors \n- young people are likely to place high importance on gaining experience and career growth, they are more likely to \nleave a job if they are bored or blocked \n- people with higher level of financial aspiration or needs are likely to place greater emphasis on money \n- older people may place greater emphasis on a good social environment at work that on advancement \nopportunities or money \n- people may choose self-employment to maximise their autonomy and scope for creativity (but not necessarily \ntheir income)  \nmodule four (taylorism and fordism) and module five (sts and the dc model) part (a)  \n \nthe problem with piece rates & taylor's time study \nfederick winslow taylor (1865-1915) was an american engineer who regarded the working methods of his day as \n4 \n\n \ninefficient. he claimed, if people are paid by piece and managers realised they worked fast, managers simply decreased \ntheir pay per piece. forced to protect their own likelihoods, they work faster, leading to increased fatigue. as a protest, \npeople who felt exploited would restrict how much they did by working in groups formally or informally. (systematic \nsoldiering) \n \ntaylor noticed that systematic soldering wasn't good for the employer nor the worker, so he developed a technique called \ntime study, stands and watches a machinist and timed each task then looked at ways of recombining tasks to make them \nmore efficient. hence, reality was observed (empirical) \n \ntaylor's five principles \n1) shift responsibility for the organisation of work from the worker to the manager \n2) use scientific methods to determine the most efficient method \n3) select the best person to perform the job \n4) train the worker to do the work efficiently \n5) monitor worker performance \n \nthe controversies surrounding taylor's five principles [taylorism] \na) shifting from workers to managers separates the brain from the brawn. this tended to produce jobs with a \nnarrow range of tightly controlled tasks in short cycles, called \"de-skilling\". it strips workers of craft knowledge \nand autonomous control with fully thought-out labour processes in which they function as robots. \nb) scientific management was subject to an uproar in american society causing congressional investigations. he \ndefended himself arguing it was best for americans, but the american government did not want to accept \ntaylorism. although, other countries (russia and japan) trained workers in taylorism and had successful \nproduction. \ntime and motion study \nalongside taylor, frank (1868-1924) and lillian gilbreth (1878-1972) developed the motion study, inventing the \"therbligs\" \nsystem of classification of human motion. a system that analyses motion, which combined with taylorism to create the \ntime and motion study. \nhenry ford and mass production \nliked taylorism but added one critical improvement; the moving assembly line, meaning work came directly to the workers \non a converter belt to reduce wasted movement and sped up production. this doubled the wage and reduced high levels of \nemployee turnover. \n \nfordism: the ford company became an icon of american \"mass production\" for the \"mass market\". automobile ownership \nbecame affordable for the masses hence society benefited in material ways while industrial workers were often employed \nin alienating conditions. \n \nlean production: a production method focused on minimising waste while maximising efficiency. it was developed from \ntoyota’s production system and focuses on continuous improvement. \n \nconclusions \n● taylorism brought greater management control over working methods into the factory system: suiting \nmanagement drive to increase productivity in large-scale operations but created highly repetitive de-skilled jobs \n● forism combined taylorism with the moving assembly line, epitomising the american model of mass production \nfor mass markets. this has now evolved into learn production \n● there is a trade-off between growing consumer incomes in an industrial society and the quality of work in \nproduction jobs \n● taylorist ideas have spread to the growing service sector (mcdonaldisation). those negatively affected are the \nworkers whose skills are greater than the skills asked for in their jobs and who have greater learning potential. \n \nsocio-technical systems (sts) as an alternative to taylorism / fordism \ntries to deal with the way taylorism separated the preparation or conception of work from the execution of work. sts \nbrings back these conceptions into the workers job, bringing decision making to the lowest level in an organisation. \n \nminers often have control of how they dug out coal, helped each other, and how to protect each other's safety. they were \nresponsible for the level of output and have autonomy on how they went about it. we should not only maximise what's \ngood for production, but also what's good for humans as social animals (joint optimisation). \n \nthe demand-control (dc) model \n \nhigh decision-making low-strain jobs active jobs \nlow decision-making passive jobs high-strain jobs \n low job demands high job demands \nthose in high-strain jobs are chronically stressed with no outlet, which causes ailing physical and mental health. increased \n5 \n\n \nlearning due to higher control helps an individual cope with stress (active learning hypothesis). where control is limited, \nsocial support from supervisors and peers can help alleviate stress.  \nmodule five (b) (sts and dc model) and module six (eri and jdr models) \ndigital equipment corporation case at enfield [joint optimisation] \nhired into a group instead of individually, with no traditional assembly line. hence there is a democracy of how things work. \nmoving from autocratic to a more enabling supervising style. \n- group as a whole decides which jobs are divided up with a job rotation for variety to relieve you psychologically \n- continuation of taylorism and fordism with lean production \n- people doing the work, make the majority of decision themselves \n- multi-skilling and skill-based pay, hence incentive to increase skills and horizontal promotion \n \nthe effort-reward imbalance [by johannes siegrist] \nin employment, we look for a fair exchange between effort and reward. eri assesses the extrinsic satisfaction of a job. an \nimbalance is a high cost for a low gain, leading to dissatisfaction and looking for alternatives. a fair balance should lead to \ngreater satisfaction and better health. \n \nthe real world is a bit more complex. there exists exceptions to an unfair exchange. the following three conditions are as \nfollows: \n1) \ndependency: you and your spouse move to a new city, one of you has a great job, the other does not, putting up \nwith an imbalance for the sake of your relationship. \n2) \nstrategic choice: strategic in terms of building your cv and experience as an investment to reach a high level job. \nfor example, a wannabe filmmaker buying their way into the industry with long hours \n3) \nover-commitment: people who just enjoy over-giving, something about their personality who may want to \nover-work due to self-esteem, or other personal reasons \n \nthe job demand resource model [by bakker and demerouti] \nexplain the dc model is too simple for a more complex reality. they also explain that eri is good, but eri only assesses \nextrinsic satisfaction, and as we know, intrinsic satisfaction plays the other half. some demands are\n challenges (1) and some \ndemands are \nhindrances (2). \n1. physical demands \n2. psychological demands \n3. social demands \nchallenge demands: stimulates you at work, is encouraging to use and grow in ability \nhindrance demands: excessive or undesirable constraints that interfere with someone's ability to do their job \n- one person's challenge demand may be another person's hindrance demand. for example, journalists working \nagainst a deadline may find hindrance in time constraints with too many clients. however, for nurses, a rush of \npatients may be a challenge demand, more mentally stimulating and can catalyse growth \nresources \n- variety \n- feedback \n- control over working methods and workp[lace \n- rests and pauses \n- community, customers, supervisors, coworkers \n- organisational policies (holiday entitlement, promotion opportunity, wages/salaries, job security) \njob resources have motivational potential, they can enhance our engagement in work and help us fulfil our human needs \n(the motivational process). \n \njdr: buffering hypothesis \nthe more resources we have, the more you should be able to cope with higher levels of job demands without getting into a \nsituation of high stress or job strain. resources \"buffer\" the impacts of high job demands. \nmodule seven: work in standardised and simple services  \nwhat's different about services? \nservices are experiences rather than good products. for example, a cup of coffee at a cafe, a marriage celebrant, a maths' \ntutor, or a taxi ride. some services are automated but services are typically labour-intensive. service quality is a function of \nthe \ntangibles and intangibles in the service encounter. \n \nhotel star system [example] \n1) ⭐⭐⭐⭐⭐: highest level of accommodations and services, high degree of personal service, may offer elegant \nintimacy that cannot be achieved in larger settings, exclusive locations, room services, fitness centres, parking, \nconcierge etc \n2) ⭐⭐⭐⭐: mostly large formal hotels with smart reception areas, front desk services, located near other hotels \n6 \n\n \nof similar caliber, and usually found near shopping malls. some offer breakfast, room service, parking are also \noften provided \n3) ⭐⭐⭐: offer more spacious accommodations, bellhop services unavailable, medium-sized restaurants, room \nservices, pools, fitness  vary \n4) ⭐⭐: typically smaller and family owned, often 204 stories high at a more personal atmosphere. most will not \nhave restaurants on site, public access, but other restrictions may exist \n5) ⭐: usually denotes independent and name brand hotel chains with a reputation for offering consistent quality \namenities. limited services and room service are usually not provided. \n \nservices are often produced and consumed as and when customers demand them (\"simultaneous production and \nconsumption\"). they are often seasonal, subject to peaks and troughs of demand, as a result, managers try to match \nworkers' schedules to the needs of customers to maximise revenues and avoid idle time. \n \nservices vary along a continuum from low to high skill  \n←—-------—-------------------------------—-\n------------------------------—-----------------------------------------------------------------> \nstandardised and simple services differentiated services knowledge-intensive services \nfast food, retail, house cleaning higher-starred hotels, up-market \nretirement villages \nlaw firms, accounting firms, medical \nspecialists \n \nsimple services \n- offered to mass markets, often by global corporations or large local firms such as fast-food franchises, large \ndepartment stores and gas stations \n- this is the less-skilled end of services, customers often engage in self-service or co-produce the service, thus \nhelping reduce its cost. such as pumping your own gas, filling your own plate at a buffet, or self-checkout at \nsupermarkets \n- typically offered by individuals or small firms \n- many people can do this level of work, so wages are kept low \n- informal methods of management prevail, which can include worker exploitation in firms less visible to regulators \nand public scrutiny \n- when owner and employee get on well in a small firm, trust levels, flexibility and job satisfaction can be good \n(but maybe not so much for advancement opportunities) \nstandardisation \n- routinisation is the process of turning what you do into a routine, standardisation creates a uniform product \nwhere workers are required to perform a task in a uniform manner \n- this drives repeatability of business which increases efficiency \n- along with standardised training, a control exists, such as time spent doing a job is monitored, then standards can \nbe set to monitor employee behaviour. \n- eg1: mystery shoppers in retail; calling monitoring in call centres \n- eg2: algorithmic control in digital labour platforms \nworkers perspectives on control \nstress can be generated by \"speed-up\" standards, emotional labour during customer service roles, and burnout can all \nbecome prevalent when workers have little control over continuously stressful situations. however, scripting can also \nprotect a worker as it can bring order into a chaotic situation and can help protect employees. for example, recording of \ncustomer calls can help to protect employees from ill-founded complaints or unfair criticism. \n \nemployee turnover \nhigh turnover but recruiters don't mind a revolving door because training does not take very long, and only a low level of \nskill is required. accommodation and food services in nz is 49.4%, retail at 43.3% and an overall national average of 17.4%. \nmodule eight: work in knowledge-intensive services \ndaniel bell's (1976) upskilling thesis \nwe have moved away from an industrial into an advanced industrial society dominated by science and technical \nknowledge, and the advancement of that. where machines enable us to process data and knowledge as opposed from \nhumans doing the machinery themselves.  \n \nupskilling vs deskilling: it is agreed upon that work is dominated by service workers, not industrial workers. although, in the \nservice sector, we see polarisation. so yes, there is a proportion of upskilling for those in services, although we also see \npeople trapped in deskilling situations (skill atrophy). \n \nthe skill polarisation thesis \n1) expansion of highly-skilled, well-paid, non-routine work in the professions and other knowledge-intensive \noccupations \n2) hollowing out of mid-level work that can be routinised and computerised \n7 \n\n \n3) expansion of low-skilled service work providing services to wealthier people, including well-paid knowledge \nworkers. \n \njobs in usa 1979 \nlow skill medium skill high skill \n13.7% 61.1% 25.2% \njobs in usa 2016 \nlow skill medium skill high skill \n18.2% 43.2% 38.6% \nfrom 1979 to 2016, the medium skilled jobs shrank from 61.1% to 43.2%, while both low and high skill jobs increased. \n \nknowledge-intensive services [law, medical, accounting, dentistry, engineering] \n25% of new zealanders tell the census that they are professionals. hence, professional work is a major sphere in our \nsociety. \n● semi-professionals: advanced knowledge but don't have a monopoly of the work [hr] \n● para-professionals: work in restricted areas under supervision [paramedics, paralegals] \nthe classical professions \na profession is 'a high-status', knowledge-based occupation characterised by (1) abstract, specialised knowledge, (2) \nautonomy, (3) autonomy over clients and subordinate occupational groups and (4) a certain degree of altruism. \n \nabstract knowledge (1): in medical professions, they often specialise in joints, hips, orthopedic, etc. although, they are not \ncompetent in areas outside their expertise. professional knowledge is not simply theoretical. for example, good law \nstudents should have great ability to publicly speak. a lot of this variability comes with experience and learnt on the job, \nnot necessarily taught at university. \n \nautonomy (2): is expected in professional services. although, there are cases of unethical conduct by professionals \nmeaning there is a demand for regulation of professionals. there is a demand in society that autonomous professions still \nneed to be held accountable. [patient confidentiality, independence, honesty, integrity, non-discrimination for barristers] \n \nauthority (3): professionals expect compliance from their orders. for example, a doctor expects a patient to take their \nprescribed medication. this may bring territorial disputes between professions. for example, dentists, dental hygienists, \nregistered nurses and nurse aids. \n \naltruism (4): regards doing things for the good of society as a contribution as a professional. for example, adls gives free \nlegal advice to prisoners as a way to give back to society. \n \nprofessional service firms are in competition for \"talent\" \na failure to recruit and retain sufficient numbers of talented knowledge workers compromises the firm's ability to meet its \nclients needs and decreases its ability to grow: with consequences for profit.  \n \nconcerned for employment branding: their proposition to you to employ you. they may advertise training, quality and \nrange of intrinsic work experience, pay and career progression, international mobility, and work-life balance. \n \n \njob quality in professional work: a jdr interpretation \ndemands: \n- good challenge demands built into it, especially at higher levels; professionals often have interesting assignments \nand good skill utilisation \n- work pressures are generally high among professionals because their talents are in high demand and/or they are \nworking in an under-resourced system \n- managerial demands can be perceived as hindrances and these can generate protest from professionals. \nindustrial relations are often difficult in public health and education. \nresources: \n- experienced, trusted professionals have greater autonomy \n- professionals typically have good access to training and career advancement \n- professionals often have good voice opportunities \n- professionals are generally well paid and are very highly paid at the most senior or highest-performing rank. \n \nhigh resources low strain \nhigh motivation \n[professionals who reduce their hours] \naverage strain \nhigh motivation \n[typical position for professional work] \n8 \n\n \nlow resources low strain \naverage motivation \nhigh strain \nlow motivation [risk area: burnout] \n low demands high demands \nprofessionalisation \nthe process where an organisational group (like hr specialist) tries to become an occupational profession by trying to grow \nits professional standing in society. often seeking to have their education / training located in universities and need to \nconvince governments to pass a statute granting them exclusive or recognised rights to work. oppositions from existing \nprogressions can be expected. \ndeprofessionalisation \ngovernments and other sections of society try to reduce authority and autonomy by standardisation and deskilling their \nwork. they may hire less qualified people and greater technology can expand professional service at a lower cost. for \nexample, replacing fully trained teachers with less qualified temporary teachers, or replacing news reported with \ncomputers and social media. \nmodule nine (globalisation and mncs) and module ten (employee voice) \nthere exists both good and bad things about multinational corporations in the globalised economy, known as \"a world of \ncontrasts\". big firms are likely multidivisional (involved in an array of businesses) and/or multinational, expanding to \nmultiple  countries (mnc). \n \nmultinational strategies [what multinationals aim to do] \n1) they want to get resources. colonisation, for example, obtains resources that cannot be easily or cost-effectively \nobtained from origins. \n2) they take advantage of greater market opportunities than they have at origins. new zealand is limited in \npopulation, hence selling to china would bring greater revenue because there are far more people there. \nwhile many mnc may originate in america, britain, france, china, etc, it does not mean to say shareholders are completely \nloyal to producing in their own country. hence, production may occur outside of the states or china. they have loyalty to \nshareholders, not their country. \n \nmnc's visibility for criticism \nmnc come in for a lot of criticism because they are a lot more visible compared to small businesses. they are affected in \nterms of damaged reputation and consumer backlash if they are exploiting local conditions. hence, the average mnc is \nmore likely to be subscribing to international standards of humanity and rights than local small businesses. evidence, by \nlarge, shows people will be paid better by mncs compared to smaller or local businesses \n \nexample cases: [the fashion industry in gap and the perfume industry] \ndown the supply chain, abuses exist. although, they are under the spotlight, hence more mncs are accepting responsibility \nfor working conditions not only for their own plants by for those of their subcontractors (the entire supply chain) \n \ngap has an explicit policy on child labour where workers shall meet a legal minimum age using a rigorous age verification \nprocedure. in countries where no reliable documentation exists, the facility shall implement a recruitment system that \nappropriately assesses the age of potential workers. \n \nclaims that mnc's make \n- subscribed to internationally recognised codes of conduct \n- the ilo also has a policy on wages and benefits and conditions of work, to be similar to those of comparable \nemployers. \n- treaty of versailles in 1919 involved key international regulatory body on worker rights (ilo) \n \n1. freedom of association and effective recognition of the right to collective bargaining. this means \nfreedom to join or not join a trade union. if you do, you should be part of a collectively  negotiated \nagreement on safer and better conditions for worker rights. \n2. elimination of forced or compulsory labour \n3. abolition of child labour \n4. elimination of discrimination in respect of employment and occupation \n5. a safe and healthy working environment \n \nwork in the multinational firm [health and safety case - bangladesh building collapse] \n9 \n\n \nworld's worst disaster at rana plaza in bangladesh in 2013, where over 1000 workers died, has led to a range of responses \nby major mncs, including 'accord on fire and building safety in bangladesh' and a north-american 'alliance for bangladesh \nworker safety'. \n \nbiggest industrial accident globally in the garment industry. again, mncs were under the spotlight. it is actually quite \ndifficult to achieve social change in the way in which industries work. aizawa and tripathi explain: \n● the need for better financing to upgrade factories in a context where 'customers want ever-cheaper clothing' \n● the need for better government inspection of factories. regulations are in place but are they actually being \nfollowed? \n● mechanisms to deal with the conflicts of interest of legislators as shareholders.  \n● driving improvement in safety and other conditions through greater leadership from, and collaboration among, \nlocal stakeholders \n● fostering more independent trade unions \n \nmncs on the bright side \n- employee development: recruitment and selection of people for mncs may develop you in the multinational. \nextensive career development opportunities, developing indigenous managers, etc. the most sophisticated \ncareer development in the world, is by multinational corporations \n- \nunilever [case]: most admired mncs for its approach to management development. joining unilever allows for a \nvariety of experience rotations (good for double majors), moving you across divisions, products, markets and \ncountries. you meet a lot of people (networking) to help you in career development as well. \n \nmodule 10: employee voice \na trade union is an organisation that represents workers and advocates for their rights and interests. \n \nemployment relationship: based on interest in common (a growing enterprise gives employment opportunities and \ngrowing wages while company benefits in profitability. but there may be competing interests.  \n \nif the company needs a restructure, your job might be at risk, so your security is based on the company's survival. duty of \ndirectors of companies is to protect the capital of owners/shareholders, which may mean sacrificing the jobs of employees. \n \n \n \nwhat do trade unions do? \n1. act like a shield (defensive) to protect interests of workforce \n2. act like a sword (offensive), advances interests, tries to expand shares to employees \n3. unions are democracies, union members vote for the election of union officials \n4. try to improve substantial conditions of work and exercise influence in procedural arrangements of changing \nconditions of work \n5. legislative support for collective bargaining (as in nz law) is the main societal means for addressing the typical \npower imbalance in the employment relationship \ncollective bargaining often occurs around \n- pay and effort: wage levels, wage systems, hours and staffing levels \n- safety and security \n- job design and skills (but not that job quality is often traded for pay) \n- procedural rights: is the right an employee has to be consulted by an employer if something is to be changed by \nan employer. a proposal should be given to you if the company undergoes a merge, etc. \nwhy do people join unions? \n1. dissatisfaction / injustice model: people feel something is unjust or unfair, and want to do something about it \n2. you don't have to be dissatisfied, instead, you just have to see a benefit that is greater than the cost of joining the \ntrade union \n3. a political belief to join a trade union \n \nsense of threat to pay and \nconditions or dissatisfaction with \nthe job \n \n→ \npositive perception of the union: \nbelief that union will make a \ndifference \n \n→ \n \nunion joining \n10 \n\n \n    \n↑ \n    left-leaning political \nbeliefs \ntypes of employee voice: \ndirect \n● individual say over how, when, where to do job \n● team briefing \n● self-directed teams \n● engagement surveys \n● social media \n● individual rights to agree their terms  \n● individual rights to consultation during work \nrestructuring \nindirect \n● management-integrated consultative forums ● recognition of trade unions \n● legally mandated consultation through works \ncouncils or consultative committees \n \nvoluntary legally enforceable \nemployment involvement increases trust. however, a critical break causes a backfire effect, entering a situation of distrust. \nif people know opinions matter, expectations raise, resulting in employee anger. \n \nthe delta airlines case: levels of involvement: \ntop delta board council (top executive) \nmiddle five employee councils (flight attendant forum, technical operations employee council, etc) \nbottom continuous improvement teams (cits), as and when needed; employee councils at base \n \nwhy should management care about employee voice? \n- willing compliance with the law on employee rights will make the organisation a more legitimate member of the \nsociety (csr) \n- an open acknowledgement of the value of listening and responding to employee interests can also help to \nstabilise management power inside the workplace \n- voice systems can improve information flows and involve employees in improving productivity, quality and \nworking conditions in win/win ways \nmodule eleven: the future of work and of the workforce \ntrends & controversies associated with the future of work \nwork trends: rapid technological change. continuing advance of new technology and new business models. the first three \nindustrial revolutions were associated with \n(1) water and steam power in factories, (2) electrification of power, (3) ict or \nthe 'information age', and now we are entering the fourth \n(4) industrial revolution, a wireless interconnection of smart and \nintelligent systems, cyber physical systems to control and collaborate with each other. \n \nindustry 4.0 \nthe most dramatic change in work is the growth of ai, the shift of agency from humans to technology as technology \nbecomes more capable of self-directed learning. the other dramatic change is globalisation. increasing globalisation \nredistributed work around the world to more cost-effective sites. these things create new job opportunities, disrupt \nworking lives for some people (redundancies, relocations, retraining), and is a challenge to us to adapt to change \n(anticipated and unanticipated). \n \ncontroversies [technological change] \nto what extent will robotisation and smart machinery \nincrease? \nto what extent will ai spread across both manufacturing \nand services? \nfour years ago, mark mills, quoted in faulds and raju (2020) argued that ai will take another 15-20 years to make \nwidespread impact. fast forward to more recently, rajaram and tinguely (2024) explains that genai disrupts how \norganisations operate as it is the first technology to generate its own content rather than disseminate or support content \ncreated by humans. \n \n11 \n\n \nshould we have a guaranteed minimum income because of widespread substitution of machines for people? \nraises debate whether or not giving out free money acts as a disincentive to find work, and also, can we afford to give \neveryone money? why not target it to people who have a need, rather than to everyone. in nz, at age 65, you receive \nuniversal pension and healthcare.  \n \ncontroversies [work from home] \n● employees who work from home are dramatically more likely to quit than employees who routinely work in an \noffice environment as they do not have the same social connections. \n● it is more difficult to generate new ideas when people are working from home rather than in an office setting \nwhere you have moment of serendipity when people come together and share ideas \n● we see greater loneliness, isolation and fewer social connections from employees who are working from home, \nposing health risks \n● generating diversity issues: more men than women want to return to the workplace, physically present to exert \nmore influence and individuals who are only working from home \n● the flexibility from home working provides autonomy with schedules, but one issue is keeping employees \naccountable when they are working from home \n \nthe future of the workforce [three big trends in workforce diversity] \n1) greater female participation \nwomen without dependent children have high labour force participation. partnered mothers' employment has \nincreased, and sole mothers have also increased from 45% to 70%. \n2) greater ethnic diversity \nthose who identify as european have decreased and maori has increased. the biggest change is asian people, \nfrom 11.8% to 17.2%. in auckland, china and india are the largest groups, and korea and philippines have also \nincreased.  \n3) greater spread of ages in the workforce (ageing workforce) \na rise in 1 in 4 of the population of those aged 65+. nowadays, 65 is not considered old, they also have a higher \nlevel of employment compared to other age groups. in nz, 65 year olds have universal pension, but still have high \npaid employment participation. aging workforce does not mean people are stopping working, people continue to \nwork and the trend is rising. \ncourse review & exam preparation \nconcepts | module seven* \nlabour intensive services, which are 75-80% of the economy, are experiences that we choose to purchase (boxall \nand purcell 2022), some are automated but typically labour-intensive as opposed to products, \nwhich are capital intensive. \n- a high investment in human labour rather than machinery, equipment, or technology \nservice quality \n(tangibles and \nintangibles) \nany service or product is a function of the tangibles and intangibles. \n- tangibles: the physical item or outcome (a meal) \n- intangibles: the experience or quality of service (how good the customer service was) \na delicious meal (tangible) + great service (intangible) = a satisfying experience \na delicious meal + rude service = a bad experience despite good food. \nsimultaneous \nproduction and \nconsumption \nservices are often produced and consumed as and when customers demand them. simultaneous \nproduction and consumption means that a service is produced and consumed at the same time. \nunlike products, which can be made, stored, and used later, many services happen in real time \nzero-hours \ncontracts \n\"simultaneous production and consumption\" affects human resource management (staffing), \nbusinesses need to be able to scale up with temporary, casual and fixed-term workers when \ndemand increases, and need to be able to scale back down when demand is not there \n \nzero-hour contracting is the extreme version of this. the employers make zero promises of hours \nthey give an employee, workers are often called in with short-notice, and hourly payment starts \nwhen they arrive. this is now illegal in nz \nstandardised and \nsimple services \nservices vary along a continuum from low to high skill. standardised and simple services are at \nthe low-skill end of work. this includes fast-food, chain retail and house cleaning. \n12 \n\n \n- offered to a mass market, often by global corporations or large local firms such as \ninternational fast-food franchises, large department stores, gas stations. \n- most people can do simple work, making simple services easy to get into and keeps \nwages low \n- informal methods of management prevail, including worker exploitation in firms less \nvisible to regulators and public scrutiny \nwhen the owner and employee get on well in a small firm, trust levels, flexibility and job \nsatisfaction can be good, but what about advancement opportunities? \nsss has the highest employee turnover rates. although, turnover may be manageable. turnover \namong highly trained and experiences works entails substantial organisational costs \ndifferentiated \nservices \nare higher quality services, illustrated by the hotel star system, where differentiation from up \nfrom 1 to 5 stars. they are often tailored or modified to stand out from competitors by offering \nunique features, quality, or experiences \nco-production of \nservices \nsince simple services are at the less-skilled end of services, customers often engage in self-service \nor co-produce the service, thus helping to reduce its cost. such as pumping your own gas, buffet \nrestaurants, self-checkouts at supermarkets \nroutinisation is the process of turning what you do into a routine. for example, in mcdonald's, workers routine \nfood prep (same steps for each burger) and follow standardised procedures (fixed cooking times, \nportion sizes) to maximise efficiency \nstandardisation is the taylorist process of requiring everyone to do a task in a uniform way. managers in mass \nservices are interested in standardising services to increase efficiency and make quality more \nuniform in order to drive profitability and repeat business \nscripting or service \ninteractions \nscripting can be used to achieve standardisation. employees and customers may be controlled by \nict to reduce costs such as automated response options at government agencies and utility \ncompanies \n- scripting isn't always entirely negative. it can protect you by bringing order into a \nchaotic situation and can help protect employees. for example, recording of customer \ncalls can help to protect employees from ill-founded complaints or unfair criticism. \nemotional labour scripting may include some degree of emotional labour when customers are rude or abusive, \nfinding it frustrating to cope with the standardised system \nchronic stress, \nburnout \nchronic stress in a service job is known as burnout. it becomes more likely when workers have \nlittle control over continuously stressful situations (hodson and sullivan 2012) \ndigital labour \nplatforms \nare online platforms that connect workers with tasks or jobs, usually on a gig or freelance basis. \nthey act as intermediaries between businesses and workers. \nalgorithmic control managers seek to exercise control by setting standards for behaviour, output, time and \nmonitoring employee behaviour. algorithms manage, monitor, and direct workers’ tasks, \nperformance, and behaviour, often in digital labour platforms. \ncore/periphery \nmodels or \nemployment \n \nthis model shows core staff that we keep in the business during the year, and during seasonal \npeaks you pull in all types of people from the periphery who are of less value. we keep the \nbusiness alive through the trough by only keeping the core. \n \nprice's supermarket case study (robin price) \nthis case study analyses what is happening to the quality of jobs in an australian supermarket.  \n13 \n\n \n \nsupermarkets pursued business strategies designed to achieve economies of scale and reduce the cost of labour by \nadjusting staffing levels as tightly as possible to fluctuations in customer flows. \n \nretail wages were the second lowest in the australian labour market, although salaries of department and store managers \nare based on individual contracts with other employees on a collective agreement which reveals a high level of wage \ncompression (when the pay gap between lower-level and higher-level employees becomes smaller). \nthe margins for skill \nand the incentives for doing an apprenticeship are not great in this sector. this is because there is a weak career structure \non the shop floor, and increases in skill aren't paid highly. hence, when you've got good employees, you do not want to lose \nthis person to another department, although it would be in their interest for job rotation, reducing the variety of good \nworkers and therefore greater job satisfaction. \n \nthe lower the advancement opportunities, the more likely an employee will have higher intention to change occupations. \nalso, the more interesting you find the job, the less likely you are to change occupations \n(vocational preference). \n \nbakeries were small places where qualified bakers got up at awful hours in the morning to bake bread daily. supermarkets \nchange this. they have a centralised bakery of qualified bakers, premix bakery products, snap freeze, then distribute to \nstores to be heated up by bakery assistance, who have some training but do not need an apprenticeship. in effect, the \nbaking jobs in the stores have been deskilled to save costs and enhance profits. \n \nnew technologies and cost minimisation strategies in supermarkets have reduced the complexity of work and de-skilled a \nlarge proportion of workers. budget constraints limit worker internal mobility across departments, reducing the potential \nfor building skill variety. there is a core of qualified bakers but most workers in stores simply require some training in safety \nand the ability to follow simple instructions, such as refilling shelves, marking down aged products and removing perished \ngoods. shop floor supermarket workers have been deskilled and despite investment in accredited training, this deskilling \nappears set to continue. \nconcepts | module eight \nknowledge-intensive services all professional service firms, such as law firms, accounting firms, medical specialists. \ninformation society \nbell's upskilling thesis \nthe rise of information workers (working with knowledge and data rather than hands). \ntheory that information workers will displace industrial work in factories, and will \nbecome the largest single group in the workforce, along with a larger skill level in \nsociety. \ncritics: however, much service work is subject to routinisation and intensification. \npost-braverman research shows computerisation has taylorised or deskilled work to \nsome extent. \nskill polarisation upskilling and deskilling are both occurring. people who want to use more of their skills \nand keep growing need to avoid situations where they are caught in a deskilling \ndynamic. \n1. expansion of highly-skilled, well-paid, non-routine work in the professions \nand other knowledge-intensive occupations \n2. replacing mid-level work with routinisation and computerisation \n3. expansion of low-skilled service work providing services to wealthier people, \nincluding well-paid knowledge workers \nthe result is a workforce with fewer opportunities for well-paying, mid-level jobs. \nprofessions ('hallmarks \ndefinition': abstract \nknowledge, autonomy, \nauthority, altruism) \n1. abstract knowledge: theoretical knowledge from university, continuously \ndeveloped throughout a career. \n2. autonomy: expected that you have a degree of control you express in a job \n3. authority: expect compliance with their orders from clients \n4. altruism: being prepared to use professional knowledge and skills to serve \ngood of society \nsemi-professionals companies employ hr people and they decide whether they'll give you a job as an hr \nspecialist. hence hr people do not have a monopoly. workers who perform tasks \nrequiring some level of formal education or training, but not necessarily a full \nprofessional qualification \npara-professionals paralegals and paramedics who work under strict supervision of higher professionals. \nworkers who assist professionals by carrying out tasks under supervision, often with \nless formal training than semi-professionals \n14 \n\n \n'finders, minders, grinders' \n- partners (finders) find the work, retain clients, expand practice and build \nbusiness. \n- managers (minders), look after entry level \n- staff accountants (grinders) \ntalent wars knowledge intensive occupations have competition for best talent. firms can be \nobserved doing certain things to retain talented individuals.  \nemployer brand and \nemployment value \nproposition \ncompanies try to develop an 'employer brand' to sell to talented workers by advertising \ntheir strengths in training, pay, career progression, international mobility and work-life \nbalance. \nprofessionalisation the process in which an occupational specialty seeks to emulate a profession. they \nseek to have their education location in universities and need to convince governments \nto pass a statute granting them exclusive or recognised rights to the work. opposition \nfrom existing professions can be expected. \ndeprofessionalisation strategy of reducing the authority and autonomy of professionals through standardising \nthe deskilling their work. increasing regulation can erode professional autonomy, \nemploying less qualified people and greater technology can expand professional \nservices at a lower cost. \n \njob quality in professional work [a jdr interpretation] \ndemands: \n- professional work often has good challenge demands built in, especially at higher levels; professionals often have \ninteresting assignments and good skill utilisation \n- work pressures are generally high among professionals because their talents are high demand or they are \nworking in an under-resourced system \n- managerial demands can be perceived as hindrances and these can generate protest from professionals. \nindustrial relations are often difficult in public health and education. \nresources: \n- experience, trusted professionals have greater autonomy \n- professionals typically have access to training and career advancement \n- professionals often have good voice opportunities \n- professionals are generally well paid and are very highly paid at the most senior or highest-performing levels \nconcepts | module nine \nmultinational corporation \n(mnc) \ncompanies that operate across more than one country. they are powerful firms \nseeking scarce and cost-effective resources and expanding markets globally \nglobalisation the increasing interconnectedness of the world. although progress is mixed and there \nare setbacks, the liberalisation of markets, privatisation, the reshaping of communism, \nimproved modes of transport and the advent of the interest, have all played their part \nin opening up the world to higher levels of trade in goods, services and ideas. \n \nmncs are key drivers of globalisation, benefiting from open markets while facing \nchallenges related to ethical practices and public perception \nworld trade organisation is an international organisation that deals with the rules of trade between nations. its \ngoal is to promote smooth and fair trade by ensuring that goods and services flow \nfreely across borders. \nthe spotlight phenomenon mnc are much more visible in the news and can be affected in terms of damaged \nreputation and consumer backlash if they are exploiting local conditions in a way that \noffends international standards for human rights or humane conditions of work \ncode of conduct \nilo and un global compact \nmnc subscribes to the well-known standards of conduct such as the ilo and the un \nglobal compact. balancing profitability with ethical responsibility is crucial, as \nnon-compliance with global standards can harm their reputation and market position \nexpatriates mncs have employee development including the selection and management of \nindividuals to serve abroad as expatriates. \n15 \n\n \nemployee and management \ndevelopment \nmnc's have great career development and graduate programmes \ntriple spital of career \nmovement \nunilever: after graduate trainees are recruited, the career movement for the best takes \non a triple spatial: between functions, divisions and countries. \nmanagement cadre the management of the management cadre is based on building elaborate networks \nand relationships. \nconcepts | module ten \ntrade unions the relationship you and an employer have is a blend of common and competing \ninterests. there is a common interest in growth. for conflicting interests, if business \nmodels change (re-structuring, mergers, acquisitions), an employee's security is at risk. \n- unions are collective organisations to (1) protect the interest of the \nworkforce and (2) advance interests to get employees more. \n- unlike companies with management hierarchies, unions do not have \nmanagement hierarchies, they are constituted as democracies. \ncollective bargaining unions seek to negotiate the substantive conditions of work and the procedural rights \nassociated with worker voice \npower imbalance legislative support for collective bargaining (as in nz law) is the main societal means \nfor addressing the typical power imbalance in the employment relationship \nsubstantive rights \nprocedural rights \na collective agreement contains agreed rules on wages and conditions ('substantive \nrights' but also rules on how the rules can be changes ('procedural rights') \ninjustice the traditional motive for people to join unions because there are certain conditions in \ntheir job that they feel as injustice or dissatisfaction.  \nutility, instrumentality is another motive for people to join trade unions. these people simply see a benefit \ngreater than the cost of joining a trade union. \nideological motives/political \nbeliefs \nis another motive for people joining trade unions. their motive is driven by political \nbelief or background \nemployee voice (direct, \nindirect/representative, \nvoluntary, legally enforceable) \n- indirect voice, managers inform, consult and negotiate with representatives \nelected by employees or appointed by trade unions. \n- direct voice is at the individual level, leverage and influence depends on how \nskilled the person is, or how involved they are, the more your opinion \nmatters \nbackfire effect the risks of strong employee involvement and voice is the backfire effect. when raising \nexpectations of people's involvement, then they come to expect it, hence raising \nexpectations. \ncorporate social responsibility one key reason to take into account employee voice is csr. willing compliance with \nthe law on employee rights will make the organisation a more legitimate member of \nsociety or societies in which it operates. an open acknowledgement of the value of \nlistening and responding to employee interests can also help stabilise management \npower in the workplace. voice systems also improve information flows and involve \nemployees in improving productivity, quality and working conditions. \nconcepts | module eleven \nfourth industrial revolution (industry \n4.0) \nthe innovations associated with industry 4.0 merge advanced manufacturing \nand ict. there is better visibility and insight into the company's operations \nand assets through integration of machine sensors, middleware, software, and \nbackend cloud computing and storage systems. \ndigitisation or digitalisation \n- digitisation: converting physical or analogue information into a \n16 \n\n \ndigital format (e.g., scanning a paper document to create a pdf). \n- digitalisation: using digital technologies to improve or transform \nprocesses (e.g., automating a paper-based workflow using digital \ntools). \nartificial intelligence the growth of ai is something that involves the shift of humans to technology \nas it becomes more capable of what we do as humans, but self-directed \nlearning. \nbig data big data can be analysed at high speed in cloud-based computer services to \ngenerate better information \nai algorithms complex mathematical formulas to allow complex iterations on complex \nprocesses as a positive view. based on the motor car case. it was disrupting \ntrains and horses, but created more jobs than it destroyed \ngenai rajaram and tinguely (2024) claim genai will disrupt organisations in how \nthey operated as its the first technology to generate its own content rather \nthan disseminate or support content created by humans \nguaranteed minimum income \n(universal basic income) \n- would this impact positively or negatively on incentives to work? \n- could society afford it? would financial support be better if targeted \nto those disrupted or most in need (as in present welfare \ninitiatives?) \ngig economy the gig economy is a labour market where people work temporary, flexible, or \nfreelance jobs instead of full-time employment. these jobs are often arranged \nthrough digital platforms like uber, fiverr, or upwork. \nteleworking; home and hybrid working \nworkforce diversity (e.g. gender, \nethnicity, ageing) \nthe growth of remote ways of working is known as teleworking for those who \ncan and want to is a trend that was accelerated by responses to the covid-19 \npandemic but is now being questions  by some employers \n- home and hybrid work bring cost savings to employers and \nemployees \n- employees working from home are more likely to quit due to \nunequal social connection opportunity \n- creating problems of lower collaboration and innovation  \n- causing higher levels of social isolation and poorer health \n- gender diversity issues? \n- encouraging greater flexibility and employee autonomy or greater \nsurveillance? \nequal employment opportunity governments can respond to inclusion: \n1) legislation \n2) education \n3) role modelling \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n17 \n\n \n \n \n \ntheories \n[a] explain terminology [b] claims, principles or hypothesis [c] strengths/weaknesses \n1. holland’s theory of vocational preferences/personalities (module two) \n2. the demand-control model (a theory of job quality, module five) \n3. the effort-reward imbalance model (a theory of job quality, module six) \n4. the job demands-resources model (a theory of job quality, module six) \n5. the theory of union joining (module ten)\n \ntrends  \n[a] explain and discuss implications \n1. skill polarisation: upskilling and deskilling (module eight and eleven) \n2. professionalisation and deprofessionalisation (module eight) \n3. globalisation (module nine and eleven) \n4. increasing application of technology to work (module one and eleven) \n5. increasing workforce diversity: gender, ethnicity, age, ... (module eleven) \ncontroversies \n[a] explain various sides of debates [b] considering available evidence [c] form reasonable views on \nthem \n1. how have taylorism and fordism affect job quality and worker well-being? can we design \njobs in ways that are better for workers and organisations or, at least, better for workers \nwithout harming organisations? (modules four, five, six, seven, eight) \n2. what are the impacts of multinationals on worker rights and working conditions? are \nmultinationals improving the quality of work? (module nine) \n3. are unions still relevant to workers? to some workers more than others? (module ten). \nshould management promote or encourage employee voice? if so, how? (module ten) \n4. in the face of rapid technological change and globalization, what is the future of work? \n(will jobs decline or grow? will they become better or worse? which knowledge/skills will \nbecome more important and which less so? should we have a guaranteed minimum \nincome/universal basic income? (modules nine and eleven) \n5. how does home-based and hybrid working affect organisational performance and \nemployee well-being? (module eleven) \n \n \n \n \n \n \n \n18 \n\n \n \n \n \nword count: 1200 words \nintroduction / conclusion template \njanet’s job \nquality and \nrecommendati\nons \njanet’s situation highlights the challenges many professionals face in high-pressure careers, \nparticularly in law. while she finds her work intellectually stimulating and rewarding, the demanding \nhours and work-life imbalance are taking a toll on her well-being. this essay will analyse janet’s job \nquality using relevant theories, such as the job characteristics model and maslow’s hierarchy of \nneeds, to evaluate her intrinsic and extrinsic motivators. additionally, it will provide \nrecommendations for how janet can address her exhaustion and negotiate better working conditions \nto achieve a sustainable balance between her career and personal life. \nin conclusion, janet’s job offers significant opportunities for growth and skill development, but the \nexcessive workload and lack of work-life balance are undermining her well-being. by setting clear \nboundaries, seeking support from her manager, and prioritizing self-care, janet can mitigate burnout \nwhile continuing to thrive in her legal career. ultimately, addressing these challenges proactively will \nnot only improve her job satisfaction but also enhance her long-term productivity and success in the \nfirm. \nmnc's  and \nworker \noutcomes \nmultinational corporations (mncs) wield significant power over global labor markets, often \ninfluencing pay, working conditions, and employee development. while some argue that mncs \nexploit workers, particularly in developing countries, others contend that they bring economic \ngrowth and improved standards. this essay will critically evaluate the extent to which mncs can act \nwithout accountability and the resulting impact on workers. using examples and theories such as \nglobal labor standards and stakeholder theory, it will assess whether the outcomes for workers are as \ndetrimental as the statement suggests. \nin conclusion, while mncs have the capacity to improve working conditions and provide \ndevelopment opportunities, their actions often prioritize profit over worker well-being, particularly in \nregions with weak labor regulations. however, the growing influence of global standards and \ncorporate social responsibility initiatives suggests that mncs are not entirely free to act as they \nplease. to ensure better outcomes for workers, stronger regulatory frameworks and ethical \nleadership are essential to hold mncs accountable for their practices. \nmanaging a \ndiverse \nworkforce in \nnew zealand \nas new zealand’s workforce becomes increasingly diverse, managers must adapt their practices to \nfoster inclusivity and leverage the benefits of diversity. this essay will explore key issues such as \nunconscious bias, cultural differences, and communication barriers, as well as concepts like equity \nand inclusion. it will recommend actionable strategies, such as diversity training and inclusive \nleadership, to help managers create a workplace where all employees feel valued and empowered. \nthese actions are crucial not only for ethical reasons but also for enhancing organizational \nperformance and innovation. \nin conclusion, managing a diverse workforce requires a proactive and thoughtful approach from \nmanagers. by understanding the challenges and implementing inclusive practices, organizations can \nunlock the full potential of their employees. embracing diversity is not just a moral imperative but \nalso a strategic advantage, as it drives creativity, problem-solving, and competitiveness in an \nincreasingly globalized world. \njohn’s union \nnegotiations \njohn’s situation underscores the importance of effective union-management relationships in \nmaintaining workplace harmony and productivity. with 60% of his workforce now unionized, john \nmust approach the upcoming negotiations with a collaborative mindset to avoid disruptions and \nbuild trust. this essay will advise john on strategies such as open communication, mutual respect, \nand transparency during the collective bargaining process. by fostering a positive relationship with \nthe union, john can ensure a fair agreement that benefits both the employees and the organization. \nin conclusion, john’s ability to create and maintain a good relationship with the union will depend on \nhis willingness to engage in open dialogue and prioritize mutual interests. by adopting a cooperative \napproach and demonstrating respect for the union’s role, john can negotiate a collective agreement \nthat minimizes conflict and supports the long-term success of his organization. a strong \nunion-management relationship is not only beneficial for employee morale but also essential for \nsustaining growth and stability in the firm. \n \n19 \n\n",
  "230": "\n\n \nsingleton class \nclass logger { \n   private list<string> logs; \n   private static logger instance; \n   private logger() {this.logs = new arraylist<string>();} \n   public static logger getinstance() { \n      if (instance == null) {instance = new logger();}  \n         return instance;} \n   public void log(string message) {this.logs.add(message);} \n   public arraylist<string> getlogs(){...}} \ncomposite pattern  \nprivate double volume = 0.0; \npublic double getcabinvolume() { \n    for (officespace o : childspaces) { \n        if (o instanceof cabin) { \n            cabin child = (cabin) o; \n            volume += child.getcabinvolume();} \n        else { \n compositespace cs = (compositespace) o; \n volume += cs.getcabinvolume(); \n } } return this.volume; } \nexceptions \narrayindexoutofboundsexception. \nclasscastexception \nnumberformatexception integer.parseint()  invalid parsable \nformat \narithmeticexception division by zero \nillegalargumentexception inappropriate argument is passed to \na method \nnosuchelementexception element is requested but not found \n.next() \nchecking if string is numeric / letter \ncharacter.isdigit(string.charat(index)) \ncharacter.isletter(string.charat(index)) \ntemplate pattern \nabstract class ...{ \n    protected string ...= \"\"; \n    public final void ...(string line) { \n        readinput(line); \n        operate(); \n        writeoutput(); } \n    abstract void readinput(string line); \n    abstract void operate(); \n    private void writeoutput() {...}} \nlinked list iterator \n    private class linkedlistiterator implements \niterator<listnode> { \n        private listnode current; \n        public linkedlistiterator() {  this.current = head;     } \n        public boolean hasnext() {  return current != null;  } \n        public listnode next() { \n            if (!hasnext()) { throw new \njava.util.nosuchelementexception(); } \n            listnode temp = current; \n            current = current.getnext(); \n            return temp; } } \nobservable \npublic void update(observable o, object arg) { \n    if (o instanceof parkingspace) { \n       parkingspace space = (parkingspace) o; \n       system.out.println(string.format(\"the current status for parking slot %s is true\", space.getid())); \n       if (space.getstatus()) { \n              if (_parkingtypeneeded == ((parkingslot) space).gettype()) { \n                     double charge = math.round(space.getrent() * _hours ); \n                     system.out.println(string.format(\"parking slot %s is available to customer %s for $%.1f\", \nspace.getid(), _name, charge)); \n               else { \n                     system.out.println(string.format(\"parking slot %s is not available to customer %s\", \nspace.getid(), _name));} }}} \nfile reading \n   try { \n      file myobj = new file(\"filename.txt\"); \n      scanner myreader = new scanner(myobj); \n      while (myreader.hasnextline()) { \n        string data = myreader.nextline(); \n        system.out.println(data); \n      }myreader.close(); \n    } catch (filenotfoundexception e) { \n      system.out.println(...);  e.printstacktrace(); } \nswingworker <long, void> \nprivate int _n; private int _r; \npublic permutationsworker() {} \nprotected long doinbackground() { \n    long x = factorial(_n); \n    long z = factorial(_n - _r); \n    return x / z;} \npublic long factorial(int n){ \n     long total = 1l;     \n     for (int i=1; i<=n; i++){ total *= i;    }  return total;} \nprotected void done() { \n        try {  long result = get();  ....settext(...); \n        } catch (...) {...       }  }} \ncourse[1..5] → student[0..10] \nclass student { \n    private static final int max.. = 5; \n    private list<course> enrolledcourses; \n    private string name; \n \n    public student(string name, course course) { \n        this.name = name; \n        this.enrolledcourses = new arraylist<>(); \n        if (!course.enrollstudent(this)) {throw new ...  } \n        enrolledcourses.add(course); } \n \n    public boolean enrollcourse(course course) { \n        if (enrolledcourses.size() >= max || \n!course.enrollstudent(this)) {return false;   } \n        enrolledcourses.add(course);  return true;  }} \nclass course { \n    private static final int max.. = 10; \n    private list<student> students;     \n    private string title; \n \n    public course(string title) { \n        students = new arraylist<student>(); \n        this.title = title; } \n \n    boolean enrollstudent(student student) {         \n        if (students.size() < max...) { \n            students.add(student); \n            return true;  }  return false; }} \nactor[0..3] → student[0..*] \nclass actor { \n    private list<movie> movies = new arraylist<movie>(); \n    string name; \n    public actor(string name) {this.name = name;}  \n    public boolean signupfor(movie movie) { \n        if (movie.getactors().contains(this) && !movies.contains(movie)) { \n            movies.add(movie); \n            return true; }return false; }} \nclass movie { \n    private static final int max_actors = 3; \n    private list<actor> actors = new arraylist<actor>(); \n    private string name; \n    public movie(string name) {this.name = name;}  \n    public boolean signup(actor actor) { \n        if (actors.size() < max_actors && !actors.contains(actor)) { \n            actors.add(actor); \n            return true; } return false; }} \niterator  \nprivate class ascendingiterator implements iterator<integer> { \n      private int index = 0; // current index in the iterator \n      private integer nextvalue = getnextvalue(); // get the first ascending value \n \n      private integer getnextvalue() { \n          while (index < numbers.size()) { \n              integer value = numbers.get(index); \n              index++; \n              if (value != null && (index == 1 || value > nextvalue)) { \n                  return value; }   } return null; // no more ascending values \n      } \n      public boolean hasnext() { \n          return nextvalue != null;  } \n \n      public integer next() { \n          if (!hasnext()) { \n              throw new java.util.nosuchelementexception(); \n          } \n          integer returnvalue = nextvalue; // store the current value \n          nextvalue = getnextvalue(); // prepare the next ascending value \n          return returnvalue; // return the current value  } } \n \n \n\nassociation types \ndependency definition: a temporary relationship where one class depends on another to function (typically for a short time). \nexample: a car needs fuel to drive. the car does not own the fuel, but it needs it. \n \nthread life cycles \nthread.sleep() or  thread.wait(long timeout) = time waiting \nthread.start() = runnable \nthread.currentthread().interrupt() = terminated \nthread t = new thread: new born \nsynchronized (lock): blocked \nrobustness \nsoftware is robust if it can handle unusual conditions such as \ncorrupt data, user error, programmer error, and environmental \nconditions. \n- use of polymorphism \n- use of protected fields \n- use of type-safety: list<staff> = arraylist<staff> \n- exception handling \n- interfaces \n- access modifiers / encapsulation \nassociation definition: a basic \"has-a\" relationship between classes where one class uses or references another. \nexample: a driver uses a car. the car may exist without the driver. \ngeneralization definition: represents an \"is-a\" relationship, typically used in inheritance. all uses of inheritance where the superclass is a concrete class \nexample: a dog is an animal. here, dog inherits from animal. \n \nrealization all uses of inheritance where the superclass is an abstract class or interface. \none model element realizes (implements or executes) the behavior that the other model element (the supplier) specifies \n \naggregation definition: a \"has-a\" relationship where the contained object (part) can exist independently of the container (whole). \nexample: a school has students. if the school is closed, students still exist. \ncomposition definition: a stronger \"has-a\" relationship where the contained object cannot exist independently of the container. when the container is \ndestroyed, the contained object is too. example: a house has rooms. if the house is destroyed, the rooms are destroyed too. \n \nsolid design principles \nsingle responsibility \nprinciple \nclass stationery{} \nclass print{} \nclass scan{} \nclass copy{} \nclass stationery{ \n    public void print() {} \n    public void scan() {} \n    public void copy() {} }\n \nopen-closed principle \ninterface shape {double area();} \nclass circle implements shape {public double area() {}} \nclass square implements shape {public double area() {}} \nclass areacalculator {}} \nclass shape { \n    public double area(string shapetype)  }} \nsoftware entities (like classes, modules, and functions) \nshould be open for extension but closed for modification. \nthis means we should be able to add new functionality to \na class without altering its existing implementation \nliskov substitution principle \nclass bird {public void move(){}} \nclass penguin extends bird{public void move(){}} \nclass sparrow extends bird{public void move(){}} \nclass bird {public void fly() {} } \nclass penguin extends bird{public void fly() {} } \nobjects of a superclass should be replaceable \nwith objects of a subclass without altering the \ncorrectness of the program. in other words, \nsubclasses should behave in a way that is \nconsistent with the expectations set by their \nsuperclass \ninterface segregation principle \ninterface eater{ void eat();} \ninterface flyer {void fly();} \nclass dog implements eater {public void eat() { }} \nclass bird implements eater, flyer {public void eat(){} public \nvoid fly(){}} \ninterface animal{void eat(); void fly();} \nclass dog implements animal { \n    public void eat() {} \n    public void fly() {}}\n \ndependency inversion \nprinciple \nclass order{ paymentmethod payment; } \nabstract class paymentmethod{} \nclass creditcard extends paymentmethod{} \nclass debitcard extends paymentmethod{} \nclass order{ \n    creditcard debit; creditcard credit; } \nclass creditcard {} \nclass debitcard {}\n \ncomposite design pattern: implementing the composite pattern lets clients treat individual objects and compositions uniformly. template-method design pattern: defines the program skeleton of an algorithm in an operation, \ndeferring some steps to subclasses. this allows subclasses to redefine certain steps of the algorithm without altering its overall structure. the concrete class  implements the primitive operations to carry out subclass-specific \nsteps of the algorithm. it’s fairly easy to create concrete implementations of an algorithm because you’re removing common parts of the problem domain by the use of an abstract class. clean code because you avoid duplicate \ncode and we separate the algorithm into private methods/functions. observer pattern design: the observer pattern facilitates a one-to-many relationship between objects, ensuring that when one object changes, its dependent \nobjects are notified automatically. subject - represents the core abstraction. observer represents the variable (or dependent or optional or user interface) abstraction. observer - represents the variable abstraction. each observer \ncan call back to the subject as needed. factory method design pattern: factory method is a creational design pattern that provides an interface for creating objects in a superclass, but allows subclasses to alter the type of objects \nthat will be created. the factory method pattern suggests that you replace direct object construction calls (using the new operator) with calls to a special factory method. don’t worry: the objects are still created via the new \noperator, but it’s being called from within the factory method. objects returned by a factory method are often referred to as products.realization vs generalization: all uses of inheritance where the superclass is a concrete class, \nthe relationship between the superclass and subclass is known as generalization.  all uses of inheritance where the superclass is an abstract class or interface, the relationship between the superclass and subclass is known as \nrealization. \ndefaulttreemodel stack example \npublic void buildtree(string expression) { \n    stack<defaultmutabletreenode> stack = new stack<>(); \n    for (char ch : expression.tochararray()) { \n        if (character.isdigit(ch)) { \n            defaultmutabletreenode node = new defaultmutabletreenode(ch);stack.push(node);  } else if (ch == '+' || ch == '-' || ch == '*') { \n            defaultmutabletreenode rightnode = stack.pop(); // right child \n            defaultmutabletreenode leftnode = stack.pop();  // left child \n            defaultmutabletreenode operatornode = new defaultmutabletreenode(ch); \n            operatornode.add(leftnode); \n            operatornode.add(rightnode); \n            stack.push(operatornode); }}    root = stack.pop();} \n\n\n\n \n \n \n \n \n \n \n             \nformatting \n\\t - insert a tab in the text at this point. \n\\b - insert a backspace in the text at this point. \n\\n - insert a newline in the text at this point. \n\\r - insert a carriage return in the text at this point. \n\\' - insert a single quote character in the text at this point. \n\\\" - insert a double quote character in the text at this point. \n\\\\ - insert a backslash character in the text at this point. \n \nconversions \nint → string | string.valueof(int)  \nstring → int | integer.parseint(string)  \nstring → integer | integer.valueof(string)  \ndouble → string | string.valueof(double) \nstring → double | double.parsedouble(string)  \nchar → string | string.valueof(char) \nstring → char | string.charat(index)  \ndouble → int | (int) data \nchar → ascii | char character = 'a';  \n int asciivalue = (int) character; \n \nwidening primitive conversion \nbyte → short, int, long, float, or double \nshort→  int, long, float, or double \nchar → int, long, float, or double \nint → long, float, or double \nlong → float or double \nfloat → double \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \n \n \n(iterator) class extends parent (iterator) no extension \nclass garden extends gardenblock implements iterable<gardenblock> { \n    private arraylist<gardenblock> blocks = new arraylist<gardenblock>(); \n    public garden() { super(100, 100); } \n    public void addgardenblock(gardenblock b){ ...} \n    public int getgardenarea(){ ...} \n    public string tostring() { ...} \n    public iterator<gardenblock> iterator() { \n        collections.sort(blocks);   // sorts list before iteration \n        return new blockiterator(blocks); } \n} \nclass blockiterator implements iterator<gardenblock> { \n    private int nextindex = 0; \n    private arraylist<gardenblock> blocks; \n    public blockiterator(arraylist<gardenblock> mylist) { \n        this.blocks = mylist; } \n    public boolean hasnext() {return nextindex < blocks.size();} \n    public gardenblock next() { \n        if (!hasnext()) {throw new nosuchelementexception();} \n        return blocks.get(nextindex++); }} \nclass basicuniqueeven implements iterable<integer>{ \n    private arraylist<integer> items = new arraylist<integer>(); \n    public basicuniqueeven() {} \n    public boolean add(int number) { ...} \n    public void addall(int[] array){ ...} \n    public void removebyvalue(int number){ ...} \n    public boolean contains(int number) { ...} \n    public string tostring() { ...} \n        // extra methods to support iteration \n    public int size() {return items.size();} \n    public integer get(int index) {return items.get(index);} \n    public iterator<integer> iterator() {return new uniqueeveniterator(this); } \n} \nclass uniqueeveniterator implements iterator<integer> { \n    private int nextindex = 0; \n    private basicuniqueeven list; \n    public uniqueeveniterator(basicuniqueeven list) {this.list = list; } \n    public boolean hasnext() {return nextindex < list.size(); } \n    public integer next() {return list.get(nextindex++); }} \n \niterate through 2d array \nfor (int i = 0; i < array.length; i++) {           \n            for (int j = 0; j < array[i].length; j++) {     \n                system.out.print(array[i][j] + \" \"); \n            } \nmodify arraylist in place \nfor (int i = 0; i < list.size(); i++) { \n            list.set(i, list.get(i) * 2);   \n        } \nsimple enum \nenum shapeperimeter { \n triangle(3), \n square(4), \n pentagon(5), \n private int numberofsides; \n private shapeperimeter(int sides) {this.numberofsides= sides;} \n public int getperimeter(int sidelength) {return numberofsides * \nsidelength;} \nshapeperimeter.ordinal() → returns position of enum object \nobject.name() → returns name of enum \n(triangle/square/pentagon) \ngetindexoflargestodd method \npublic static int getindexoflargestodd(int[] numbers) { \n  int largestodd = -1; \n  int index = -1; \n  for (int value: numbers){if (value%2 == 1 && value > largestodd) { \nlargestodd = value; } } \n  for (int i=0; i<numbers.length; i++){ if (numbers[i] == largestodd) {return \ni;}} \n  return -1;} \ninterface financialaideligible { } \nabstract class person { \n  int id; \n  public int getid() { return id; } \n} \nclass student extends person {} \nclass undergraduate extends student implements financialaideligible {} \n \nfinancialaideligible p1 = new undergraduate(); legal \nthe undergraduate class implements the financialaideligible interface. this means that an undergraduate object can be assigned to a variable of type \nfinancialaideligible p2 = new financialaideligible(); compile-time error \nfinancialaideligible is an interface, and you cannot instantiate an interface directly in java. \nfinancialaideligible p3 = new student(); compile-time error \nthe student class does not implement the financialaideligible interface \nfinancialaideligible[] people = new financialaideligible[10]; legal \nin java, you can create an array of any reference type, including interfaces. \nother arraylist methods \nsublist(int fromindex, int toindex): returns a view of the portion of the list between fromindex, inclusive, and toindex, exclusive. \nremove(int index): removes the element at the specified position in the list. \nremove(object o): removes the first occurrence of the specified element from the list. \nset(int index, e element): replaces the element at the specified position with the specified element. \ntoarray(): returns an array containing all elements in the list. \nisempty(): returns true if the list contains no elements. \nget(int index): returns the element at the specified position in the list. \nindexof(object o): returns the index of the first occurrence of the specified element, or -1 if not found. \nlastindexof(object o): returns the index of the last occurrence of the specified element, or -1 if not found. \nadd(int index, e element): inserts the specified element at the specified position in the list. \n \n\n",
  "235": "",
  "300": "\n\nbrianna yeung (byeu337) \ns1 2025 busan 300 test 2 | python \nbusan 300 - python \nset-up.............................................................................................................................................2 \nmount google drive.........................................................................................................................2 \nread in dataset.................................................................................................................................2 \n.loc[] and .iloc[]...............................................................................................................................2 \nsimple queries................................................................................................................................2 \nselect a column................................................................................................................................2 \nsort...................................................................................................................................................2 \npulling & counting unique values....................................................................................................2 \npulling the most frequent.................................................................................................................3 \nextracting entries with conditions....................................................................................................3 \npersonally identifiable information.................................................................................................4 \ndate modification...........................................................................................................................4 \nregular expressions (regex)............................................................................................................6 \nremoving <html> tags....................................................................................................................6 \nvalidating emails...............................................................................................................................7 \nregex in a data frame......................................................................................................................8 \ndata cleaning..................................................................................................................................8 \ngrouping, merging, reshaping........................................................................................................9 \nmelting.............................................................................................................................................9 \ncleaning column names.................................................................................................................11 \npivot................................................................................................................................................11 \ntranspose.......................................................................................................................................11 \ngroupby..........................................................................................................................................11 \ncount() vs size()..............................................................................................................................12 \nmerge.............................................................................................................................................12 \n \n1 \n\nbrianna yeung (byeu337) \ns1 2025 busan 300 test 2 | python \nset-up \nimport pandas as pd \ndata = { \n \"name\": [\"alice\", \"bob\", \"charlie\"], \n \"age\": [25, 30, 25], \n \"city\": [\"new york\", \"los angeles\", \"chicago\"] \n} \ndf = pd.dataframe(data) \n# take dictionary and instantiate into a data frame \nprint(df) \n \nmount google drive \nfrom google.colab import drive \ndrive.mount('/content/drive') \n# mount google drive for google colab \n \nread in dataset \ndf = pd.read_csv(\"content/drive/mydrive/data.csv\" , dtype='unicode') \n# read in dataset \n \n.loc[] and .iloc[] \n.loc[] - locates by name (label-based indexing) \n.iloc[] - locates by numerical index \n \ndf.iloc(['nami', 'gender']) \n# f \ndf.iloc(['usopp'])  \n# 'usopp', 18, 'm', 3 \ndf.iloc([:, 'gender'])  \n# 'm', 'f', 'm', 'm' \ndf.iloc(['chopper', 'rating']) \n# 4 \n \ndf.loc([0, 0])   \n# 'luffy' \ndf.loc([0])   \n# 'luffy', 17, 'm', 5 \ndf.loc([:, 1])   \n# 17, 18, 18, 15 \ndf.loc([0:1])   \n# ['luffy', 17, 'm', 5], ['nami', 18, 'f', 4] \n \nsimple queries \nselect a column \ncomplaints = pd.read_csv('/content/drive/data.csv', dtype='unicode') \ncomplaints['complaint type'] \n \nsort \ncomplaints = complaints.sort_values(by=\"created date\", ascending=true) \n \npulling & counting unique values \ncomplaints['complaint type'].unique() \ncomplaints['complaint type'].nunique() \n2 \n\nbrianna yeung (byeu337) \ns1 2025 busan 300 test 2 | python \npulling the most frequent \ncomplaints['complaint type'].value_counts() \ncomplaints['complaint type', 'city'].value_counts() # count unique complaint type and which city \n \ncount how many complaints of each type happened in each city. \n \nwe see that the most frequent complaint type is heating, and \nthis complaint is most prominent in the city of brooklyn. \n \n \nextracting entries with conditions \n# from the complaints dataframe, subset this to include only the column complaint types where \nthose entries have the word heat or heat in them, and ignore na's.\n \ncomplaints[complaints['complaint type'].str.contains('heat', case=false, na=false)] \n \n# from the complaints dataframe, subset this to include column complaint types where those \nentries have the word heat/heat or rodent/rodent in them, and ignore na's\n \ncomplaints[complaints['complaint type'].str.contains('heat | rodent', case=false, na=false)] \n \n# assign the above to a variable called 'heat_complaints' \nheat_complaints = complaints[ \ncomplaints['complaint type'].str.contains('heat', case=false, na=false)] \n \n# from all heating complaints, extract entries where the complaint type isn't 'heating' \nheat_complaints[heat_complaints['complaint type'] != 'heating'] \n \n# extract noise complaints and count how many there are \nnoise_complaints = complaints[complaints['complaint type'] == \"noise - street/sidewalk\"] \nprint(len(complaints)) \nprint(len(noise_complaints)) \n \n# find and count noise complaints in the bronx \nnoise_complaints_bronx = noise_complaints[noise_complaints['city'] == \"bronx\"] \nprint(len(noise_complaints_bronx)) \n \n# specify the columns we want to see as a temporary view \nnoise_complaints_bronx[['complaint type', 'city', 'created date', 'descriptor']] \n \n# finding the city with the most noise complaints \nis_noise = complaints['complaint type'] == 'noise - street/sidewalk' \nnoise_complaints = complaints[is_noise] \nnoise_complaints['city'].value_counts() \n \n# plot the noise complaints by city \nnoise_complaints['city'].value_counts().plot(kind='bar') \n3 \n\nbrianna yeung (byeu337) \ns1 2025 busan 300 test 2 | python \npersonally identifiable information \n# using nested json data, normalising, and converting into a data frame \ndf_nested = pd.json_normalize(data) \n \n \n# rename the column names for clarity using .columns \ndf_nested.columns = ['customer_name', 'email', 'phone', 'restaurant', 'rating', 'review'] \n \n \n# anonymise the data \n# step 1: create a new column 'reviewer_id' and create an id per cell \ndf_nested['reviewer_id'] = ['reviewer_' + str(i + 1) for i in df_nested.index] \n# step 2: remove the identifying rows \ndf_anonymised = df_nested.drop(columns=['customer_name', 'email', 'phone']) \n \n \nit is relatively easy to identify individuals even after analysing it, for example, copying and pasting \ntheir review text into google. hence, we can attempt to obfuscate the text. \n \n# remove commas from \"review\" using .str.replace() \ndf_anonymised['review'] = df_anonymised['review'].str.replace(',', '') \ndate modification \n# check types \nprint(complaints['created date'].dtype) \n \n \n# convert 'created date' into a datetime object \n4 \n\nbrianna yeung (byeu337) \ns1 2025 busan 300 test 2 | python \ncomplaints['created date'] = pd.to_datetime(complaints['created date']) \nprint(complaints['created date'].dtype) \n \n# extract dates year, month and day using dt.year, dt.month, dt.day \ncomplaints['year'] = complaints['created date'].dt.year \ncomplaints['month'] = complaints['created date'].dt.month \ncomplaints['day'] = complaints['created date'].dt.day \n \n# extract date in a new format \ncomplaints['date'] = complaints['created date'].dt.strftime('%d-%m-%y') \n \n'%d-%m-%y'   \n # 29-05-2025 \n'%y-%m-%d' \n   # 2025-05-29 \n'%b %d, %y'  \n # may 29, 2025 \n'%m/%d/%y'\n    # 05/29/25 \n'%a'  \n        # day of the week, e.g. thursday \n'%i:%m %p'\n    # 12-hour time, e.g. 03:15 pm \n \n# %d  - day of the month (01 to 31) \n# %m  - month number (01 to 12) \n# %b  - abbreviated month name (jan, feb, ...) \n# %b  - full month name (january, february, ...) \n# %y  - 2-digit year (25) \n# %y  - 4-digit year (2025) \n \n# %h  - hour (00 to 23, 24-hour clock) \n# %i  - hour (01 to 12, 12-hour clock) \n# %p  - am or pm \n# %m  - minute (00 to 59) \n# %s  - second (00 to 59) \n \n# %a  - abbreviated weekday name (mon, tue, ...) \n# %a  - full weekday name (monday, tuesday, ...) \n# %w  - weekday as a number (0 = sunday, 6 = saturday) \n# %j  - day of the year (001 to 366) \n# %u  - week number (sunday as first day of week, 00 to 53) \n# %w  - week number (monday as first day of week, 00 to 53) \n \n# interstly, if we extract all the date counts, we only get two unique dates \ncomplaints['date'].value_counts() \n \n \n \n \n# we can filter complaints from a specific year \n5 \n\nbrianna yeung (byeu337) \ns1 2025 busan 300 test 2 | python \ncomplaints['created date'].pd.to_datetime(complaints['created date']) \ncomplaints['year'] = complaints['created date'].dt.year \ncomplaints_2010 = complaints[complaints['year'] == '2010'] \nprint(len(complaints_2010)) \n \n# calculate the difference between 'created date' and 'closed date' \ncomplaints['created date'].pd.to_datetime(complaints['created date']) \ncomplaints['closed date'].pd.to_datetime(complaints['closed date']) \ncomplaints['time difference'] = complaints['created date'] - complaints['closed date'] \n \n# display complaints with a resolution time greater than 1 day \nlong_resolutions = complaints[complaints['time difference'] > pd.timedelta(days = 1)] \n \n# alternatively, finding complaints with resolution times greater than 1 minute, hour, week \nlong_resolutions = complaints[complaints['time difference'] > pd.timedelta(minutes = 1)] \nlong_resolutions = complaints[complaints['time difference'] > pd.timedelta(hours = 1)] \nlong_resolutions = complaints[complaints['time difference'] > pd.timedelta(weeks = 1)] \n \n# group by date and find the count of each unique date \ncomplaints.groupby('date').count() \nregular expressions (regex) \nabc matches \"abc\" in \"abc123\" \na.c matches \"abc\" in \"abc123\" \n^abc matches \"abc\" at the beginning of \"abc123\" \n123$ matches \"123\" at the end of \"abc123\" \nab*c matches \"ac\", \"abc\", \"abbc\" (you can have 'b' anywhere 0, 1, 2, etc times) \nab+c matches \"abc\" in \"abbc\" (you can have 'b' anywhere 1, 2, etc times) \nab?c matches \"ac\", \"abc\"  (you can have 'b' anywhere 0 or 1 times) \n[abc] matches \"a\", \"b\", or \"c\" \n` `abc    (or operator) \n(abc)+ matches \"abc\", \"abcabc\" (groups patterns together) \n \nremoving <html> tags \n# step 1: finding all '<' in a string using re.findall() \nimport re \ntext = \"<p>this product is <b>amazing</b>!</p>\" \npattern = r\"<\"   \n# when specifying a regex pattern, it has to begin with r \nmatches = re.findall(pattern, text) \n# ['<', '<', '<', '<'] \n \nsquare brackets, [ ], define a character class, that is, anything inside the [ ] define our match criteria. \nalthough, if we use a ^, it negates the characters class specified in [ ]. hence,  r\"[^<]\" means our \ncharacter class is '<', but we negate using ^, so anything that isn't '<' is a match. \n \n6 \n\nbrianna yeung (byeu337) \ns1 2025 busan 300 test 2 | python \n# step 2: finding all '<' in a string using re.findall() \ntext = \"<p>this product is <b>amazing</b>!</p>\" \npattern = r\"<[^<]\"  \nmatches = re.findall(pattern, text) \n# ['<p', '<b', '</', '</'] \n \nthe above code finds any literal '<' followed by another single character. that single character is \ndefined by the [ ] class, that is, anything that is '<', although, we negate this using the ^, so overall, \nwe say; find two characters where it starts with '<', followed directly by something that isn't '<' \n<p>this product is <b>amazing</b>!</p> \n \nthe ? makes the + non-greedy (lazy), meaning it will match as few character as possible \n- greedy = <.*> = <tag>content</tag> \n- non-greedy = <.*?> = <tag>, </tag> \n \nexample \ntext = \"<abc>def<ghi>\" \nprint(e.findall(r\"<.*>\", text)) # [<abc>def<ghi>] \nprint(e.findall(r\"<.*?>\", text)) # [<abc>, <ghi>] \n \nhence, the greedy pattern does not stop until it finds the last instance of '>', while non-greedy will \nstop once it finds one '>' \n \n# step 3: match any preceding character as long as it's not another '<' \ntext = \"<p>this product is <b>amazing</b>!</p>\" \npattern = r\"<[^<]+?>\"  \nmatches = re.findall(pattern, text) \n# ['<p>', '<b>', '</b>', '</p>'] \n \n# step 4: to remove all <html> tags, we can use the re.sub to replace them with nothing \ntext = \"<p>this product is <b>amazing</b>!</p>\" \npattern = r\"<[^<]+?>\"  \nmatches = re.sub(pattern, \"\", text) \n# this product is amazing! \n \nvalidating emails \nimport pandas as pd \ndata = { \n 'name' : ['alice', 'bob', 'charlie'], \n 'email' : ['alice@example.com', 'bob.example', 'charlie@domain.com'] \n} \ndf = pd.dataframe(data) \n \n \n \n \n \npattern = r\"[^@]+@[^@]+\\.[^@]+\" \n7 \n\nbrianna yeung (byeu337) \ns1 2025 busan 300 test 2 | python \n \ndef validate_email(email):   \n# re.match allows us to check the pattern \n  if re.match(pattern, email): \n    return true \n  else: \n    return false \n \ndf['isvalid'] = df['email'].apply(validate_email) \n# apply allows us to apply function to every entry \n \nregex in a data frame \n# find in the data where the complaint type contains the word 'noise', ignore na's \nnoise_complaints = complaints[complaints['complaint type'].str.contains('noise', na=false)] \nnoise_complaints[['complaint type]].head() \n \n# find complaints that have multiple words in them \npattern = r\"loud|music\" \npattern  = complaints[complaints['descriptor].str.contains(pattern , na=flase, regex=true)] \npattern[['descriptor']] \ndata cleaning \n# check for which column has the most missing values \ncomplaints.isnull.sum() \ncomplaints.isnull.sum().sort_values(ascending=false) \n \n# change types and replace blanks, messy or blank values with nan \ncomplaints['closed date'] = pd.to_datetime(complaints['closed date'], errors='coerce') \ncomplaints['incident zip'] = pd.to_datetime(complaints['incident zip'], errors='coerce') \n \n# standardise columns \ncomplaints['complaint type'] = complaints['complaint type'].str.lower()\n # convert all to lowercase \ncomplaints['complaint type'] = complaints['complaint type'].str.strip() \n# remove all whitespaces \n \n# replace missing values of a single column with 'unknown' \ncomplaints['descriptor'].fillna('unknown', inplace=true) \ncomplaints['incident zip'].fillna(complaints['incident zip'].mean(), inplace=true) \n \n# replace missing values in multiple columns with 'unknown' \ncolumns_to_fill = ['location type', 'incident zip', 'city'] \nfor col in columns_to_fill: \n complaints[col].fillna('unknown', inplace=true) \n \n# remove missing values in a single column \ncomplaints.dropna(subset=['closed date']) \n# remove missing values where two values in two columns are missing \n8 \n\nbrianna yeung (byeu337) \ns1 2025 busan 300 test 2 | python \nno_closed_or_res = complaints.dropna(subset=['closed date', 'resolution action updated date']) \n \n# drop rows under conditions \ncleaned_data = cleaned_data.drop(cleaned_data[cleaned_data['tolls_amount'] <= 0].index) \n \n# check duplicates and remove duplicate entries \ncomplaints['city'].duplicated().sum() \ndrop_city_duplicates = complaints.drop_duplicates(subset=['city']) \n \n# drop rows where a specific column has null's or duplicates \ncomplaints.dropna(subset=['city'], inplace=true) \n \n# removing zips that are outside of a particular range \nvalid_range_min = 10000 \nvalid_range_max = 10500 \ncomplaints['incident zip'] = pd.to_numeric(complaints['incident zip'], errors='coerce') \ncomplaints = complaints[complaints['incident zip'].between(valid_range_min, valid_range_max)] \ngrouping, merging, reshaping \nmelting \nwhen data sits in a wide format, and putting into a long format (opposite of pivoting) \n \n        \n    \n→ \n   \n        \n        \ngather columns into rows     \npd.melt(df)     \n        \n \n         \n   \n→ \n     \n         \n         \n    spread rows into columns  \n   df.pivot(columns='var', values='val') \n9 \n\nbrianna yeung (byeu337) \ns1 2025 busan 300 test 2 | python \n         \n \nimport pandas as pd \ndf = pd.dataframe({ \n'product': ['a', 'b'], \n'jan_sales': [100, 80], \n'feb_sales': [150, 120], \n'mar_sales': [200, 160] \n}) \n \nproduct jan_sales feb_sales mar_sales \na 100 150 200 \nb 80 120 160 \n \n# melt the dataframe \ndf = pd.melt(df, id_vars=['product'], var_name='month', value_name='sales') \n \ndf: your original dataframe. \nid_vars=['product']: columns to keep fixed (not unpivoted). here, 'product' stays as is. \nvar_name='month': name for the new column that will hold the original column headers being \nunpivoted. so the former column names (like months) become values under 'month'. \n \nproduct month sales \na jan 100 \na feb 150 \na mar 200 \nb jan 80 \nb feb 120 \nb mar 160 \n \n# pivot the dataframe \ndf = df.pivot(index='product', columns='month', values='sales').reset_index() \n \nproduct jan_sales feb_sales mar_sales \na 100 150 200 \nb 80 120 160 \n \n10 \n\nbrianna yeung (byeu337) \ns1 2025 busan 300 test 2 | python \nindex='product': the column to use as the new row labels (rows will be grouped by each unique \nproduct). \ncolumns='month': the column whose unique values will become the new column headers. \nvalues='sales': the column containing the values to fill the table cells. \ncalling .reset_index() after pivots turns the index (product) back into a regular column. \n \n# melting with the complaints dataset \nmelted_complaints = complaints.melt( \nid_vars=['unique keys'],  \nvalue_vars=['complaint type', 'descriptor'],  \nvar_name = 'category',  \nvalue_name = 'value') \n \nmelted_complaints.head() \ncleaning column names \ncomplaints['city'] = complaints['city'].str.upper() \ncomplaints['city'] = complaints['city'].str.strip() \npivot \npivot_table = complaints.pivot_table( \nindex='agency', \ncolumns='city',  \nvalues='unique key',  \naggfunc='count',  \nfill_value=0) \n \npivot_table.sort_values(by='brooklyn', ascending=false) \ntranspose \npivot_table.transpose() \n \n# how can we show the different complaint types occurring in each city? \ngrouped_complaints = complaints.groupby('city')['complaint type'].count() \ngrouped_complaints.sort_values(ascending=false).head() \ngroupby \ndf = pd.dataframe({ \n'customer': ['john', 'john', 'jane', 'jane'], \n'product': ['a', 'b', 'a', 'b'], \n'sales': [100, 150, 200, 250] \n}) \n11 \n\nbrianna yeung (byeu337) \ns1 2025 busan 300 test 2 | python \ncustomer product sales \njohn a 100 \njohn b 150 \njane a 200 \njane b 250 \n \ndf.groupby('customer').agg({'sales': 'sum'}) \ncustomer product \njohn 250 \njane 450 \n# show different complaint types occurring in each city \ngrouped_complaints = complaints.groupby('city')['complaint type'].count() \ngrouped_complaints = sort_values(ascending=false).head() \n \n# count the number of different complaints occurring in each city \ngrouped_multiple = complaints.groupby(['complaint type', 'city'])['unique key'].count() \ngrouped_multiple .sort_values(ascending=false) \n \ncount() vs size() \nsize includes nan values, but count does not \ncomplaints.groupby(['city'])['closed date'].size().sort_values(ascending=false) \ncomplaints.groupby(['city'])['closed date'].count().sort_values(ascending=false) \nmerge \ninner join: everything that overlaps between group a and group b \nouter join: absolutely everything in both group a and group b \nleft join: everything in group a and its overlap with group b \nright join: everything in group b and its overlap with group a \n \ndata = { \n'agency name': [ \n'new york city police department',  \n'department of transportation',  \n'department of parks and recreation' \n], \n         'severity': ['high', 'medium', 'low'] \n} \nseverity = pd.dataframe(data) \n12 \n\nbrianna yeung (byeu337) \ns1 2025 busan 300 test 2 | python \n \nsubset = complaints.head(25) \nsubset['agency name'].unique() \narray([ \n 'new york city police department',  \n'department of health and mental hygiene', \n'department of transportation' \n]) \nwe see that 'department of health and mental hygiene' exists in the subset, but not in the data \n \ninner join \ninner_merge = pd.merge(subset, severity, on='agency name', how='inner') \nprint(\"rows of data:\", len(inner_merge)) \ninner_merge \n \nleft join \nleft_merge = pd.merge(subset, severity, left_on='agency name', right_on='agency name', how='left') \nprint(\"rows of data:\", len(left_merge)) \nleft_merge \n \nright join \nright_merge = pd.merge(subset, severity, on='agency name', how='right') \nprint(\"rows of data:\", len(right_merge)) \nright_merge \noutter (full) join \nouter_merge = pd.merge(subset, severity, on='agency name', how='outer') \nprint(\"rows of data:\", len(outer_merge)) \nouter_merge \nconcatenation \nfor two different data structures with similar layouts, we should concatenate them together. instead \nof merging them, we should concatenate them, adding their rows together. \n \ndf1 = complaints.head(5) \n13 \n\nbrianna yeung (byeu337) \ns1 2025 busan 300 test 2 | python \ndf2 = complaints.tail(5) \nconcatenated_df = pd.concat([df1, df2], axis=0) \nprint(len(concatenated_df)) \n \nother \nnumber of rows of df: len(df) \n \n14 \n\n",
  "302": "\n\n302 machine learning\n2025 semester 1\n\nweek 2\npython numpy\n\n123\n123\n123\nnumpy (numerical python)\n1d, 2d, and 3d numpy array shape\n123\n123\n123\n123\n123\n123\n123\n123\n123\n123\narray = (3, )\narray = (3, 3)\narray = (3, 3, 3)\nimport numpy as np\na = np.array([10, 20, 30, 40, 50])\n\nprint(a.dtype)# int64\nprint(a.shape)# (5,)\nprint(a.ndim)# 1\nprint(type(a))# <class 'numpy.ndarray'>\nimport numpy as np\nb = np.array([6, 7.5, 8, 0, 1, 2])\n\nprint(b.dtype)# float64\nprint(b.shape)# (6,)\nprint(b.ndim)# 1\nprint(type(b))# <class 'numpy.ndarray'>\n\n246\n812\n468\n535\n717\n828\n123\n456\n789\nimport numpy as np\nc = np.array([\n[1, 2, 3], [4, 5, 6], [7, 8, 9],\n[5, 3, 5], [7, 1, 7], [8, 2, 8],\n[2, 4, 6], [8, 1, 2], [4, 6, 8]])\nimport numpy as np\ng = np.ones((2, 3))\nimport numpy as np\ne = np.zeros((3, 6))\n1.1.1.\n1.1.1.\n0.0.0.0.0.0.\n0.0.0.0.0.0.\n0.0.0.0.0.0.\nimport numpy as np\nf = np.eye(3)\n1.1.0.\n0.1.0.\n0.0.1.\nimport numpy as np\nd = np.arange(0, 12, 2)\n0246810\nimport numpy as np\nh = np.random.randint(0, 10, (3,3))\n044\n269\n145\n01234\n56789\n1011121314\n1516171819\nimport numpy as np\ni = np.arange(20).reshape(4,5)\n\nelement-wise arithmetic operations\ni = np.arange(6).reshape(2,3)\nprint(i * i)\n\n012\n345\n012\n345\nx\n014\n91625\n=\nindexing and slicing\n123\n456\n789\nprint(c[0:2, :])\n\nc = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n123\n456\nprint(c[2, 1:])\n\n89\nprint(c[:2, 1,2])\n23\n56\n\nweek 3\npython pandas\n\npython panda data structures\nnameagegenderrating\nluffy17m5\nnami18f4\nusopp18m3\nchopper15m4\nseries\n1d labeled homogeneous array with immutable size\ndata frames\n2d labeled heterogeneous array with mutable size\nname\nluffy\nnami\nusopp\nchopper\nage\n17\n18\n18\n15\ngender\nm\nf\nm\nm\nrating\n5\n4\n3\n4\n+++\nimport pandas as pd\npandas.dataframe(data, index, columns, dtype, copy)\n\ndata = {'age': [17, 18, 18, 15], 'gender': [m, f, m, m], 'rating': [5, 4, 3, 4]}\ndf = pd.dataframe(data, index=['luffy', 'nami', 'usopp', 'chopper'], index = ['name', 'age','gender', 'rating'])\n\nreading data from csvs\ndf = pd.read_csv('dataset.csv')\ndf = pd.read_csv('dataset.csv', index_col='title')\n\nhandling duplicates\ntemp_df = onepiece_df.append(onepiece_df)\ntemp_df  = temp_df .drop_duplicates(inplace=true)\n\nisolate single rows\ntemp_df[temp_df.index == 1]\ntemp_df[temp_df['gender'] == 'm']\n\nnameagegenderrating\nluffy17m5\nnami18f4\nusopp18m3\nchopper15m4\nusing .loc[] and .iloc[] to fetch / slicing\n.loc[] - locates by name (label-based indexing)\n.iloc[] - locates by numerical index\n\ndf.iloc(['nami', 'gender'])# f\ndf.iloc(['usopp'])# 'usopp', 18, 'm', 3\ndf.iloc([:, 'gender'])# 'm', 'f', 'm', 'm'\ndf.iloc(['chopper', 'rating'])# 4\n\ndf.loc([0, 0])# 'luffy'\ndf.loc([0])# 'luffy', 17, 'm', 5\ndf.loc([:, 1])# 17, 18, 18, 15\ndf.loc([0:1])# ['luffy', 17, 'm', 5], ['nami', 18, 'f', 4]\n\n\ndata cleaning - handling missing values\n1.removing rows with mv's\n2.replace mv's with a mean, median, mode\n\nisnull() - check for mv's\ndropna() - drop mv's\nfillna(), replace() - fill mv's\n\ndf.isnull().any()# returns a boolean per column for mv's\ndf.isnull.sum()# returns a sum of mv's\n\ndf.fillna(df['column_name'].mean(), inplace=true)\ndf['column_name'] = df['column_name'].replace('old_value', 'new_value')\n\nnameagegenderrating\nluffy17m5\nnami18f4\nusopp18mnan\nchopper15m4\nsanjinanm4\nzoro16m3\nrobin19nan5\n\nweek 4\npython - data visualisation\n\ndata visualisations\nline graph (no x-axis information)\napples = [0.895, 0.91, 0.919, 0.926, 0.929, 0.931]\nplt.plot(apples)\n\nline graph (with x-axis information)\nyears = [2010, 2011, 2012, 2013, 2014, 2015]\nplt.plot(years, apples)\nplt.xlabel('year')\nplt.ylabel('yield (tons per hectare)')\n\nmultiple line graph (with x-axis information)\noranges = [0.962, 0.941, 0.930, 0.923, 0.918, 0.908]\nplt.plot(years, apples)\nplt.plot(years, oranges)\n\nplt.xlabel('year')\nplt.ylabel('yield (tons per hectare)')\n\nplt.title('crop yields in kanto')\nplt.legend(['apples', 'oranges'])\nimports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nspecify graph size\nplt.figure(figsize=(12, 6))\n\nspecify markers\nplt.plot(years, apples, marker='o')\nplt.plot(years, oranges, marker='x')\n\nspecify grid\nsns.set_style(\"whitegrid\") # applies to entire file\nsns.set_style(\"darkgrid\") \n\nload seaborn default datasets\ntips_df = sns.load_dataset(\"tips\")\n\ndifferent types of charts\nbar graphs\nyears = [2010, 2011, 2012, 2013, 2014, 2015]\napples = [0.895, 0.91, 0.919, 0.926, 0.929, 0.931]\noranges = [0.962, 0.941, 0.930, 0.923, 0.918, 0.908]\n# seaborn uses barplot() while matplotlib uses bar()\nplt.bar(years, oranges, bottom=apples) # stacked bar graph\nsns.barplot(x='day', y='total_bill', data=tips_df)# seaborn requires parameter specification\n\nsns.barplot(x='day', y='total_bill', data=tips_df, palette='viridis') # customise color\nsns.barplot(x='day', y='total_bill', data=tips_df, hue=sex) # color by a group\n\nsns.barplot(x='total_bill', y='day', hue=sex)# switching x and y creates a horizontal bar plot\n\nhistograms\nflowers_df = sns.load_dataset('iris')\nplt.title(\"distribution of sepal width\")\nplt.hist(flowers_df.sepal_width, bins=np.arange(2, 5, 0.25))# specify bins using arange(start, stop, step)\nplt.hist(flowers_df.sepal_width, bins=[1, 3, 4, 4.5])# specify bins of unequal size\n\nflowers_df.species.unique()# find unique items in a specific column\n\nmultiple histograms\nsetosa_df = flowers_df[flowers_df.species == 'setosa']\nversicolor_df = flowers_df[flowers_df.species == 'versicolor']\nvirginica_df = flowers_df[flowers_df.species == 'virginica']\n\n# adjust alpha to customise transparency\nplt.hist(setosa_df.sepal_width, alpha = 0.4, bins = np.arange(2, 5, 0.25))\nplt.hist(versicolor_df .sepal_width, alpha = 0.4, bins = np.arange(2, 5, 0.25))\n\ndifferent types of charts (cont.)\nstacked histograms\nplt.hist([setosa_df.sepal_width, versicolor_df .sepal_width, virginica_df .sepal_width], \nbins = np.arange(2, 5, 0.25),\nstacks = true)\nplt.legend(['setosa', 'versicolor', 'virginica'])\n\nscatter plots [plotting more than two variables]\n# s adjusts the dot size\nsns.scatterplot(x=flowers_df.sepal_length, y = flowers_df.sepal_width, hue = flowers_df.species, s = 70)\nbox plots\nsns.boxplot(x = df['species'], y = df['sepal_length'], palette = \"blues\")\nplt.show()\n\nheatmaps\nflights_df = sns.load_dataset('flights').pivot('month', 'year', 'passengers')\nplt.title('no. of passengers (1000s)')\n\n# fmt = 'd' specifies = integers\n# annot = true specifies numbers show\n# cmap = 'blues' specifies color\nsns.heatmap(flights_df, fmt = 'd', annot = true, cmap = 'blues')\n\nweek 5\npython: eda & data preprocessing\n\nkey steps for exploratory data analysis (eda)\n1. understand the problem and the data\n2. importing libraries\n3. loading / reading dataset\n4. data inspection\n5. data cleaning\n5.1. checking for missing values\n5.2. checking for duplicates\n6. analysing the data\n6.1. univariate analysis\n6.2. bivariate analysis\n6.3. multivariate analysis\n1.understand the problem and the data\n-what is the business goal or research question?\n-what are the variables in the data and what do they represent?\n-what types of data (numerical, categorical, text, etc\n-are there any known data quality issues or limitations\n-are there any domain-specific concerns or restrictions?\n\n2.importing libraries\n-pandas\n-numpy\n-matplotlib.pyplot\n-seaborn\n\n3.loading / reading dataset\n-pd.read_csv()\n\n4.data inspection\n-get an overview of the data using df.head(), tail(), info()\n-check data types with df.dtypes()\n\n5.data cleaning\n5.1\n-identify and handling missing values using df.isnull().sum()\n5.2\n-find and address duplicates with df.duplicated().sum()\n6. analysing the data\n6.1\n-analyse single variables at a time\n-use descriptive statistics with df.describe() for numerical data\n-create histograms, box-plots, and density plots for visualise \ndistributions\n6.2\n-explore relationships between two variables\n-create scatter plots, pair plots to identify trends and potential \ncorrelations\n6.3\n-interactions between three or more variables in a dataset are \nsimultaneously analysed and interpreted in multivariate \nanalysis\n-use various plots like heatmaps, line charts to explore\neda\ndata\npre-processing\nfeature engineering\n\nexploratory data analysis (eda): insurance.csv\n\n1.understand the problem and the data\nthe dataset insurance.csv is used to predict customer chargers for an insurance company based on given variables so the company can decide how much they charge \npeople correctly.\n-age\n-sex\n-bmi\n-smoker\n-children (number of children covered by health insurance / number of dependents\n-region (the beneficiary's residential area in the us, northeast, southeast, southwest and northwest)\n-charges (individual medical costs billed by health insurance)\n\n2.importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nsns.set_style('whitegrid')\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n3.loading / reading dataset\ndf = pd.read_csv('/content/drive/mydrive/insurance.csv')\n\nexploratory data analysis (eda): insurance.csv (cont.)\n\n4. data inspection\ndf.head(10)\ndf.tail(10)\ndf.sample(5) # randomly select 5 rows\ndf.info()\ndf.dtypes()\ndf.describe()# summary statistics for numerical data only\ndf.describe(include = 'o')# 'o' = object data type; descriptive statistics for categorical variables such as count, unique, top, frequency\nlist(df.sex.unique())\n\n5. data cleaning\ndf.isnull().sum()\nduplicate_rows = df[df.duplicated()]\ndf.drop_duplicates(keep='first', inplace = true)\n\n6. analysing the data\n6.1 univariate analysis\n\nplt.figure(figsize=(10,6))\nsns.distplot(df.charges, color='b')\nplt.title('charges distribution', size = 18)\nplt.xlabel('charges', size = 14)\nplt.xlabel('density', size = 14)\nplt.show()\n\nplt.figure(figsize=(10,6))\nsns.distplot(df.age)\nplt.title('age distribution', size = 18)\nplt.xlabel('age', size = 14)\nplt.xlabel('count', size = 14)\nplt.show()\nplt.figure(figsize=(10,6))\nplt.hist(df.bmi, colo = 'g')\nplt.title('bmi distribution', size = 18)\nplt.show()\nplt.figure(figsize=(10,6))\nsns.boxplot(df.charges)\nplt.title('distribution charges', size = 18)\nplt.show()\ndistplot: chargesdistplot: agehist: bmiboxplot: charges\n\nexploratory data analysis (eda): insurance.csv (cont.)\n\n6. analysing the data\n6.1 univariate analysis\nplt.figure(figsize=(10,6))\nsns.countplot(x = 'sex', data = df)\nplt.title('total num of m and f', size = 18)\nplt.xlabel('sex', size = 14)\nplt.show()\nplt.figure(figsize=(10,6))\nsns.countplot(x = 'smoker', data = df)\nplt.title('smoker distribution', size = 18)\nplt.xlabel('smoker', size = 14)\nplt.xlabel('count', size = 14)\nplt.show()\nplt.figure(figsize=(10,6))\nsns.countplot(x = 'region', data = df, palette = 'blues')\nplt.title('region distribution', size = 18)\nplt.xlabel('region', size = 14)\nplt.xlabel('count', size = 14)\nplt.show()\ncountplot: gendercountplot: smokercountplot: region\n6.2 bivariate analysis\nplt.figure(figsize=(10,6))\nsns.countplot(x = 'age', y = 'charges', data = df)\nplt.title('age vs charges', size = 18)\nplt.xlabel('age', size = 14)\nplt.xlabel('charges', size = 14)\nplt.show()\nscatter plot: age vs gender\nsns.pairplot(df, diag_kind = 'kde')\nplt.show()\npairs plot: all numeric variables\n6.3 multivariate analysis\nheatmap: all numeric variables\nplt.figure(figsize=(10,6))\nnumeric_df = df.select_dtypes(include=np.number)\nsns.heatmap(numeric_df.corr(), annot=true, cmap='coolwarm')\nplt.title('correlation matrix')\nplt.show()\n\nweek 6\nintroduction to machine learning and regression\n\nmachine learning tasks\nbanks look for trends in transaction data to detect outliers that may be \nfraudulent\nemail inboxes use text to classify whether an email is spam or not, and \nadjust their classification rules based upon how we flag emails\ntravel apps use live and historic data to predict traffic, travel times, \nand journey routes\nretail companies and streaming services use data to recommend new \ncontent we might like based upon our demographic and historical \npreferences\nself-driving cars and robots use object detection and performance \nfeedback to improve their interaction with the world\n finding trends in data\nlearning to interact in an environment\n making decisions and predictions\nclassifying data into groups and categories\n尀尀\n\n\n\n✈\n\ntypes of machine learning algorithms\nmachine learning\nunsupervised\nsupervised\nclustering\nclassification\nregression\nk-means\nknnlogistic regressionlinear regression\n\nsupervised\nneeds external supervision to learn\nmodels are trained using the labeled dataset\ntakes a known set of input data and known \nresponses to the data and trains a model to \ngenerate predictions for the response to new data\nonce training is done, the model is tested by \nproviding a sample test data to check whether it \npredicts the correct output\nuse supervised learning if you have known data \nfor the output you are trying to predict\nsquare     pentagon      circle       triangle \nlabels\nlabeled data\nmodel \ntraining\nprediction\ntest data\nsquare\npentagon \nregression [linear regression]\nif theres a relationship between input and output variables. used to make predictions of continuous variables, like weather forecasting or market trends\n\nclassification [knn, logistic regression]\nwhen the output variable is binary and is a categorical variable, which means there are two classes such as yes-no, male-female, true-false, 0-1, etc\n\nunsupervised\nthe machine does not need any external \nsupervision to learn from the data\ncan be trained using the unlabelled \ndataset that is not classified, nor \ncategorised\nit is used to draw inferences from data \nsets consisting of input data without \nlabeled responses\nno predefined output and tries to find \nuseful insights from a large amount of \ndata\ninput raw data\ninterpretationalgorithm\ndogs\ncats\n\n\n\n瑩\n\n\nprocessing\noutput\n瑩\n\nclustering [k-means]\napplications include gene sequence analysis, market research and anomaly detection, for example, customer segmentation\n\n\n\nmachine learning workflow [steps]\n\n1.extract features\n2.split dataset\n3.train dataset\n4.train model\n5.evaluate (using test dataset)\n\nchoosing the right algorithm\nthere is no best method, it is partly trial and error, and partly depends on the size and type of data, the insights you want, and how insights will be used\n\n\nsupervised-you need to make a prediction such as the future value of a continuous variable (such as temperature, stock) or a \nclassification (such as identifying car makers from webcam video footage)\nunsupervised-you need to explore your data and want to train a model to find a good internal representation (such as splitting data up \ninto clusters)\n\nregression\nmultiple linear regression: one dependent variable and multiple independent variables\nsimple linear regression: one independent and one dependent variable\n-predicting what the price of a product will be in the future, whether prices go up or down\n-estimating the number of houses a builder will sell in the coming months and at what price\n-predicting the number of runs a baseball player will score in upcoming games based on previous performance\n-understanding how temperature affects ice cream sales\n-predicting the price of a house given house features\n-predicting the impact of college scores on university admission\n\nexample\ncreate a regression model for a dataset that will predict exam scores from hours spent revising using score.csv\nimport pandas as pd\nstudy_scores = pd.read_csv('score.csv')\nsns.relplot(x='scores', y = 'hours', data=study_scores)# after seeing that a linear regression is appropriate (straight-line), fit the model\n\nfrom sklearn.linear_model import linearregression\nregressor = linearregression()\n\nx = study_scores .iloc[:,:-1].values\ny = study_scores .iloc[:,1].values\n\nfrom sklearn.model_selection import train_test_split# split dataset in ration of 70:30, where 70% is the training set and 30% is the testing set\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state = 1)\nregressor.fit(x_train, y_train)\n\ny_pred = regressor.predict(x_test)\n\nfrom sklearn.metrics import mean_squared_error\nmean_squared_error(y_test, y_pred)# mean squared error (mse)\nnp.sqrt(mean_squared_error(y_test, y_pred))# root mean squared error (rmse)\n\n\nmse & rmse\n\nmse:\n-measures average squared error per prediction.\n-higher mse means worse performance (larger errors).\n-because it squares the errors, larger errors are penalised more than smaller ones.\nrmse:\n-rmse is in the same units as the target variable, which makes it more interpretable than mse.\n-gives a sense of the typical size of the prediction error.\n-like mse, larger errors have a higher impact due to squaring.\n\nfor a rmse of 7.5: on average, if your score is between 0-100, the model predicts values below or above 7.5 marks, which is pretty bad\n\n\nweek 7\nregression, and model performance\n\nlogistic regression\nlogistic regression\n-determining whether an employee would get a promotion or not based on their performance\n-banks predicting whether a customer would default on loans or not\n-predicting weather conditions of a certain place (sunny, windy, rainy)\n-identifying buyers if they are likely to purchase a certain product\n-predicting whether they will gain or lose money in the next quarter, year or month based on their current performance\n-to classify objects based on their features and attributes\n\nexample\nfrom sklearn.linear_model import logisticregression\nlogreg = logisticregression(random_state=1, max_iter=1000)# max_iter is optional\n\nx = df['pregnant', 'insulin', 'bmi', 'age', 'glucose', 'bp', 'pedigree']\ny = df.label\n\nfrom sklearn.model_selection import train_test_split# split dataset in ration of 75:25, where 75% is the training set and 25% is the testing set\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state = 16)\nlogreg.fit(x_train, y_train)\n\ny_pred = logreg.predict(x_test)\n\nlogistic regression performance metrics [confusion matrix, accuracy, precision, recall, f1-score]\nfrom sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n\nlogistic regression (cont.)\nlogistic regression performance metrics [confusion matrix, accuracy, precision, recall, f1-score]\nfrom sklearn.metrics import accuracy_score# tp + tn / (tp + tn + fp + fn)\naccuracy = accuracy_score(y_test, y_pred)# an accuracy score of 0.73, means 27% of the time, the model is not correctly predicting values\n\nfrom sklearn.metrics import precision_score# tp / (tp + fp)\nprecision = precision_score(y_test, y_pred)\n\nfrom sklearn.metrics import recall_score# tp / (tp + fn)\nrecall = recall_score(y_test, y_pred)\n\nfrom sklearn.metrics import f1_score# inverse of the mean; mean = (x1 + x2 + x3 + ... + xn) / n\nf1score= f1_score(y_test, y_pred)# 2 * precision * recall / (precision + recall)\n\nuse both precision and recall: when there is an imbalance in the observations between the two classes\n-eg: there are more of one class (1) and only a few of the other class (0) in the dataset\n\nuse precision: false positives are costly (spam detection, fraud alerts) or if you have a balanced dataset\nuse recall: false negatives are costly (disease diagnosis, safety alerts)\nuse f1 score: both false positives and false negatives should be small\n\nweek 8\nclassification\n\npredicting the green guys favourite sport based on \nhis nearest neighbours\nk = 4\nknn (k-nearest neighbour)\n\nnameage\nloandefault\neuclidean \ndistance\nusopp155000n40002\nsanji162000n10240001\nrobin181500n13689999\ngaban156400n1440002\nvivi189300n16809999\nzoro165200y1\nnami175400y40000\nbuggy151500y13690002\nlucci178200y11560000\nluffy\n175200?\npredicting luffy's default status (y or n)\n= sqrt(17 - 15)^2 + (5200 - 5000)^2\n= sqrt(17 - 16)^2 + (5200 - 2000)^2\n= sqrt(17 - 18)^2 + (5200 - 1500)^2\n= sqrt(17 - 15)^2 + (5200 - 6400)^2\n= sqrt(17 - 18)^2 + (5200 - 9300)^2\n= sqrt(17 - 16)^2 + (5200 - 5200)^2\n= sqrt(17 - 17)^2 + (5200 - 5400)^2\n= sqrt(17 - 15)^2 + (5200 - 1500)^2\n= sqrt(17 - 17)^2 + (5200 - 1800)^2\nminimum \ned\n3\n5\n\n4\n\n1\n2\n\n\n\nfor k = 5\neuclidean distance = sqrt(x1 - y1)^2 + (x2 - y2)^2\nthe default status for luffy is n \nbecause  for  k=5,  there  are  3 \npoints = n and only 2 points = y.\nknn is a lazy learner, it parses through all data \npoints for each classification, and therefore only \nworks on smaller datasets.\n\nknn  also  fails  when  variables  have  different \nscales.   feature   scaling   (standardisation   and \nnormalisation) is required before applying knn.\nin python:\n1.the k-nearest neighbour import\n2.create feature and target variables\n3.split data\n4.generate knn model using neighbour value\n5.train or fit data into the model\n6.predict the future\n\nknn - classification: predicting a category or class\n\nfrom sklearn.neighbors import kneighborsclassifier\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\ndf = pd.read_csv('iris.csv')\nx = df.iloc[:,:4]\ny = df.iloc[:, 4].values\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state = 42)\nknn = kneighborsclassifier(n_neighbirs = 7)\nknn.fit(x_train, y_train)\n\ny_pred = knn.predict(x_test)\n\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, y_pred)\n\nfrom sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n\nknn - regression: predicting a continuous number\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state = 42)\nmodel = kneighborsregressor(n_neighbirs = 9)\nmodel .fit(x_train, y_train)\ny_pred = model.predict(x_test)\n\nimport math\nimport sklearn.metrics as skl_metrics# outcome is a continuous, so no confusion matrix\ne1 = skl_metrics.mean_squared_error(y_test, y_pred)# mean square error\nmath.sqrt(e1)# root mean square error\n\nk-means simulation\n\n\n\nk = 3\nrandomly allocated centroid red\nrandomly allocated centroid orange\nrandomly allocated centroid blue\n\nk = 3\n\nk = 3\n\nk = 3\n\nk = 3\n\nk = 3\n\nk = 3\n\n***unsupervised k-means clustering\n\nk-means is an unsupervised clustering algorithm designed to partition unlabeled data into a certain number (k) of distinct groups.\n\neach cluster is represented by its center (centroid) which corresponds to the arithmetic mean of data points assigned to the cluster. \n'k' represents the number of clusters we want to classify our data points into. a centroid is a datapoint that represents the mean and \nmight not necessarily be a member of the dataset. the algorithm works iteratively until each datapoint is closer to its own clusters' \ncentroid than to other clusters' centroids, minimising intra-cluster distance at each step.\nfrom sklearn.cluster import kmeans\nkmeans_model = kmeans(n_clusters = 3)\nkmeans_predict = kmeans_model.fit_predict(x)\n\niris['cluster'] = kmeans_predict# merge the result of the clusters with our original dataset\ncentroids = km.cluster_centers_\n\nx1 = x[x['cluster']==0]\nx2 = x[x['cluster']==1]\nx3 = x[x['cluster']==2]\nplt.scatter(x1['age'],x1['income($)'],color=\"blue\",s=100)\nplt.scatter(x2['age'],x2['income($)'],color=\"red\",s=100)\nplt.scatter(x3['age'],x3['income($)'],color=\"purple\",s=100)\nplt.scatter(centroids[:,0],centroids[:,1],color=\"orange\",marker=\"*\",s=150);\n\npredicted_cluster=km.predict([[23,50000]])\n\n\nweek 9\nclustering\n\nk-means performance metrics\n\nelbow methodsilhouette score / analysis\nto find the best value of k\n\nwcss: the total within-cluster sum of squares measures the compactness of the \nclustering, and we want it to be as small as possible. wcss is the average distances \nto the centroid across all data points.\n\nthe elbow method runs k-means clustering on the dataset for a range of values k.\n\neli5:  imagine  you  have  a  big  box  of  legos,  they  are  different  sizes  and  different \ncolors. you want to sort them, so you start with two boxes, this is better, but then \nyou use three boxes, even better, you then feel overly optimistic and try 10 boxes, \nbut this is just too much work. k-means elbow method finds a threshold value for k \nwhich provides the perfect wcss for a particular k-value\n\n\n\n\n\n\n\n\n\n\n\n\n\na metric to evaluate the quality of clustering performed by k-means. measuring \nhow well data points are grouped within their assigned clusters compared to data \npoints in other clusters.\n\nyou  use  k-means  ml  algorithm,  and  now  you  want  to  measure  how  good  the \nclusters are.\n\na = how close it is to points in its own cluster (you want this to be small)\nb = how close it is to points in the next nearest cluster (you want this to be large)\n\nsilhouette score = (b - a) / max(a, b)\nthe silhouette score ranges from -1 to 1\n\n 1: ideally close data points within a cluster and far from other clusters (good)\n 0: data points on the border between clusters indicating some overlap (average)\n-1: data points might be assigned to the wrong cluster (bad)\n\n\n\n\n\n\n\n\n\n\n\nfeature engineering\nimputationoutlier handlingone-hot encodingscalinglog transformation\nthe most common techniques of feature scaling are normalisation and standardisation\n\nnormalisation: values are bound between 0 and 1 or -1 and 1\nstandardisation: transforms values to have zero mean and a variance of 1\n\nabsolute maximum scaling (very sensitive to outliers)\n1.select the maximum absolute value out of all entries of a column\n2.divide each entry by this maximum value\n3.observe that each entry lies in the range of -1 to 1\n\nmax_vals = np.max(np.abs(df))\nprint((df - max_vals) / max_vals)\n\nmin-max scaling\n1.find the minimum and the maximum value of the column\n2.subtract the minimum value from the entry and divide the result by the difference between the maximum and the minimum value\n\nfrom sklearn.preprocessing import minmaxscaler\nscaler = minmaxscaler()\nscaled_data = scaler.fit_transform(df)\nscaled_df = pd.dataframe(scaled_data, columns = df.columns)\n\nmean normalisation\nfrom sklearn.preprocessing import normalizer\nscaler = normalizer()\nscaled_data = scaler.fit_transform(df)\nscaled_df = pd.dataframe(scaled_data, columns = df.columns)\n\n\nfeature engineering (cont.)\nimputationoutlier handlingone-hot encodingscalinglog transformation\nstandardisation\nfrom sklearn.preprocessing import standardscaler\nscaler = standardscaler()\nscaled_data = scaler.fit_transform(df)\nscaled_df = pd.dataframe(scaled_data, columns = df.columns)\n\nlabel encoder\nfrom sklearn.preprocessing import labelencoder\nlabel_encoder = labelencoder()\ndataset['gender'] = label_encoder.fit_transform(dataset['gender'])\ndataset['gender'].unique()\n\nhierarchical clustering - dendrogram\ncscomstatmatpsyengllbartlinphil\nagglomerative: bottom-up\n\ndivisive: top-down\n\nfinding k using dendrogram\n1.scan the dendrogram to identify the longest vertical line that does not intersect with any horizontal lines (clusters)\n2.draw a horizontal line through it\n3.count the number of times the horizontal line intersects with the horizontal lines representing clusters in the dendrogram\n\np1\np2\np3p4\np5\np6\n1234\np1p2p3p4p5p6\np10\np210\np31.50.50\np42.51.510\np5310.50.50\np6421.50.510\nthe  shortest  distance  is  p1  to  p2,  hence  we  should \nmerge.  we  will  then  be  left  with  5  clusters.  again, \nrecalculate the euclidean distance to get a 5x5 matrix\n1.treat each data points as a separate cluster\n2.calculate the distance between each pair of clusters (euclidean distance), resulting in an n x n distance matrix where the distance between a cluster and itself is zero\np1p2p3p4p5p6\np1p20\np31.50\np4210\np51.50.10.50\np62.51.50.510\nthe shortest distance is p3 and p4, hence we should \nmerge. we will then be left with 4 clusters. again, \nrecalculate  the  euclidean  distance  to  get  a  4x4 \nmatrix\np1p2p3p4p5p6\np1p20\np3p41.50\np51.500\np62.5110\np1p2\np3p4\np5\np6\n1234\np1p2\np3p4\np5\np6\n1234\nthe  shortest  distance  is  p3p4  and  p5, \nhence we should merge. we will then be \nleft with 4 clusters. again, recalculate the \neuclidean distance to get a 4x4 matrix\n\nweek 10\nfeature engineering\n\ndendrograms in python\n\nx = data[['age', 'income($)']]\n\nfrom sklearn.preprocessing import standardscaler()\nsc = standardscaler()\nx = sc.fit_transform(x)\nx = pd.dataframe(x)\nx.columns = ['age', 'income($)']# standardise age and income to a computer-interpretable scale\n\nfrom sklearn.cluster import agglomerativeclustering\nac = agglomerativeclustering(n_clusters=3, linkage='single')\nypred = ac.fit_predict(x)\n\nx['cluster'] = ypred\n\nimport scipy.cluster.hierarchy as sch\ndend = sch.dendrogram(sch.linkage(x, method='single'))\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n\nlinked = linkage(x_scaled, method='ward')\nx['cluster'] = fcluster(linked, t=3, criterion='maxclust')\n\n\n\nlinear regression\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nstud_scores = pd.read_csv('score.csv')\nstud_scores.head()\nstud_scores.shape\nimport seaborn as sns\nsns.relplot(x='scores', y='hours', data=stud_scores, height=3.8, aspect=1.8, kind='scatter')\nsns.set_style('darkgrid')\n\nx = stud_scores.iloc[:,:-1].values # feature matrix\ny = stud_scores.iloc[:,1].values # response vector\n\n# splitting the data\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=1)\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n\nfrom sklearn.linear_model import linearregression\nregressor = linearregression()\nregressor.fit(x_train, y_train)\n\nregressor.coef_\nregressor.intercept_\ny_pred = regressor.predict(x_test)\ncomparison_df = pd.dataframe({\"actual\":y_test,\"predicted\":y_pred})\n\nfrom sklearn.metrics import mean_squared_error\nprint(\"mse\",mean_squared_error(y_test,y_pred))\n\nimport numpy as np\nprint(\"rmse\",np.sqrt(mean_squared_error(y_test,y_pred)))\n\n\nlogistic regression\n\nsns.countplot(x=data['pclass'],hue=data['survived']);\nsns.countplot(x=data['sex'],hue=data['survived']);\nsns.countplot(x=data['embarked'],hue=data['survived']);\n\ncols = ['passengerid','name','ticket','fare','cabin']\ndata = data.drop(cols,axis=1)\ndata.isnull().sum()\nmean_age = round(data['age'].mean(),2)\ndata['age'] = data['age'].fillna(mean_age)\ndata.isnull().sum()\ndata = data.dropna()\n\nfrom sklearn.preprocessing import labelencoder\nencoder = labelencoder()\ndata['sex'] = encoder.fit_transform(data['sex'])\ndata['embarked'] = encoder.fit_transform(data['embarked'])\n\ny = data['survived'].values\nx = data.drop(['survived'],axis=1).values\n\nfrom sklearn.model_selection import train_test_split\nxtrain,xtest,ytrain,ytest = train_test_split(x,y,train_size=0.8,random_state=9014)\n\nfrom sklearn.linear_model import logisticregression\nmodel = logisticregression()\nmodel.fit(xtrain,ytrain)\nypred = model.predict(xtest)\n\nfrom sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score,confusion_matrix\nconfusion_matrix(ytest,ypred)\naccu = accuracy_score(ytest,ypred)\nf1_score(ytest,ypred)\nprecision_score(ytest,ypred)\nrecall_score(ytest,ypred)\n\nknn\n\nx=df.iloc[:,:4]\ny = df.iloc[:, 4].values\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=42)\n\nfrom sklearn.neighbors import kneighborsclassifier\nknn = kneighborsclassifier(n_neighbors=7)\nknn.fit(x_train, y_train)\ny_pred=knn.predict(x_test)# predict on dataset which model has not seen before\n\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test,y_pred)\n\nfrom sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n\nx = df.iloc[:, [1]].values\ny = df.iloc[:, 4].values\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=42)\n\nfrom sklearn.neighbors import kneighborsregressor\nmodel = kneighborsregressor(n_neighbors = 9)\nmodel.fit(x_train, y_train)\ny_pred=model.predict(x_test)# predict on dataset which model has not seen before\n\nimport math\nimport sklearn.metrics as skl_metrics\ne1=skl_metrics.mean_squared_error(y_test, y_pred)\nprint(\"mean square error=\", e1)\nerror = math.sqrt(e1)\nprint(\"root mean square error=\", error)\nknn classifier: when your target variable is categorical (e.g., 'spam' vs. 'not spam', or 'dog', 'cat', 'rabbit').\nknn regressor: when your target variable is numerical (e.g., house price, temperature, age).\n\nk-means clustering\n\n\n\nbusan300 break revision \nweek 2..............................................................................................................................................................................2\n \nnumpy.................................................................................................................................................................................2\n \nweek 3..............................................................................................................................................................................3\n \npandas.................................................................................................................................................................................3\n \ndata cleaning......................................................................................................................................................................5\n \nweek 4..............................................................................................................................................................................6\n \nmatplotlib & seaborn..........................................................................................................................................................6\n \nline charts...........................................................................................................................................................................6\n \nbar plots..............................................................................................................................................................................7\n \nhistogram............................................................................................................................................................................7\n \nscatter plots........................................................................................................................................................................8\n \nbox plots.............................................................................................................................................................................8\n \nheat maps...........................................................................................................................................................................8\n \nweek 5..............................................................................................................................................................................9\n \nexploratory data analysis....................................................................................................................................................9\n \nexample: insurance.csv.....................................................................................................................................................10\n \nweek 6............................................................................................................................................................................11\n \nmachine learning.............................................................................................................................................................11\n \nregression.........................................................................................................................................................................14\n \nregression example..........................................................................................................................................................14\n \nweek 7............................................................................................................................................................................15\n \nclassification algorithm: logistic regression....................................................................................................................15\n \nweek 8............................................................................................................................................................................17\n \nk - nearest neighbour (knn)............................................................................................................................................17\n \n \n\nweek 2 \nnumpy \nnumpy stands for numerical python, it is the fundamental package in python for high-performance \ncomputing and data analysis. numpy arrays are important because they enable vectorisation, that is,  \nto apply a batch of operations to an entire array without writing loops. \n \ncreating a numpy array \nimport numpy as np \na = np.array([10, 20, 30, 40, 50]) \nprint(a) \nprint(type(a)) \n[10 20 30 40 50] \n<class 'numpy.ndarray'> \nimport numpy as np \ndata = [10, 20, 30, 40, 50] \narray1 = np.array(data) \nprint(array1.dtype) \nprint(array1.shape) \nint64  \n(5, ) \nimport numpy as np \ndata = [10, 20.5, 30, 40, 50] \narray1 = np.array(data) \nprint(array1) \nprint(array.dtype) \n[10. 20.5 30. 40. 50. ] \nfloat64 \narray = np.zeros((2,3)) \n[[0. 0. 0. ] \n [0. 0. 0. ]] \narray = np.ones((2,3)) \n[[1. 1. 1. ] \n [1. 1. 1. ]] \narray = np.eye(3) \n[[1. 0. 0. ] \n [0. 1. 0. ] \n [0. 0. 1. ]] \narray = np.arange(0, 10, 2) \n[0, 2, 4, 6, 8] \narray = np.random.randint(0, \n10, (3,3)) \n[[6 4 3 ] \n [1 5 6 ] \n [9 8 5 ]] \narray = \nnp.ones((2,3),dtype=np.int32) \n[[1 1 1] \n [1 1 1]] \narray type conversion \narray = np.array([[2.5, 3.8, 1.5], [4.7, 2.9, 1.56]]) \nb = array.astype('int') \nprint(b) \n[[2 3 1] \n [4 2 1]] \narange() to create a 2d array \narray = np.arange(20).reshape(4,5) \nprint(array) \narray([[0, 1, 2, 3, 4], \n            [5, 6, 7, 8, 9, 9], \n            [10, 11, 12, 13, 14], \n            [15, 16, 17, 18, 19]]) \nprint(array[3,4]) \n19 \n\nfull function \nnp.full((2,2),3)  \n# creates a 2x2 array with all elements to be '3' \narray([[3, 3],[3, 3]]) \nindex slicing \nin index living, the comma separates from the row to the column. hence everything before the \ncolumn refers to the rows, and everything after the comma refers to the columns. \n0 1 2 3 4  \narray[0:2,:] \n \narray[2:1,:] \n5 6 7 8 9 \n10 11 12 13 14 \n \n0 1 2 3 4  \narray[:2, 2:3] \n5 6 7 8 9 \n10 11 12 13 14 \nmatrix multiplication \np = [[1,0], [0, 1]] \nq = [[1,2], [3, 4]] \np * q \n1 * 1 + 0 * 3 1 * 2 + 0 * 4 \n \n  = \n1 2 \n0 * 1 + 1 * 3 0 * 2 + 1 * 4 3 4 \nweek 3 \npandas \npython pandas are used to import datasets from databases, spreadsheets, csv's, etc. pandas are \nused to clean datasets, tidy datasets by reshaping their structure into a suitable format for analysis, \naggregate data by calculating summary statistics, and ofcourse, data visualisation. \n \ninstall pandas using pip install pandas if you are using an ide other than google colab. \nimport pandas as pd \n \nthere are different types of data structure in pandas: \nseries 1 dimension 1d labeled homogeneous array with immutable size. \ndata frames 2 dimensions general 2d labeled, size mutable tabular structure with potentially \nheterogeneous types columns. \n\nseries are essentially a column, while a data frame is essentially a collection of series. for both series \nand data frames, the same functions and methods can be used on both. \ncreating a pandas dataframe \npandas.dataframe(data, index, columns, dtype, copy) \n● data: data takes various forms such as ndarray, series, map, lists, dict, dataframes \n● index: for the row labels, this is optional \n● columns: for the column labels, this is optional \n● dtype: data type of each column \n● copy: this command is used for copying data, the default is false. \n \ndata = {'apples':[3,2,0,1], 'oranges':[0,3,7,2]} \ndf = pandas.dataframe(data) \n apples oranges \n0 3 0 \n1 2 3 \n2 0 7 \n3 1 2 \n \ndf = pandas.dataframe(data, index=['a', 'b', 'c', 'd']) \n apples oranges \na 3 0 \nb 2 3 \nc 0 7 \nd 1 2 \n \nreading data from csv \nimport pandas as pd \ndf = pd.read_csv('dataset.csv', index_col=0) \n \nin google colab: \nfrom google.colab import drive \ndrive.mount('/content/drive') \ndf = pd.read_csv('/content/drive/my drive/path_to_your_csv/iris.csv') \n\nviewing data \ndf.head()  \n# by default, it shows the first 5 rows, can modify the parameter \ndf.tail()   \n# by default, it shows the last 5 rows, can modify the parameter \ndf.info()   \n# shows info about the dataframe, including the number of rows, data types \ndf.shape  \n# shows the number of (rows, columns), aka dataframe dimensions \ndf.describe()  \n# continuous numeric values are used to return summary statistics \ndf['gender'].describe() \n# categorical variable summary statistics: count, unique, top, freq \ndf[df.index==1]  \n# extracts a single row using [] index number \nusing .loc[] and .iloc[] \ncan be used to fetch rows using slicing. loc stands for locations. .loc locates by name, while .iloc \nlocates by the index number. \n \nexample dataset: \npregnancies glucose bloodpressure skinthickness insulin bmi \n6 148 72 35 0 33.6 \n1 85 66 29 1 26.6 \n \ndf.loc[df['insulin']==0].head() \nthe above code will return the first five rows of data where the column 'insulin' is equal to 0. \ndf.iloc[2] \nthe above code simply returns the row for the second [2] index. \ndf.iloc[0:4, 2:5] \nthe above code will return the row of index 0 to the row of index 3 but not all the columns. it returns \nthe columns with index 2 to index 4. \nhandling duplicates \ntemp_df = my_df.append(my_df)  \n# create a copy of the raw data \ntemp_df = temp_df.drop_duplicates()  \n# drops duplicate values \ntemp_df.drop_duplicates(inplace=true)  \n# do and apply changes in the same dataframe \n \ndata cleaning \nmissing values \nmissing values may be represented by nan, na, null, or simply just blank. there is no single \nuniversally acceptable method to handle missing values. it is often left to the judgement of the \nanalyst, to either replace or drop them all together. \n \ndetecting, removing and replacing \nisnull(), dropna(), fillna(), replace(), df.isnull().any(), df.isnull().sum() \n\nfilling missing values \n1. df['model_year'].fillna(df['model_year'].mean(), inplace=true) \n#fills with the datasets mean \n2. df['column_name'] = df['column_name'].replace('old_value', 'new_value') \n# replaces values \nweek 4 \nmatplotlib & seaborn \nimport matplotlib.pyplot as plt import seaborn as sns \nit is used for basic graph plotting like line charts, \nbar graphs, etc, and acts productively with data \narrays. \nit is used for statistics visualisation and can \nperform complex visualisations with fewer \ncommands, it is considerably more organised \nand functional than matplotlib and treats the \nentire dataset as a unit \n \nseaborn datasets \nthe seaborn provides many datasets built-in that are stored in the pandas data frame, making them \neasy to use with seaborns plotting functions. \n1. tips dataset \n2. iris dataset \n3. penguins dataset \n4. flights dataset \n5. diamonds dataset \n6. titanic dataset \n7. exercise dataset \n8. mpg dataset \n9. planets dataset \nloading in a seaborn dataset: tips_df = sns.load_dataset(\"tips\") \n \nline charts \na line chart is a graph that represents information as a series of data points \nconnected by a straight line. each data point is plotted and connected with a \nline or a curve. it is used to show a change overtime for numeric data and \ncan be used to make predictions \n \nsimple plot \nyield_apples = [0.895, 0.91, 0.919, 0.926, 0.929, 0.931] \nyears = [2010, 2011, 2012, 2013, 2014, 2015] \nplt.plot(years, yield_apples)   \n# initialise the plot \nplt.xlabel('year')     \n# add a x-axis label \nplt.ylabel('yield (tones per hectare)')  \n# add a y-axis label \n\nmulti-plots \nyield_oranges = [0.962, 0.941, 0.930, 0.923, 0.918, 0.908] \nplt.plot(years, yield_apples) \nplt.plot(years, yield_oranges)   \n# layer the plot under the first initialisation \nplt.title('crop yields in kanto')   \n# add a main title \nplt.legend(['apples', 'oranges'])\n   \n# adding a legend (list in order of plot initialisation \n \nplt.plot(years, yield_apples, marker='o')  \n# adding a marker \nplt.plot(years, yield_oranges, marker='x') \n# adding a marker \nplt.figure(figsize=(12,6))    \n# sets size of figure 12 inches wide and 6 inches tall \n \nsetting the style using set_style will change the style for all graphs in the file. \nplt.figure(figsize=(12,6))  \nsns.set_style(\"whitegrid\")\n \n# sets gridlines, is used with the matplotlib plot / \"darkgrid\" \n \nbar plots \na bar plot is used for nominal or ordinal categories. it can compare data \namongst different categories, and is mainly ideal for more than three \ncategories. it can also show large data changes over time.  \n \nstacked bar charts \nyears = range(2000, 2006) \napples = [0.35, 0.6, 0.9, 0.8, 0.65, 0.8] \noranges = [0.4, 0.8, 0.9, 0.7, 0.6, 0.8] \nplt.bar(years, apples)    \n# plotting a bar plot using matplotlib \nplt.bar(years, oranges, bottom=apples)  \n# creates a stacked bar chart \nplt.xlabel('year') \nplt.ylabel('yield (tones per hectare)') \nplt.title('crop yields in kanto') \n \nplotting averages of each bar \nsns.barplot(x='day', y='total_bill', data=tips_df, palette='viridis', hue='sex') # uses seaborn to plot \nhistogram \nare used for continuous data, they display the frequency distribution (shape) \nof the data. they summarise large data sets graphically and compare multiple \ndistributions. \n \nsimple histograms \nflowers_df = sns.load_dataset(\"iris\") \nflowers_df .sepal_width      \n# extracting a single column \nplt.title(\"distribution of sepal width\") \nplt.hist(flowers_df.sepal_width, bins=5)    \n# modifying bin size \n\nplt.hist(flowers_df.sepal_width, bins=np.arange(2,5,0.25))  \n# specify boundaries of each bin \nplt.hist(flowers_df.sepal_width, bins=[1,3,4,4.5])   \n# unequal bin sizes \nmultiple histograms \nflowers_df.species.unique()  \n# get the unique() values for a specified column \nsetosa = flowers_df[flowers_df.species=='setosa']\n \n# make subset of setosa \nversi = flowers_df[flowers_df.species=='versicolor'] \n# make subset of versicolor \nvirgi = flowers_df[flowers_df.species=='virginica'] \n# make subset of versicolor \n \nplt.hist(setosa.sepal_width, bins=np.arange(2,5,0.25), alpha=0.4) # alpha controls transparency \nplt.hist(versicolor.sepal_width, bins=np.arange(2,5,0.25), alpha=0.5) \nplt.hist(virginica.sepal_width, bins=np.arange(2,5,0.25), alpha=0.6) \n \nstacked histograms \nplt.hist( \n[setosa.sepal_width, versi.sepal_width, virgi.sepal_width], \n       bins = np.arange(2, 5, 0.25),  \nstacked=true) \nplt.legend(['setosa', 'versicolor', 'virginica']) \nscatter plots \nscatter plots are used to visualise relations between two numeric \nvariables. it is used to visualise correlation in a large dataset, and to \npredict behaviour of dependent variables based on the independent \nvariable. \n \nsimple scatterplot \nsns.scatterplot(x=flowers_df.sepal_length, y=flowers_df.sepal_width, hue=flowers_df.species, s=70) \n# the color of the dots can be changed using hue=, while the size of the dots are modified using s= \nbox plots \nalso known as whisker plots are a statistical graph used on sets of \nnumerical data, it shows the range, spread and central tendency. they are \nmostly used to compare data from different categories. \n \nsimple boxplots \nplt.figure(figsize=(3,4))  \nsns.boxplot(x=flowers_df['species'], y=flowers_df['sepal_length'], palette='blues') \nheat maps \nheatmaps are used to see changes in behaviour or gradual changes in \ndata using different colors to represent different values, we can use the \npivot() method to create a heat map. \n\nsimple heat map \nflights_df = sns.load_dataset(\"flights\").pivot(index=\"month\", columns=\"year\", values=\"passengers\") \nplt.title(\"number of passengers (1000s)\") \nsns.heatmap(flights_df, fmt=\"d\", annot=true, cmap='blues') \nweek 5 \nexploratory data analysis \nexploratory data analysis (eda) is an important first step in the data analysis process to understand \nthe dataset better, and guide subsequent modelling and analysis. it involves looking at and visualising \ndata to understand its main features, finding variables that are relevant to our problem and \nunderstanding the relationships between variables. \n \neda can help us identify hidden patterns and relationships between different data points, helping us \nto build our machine learning models. insights obtained from eda help you decide which features \nare most important for building models and how to prepare them to improve performance. eda \nhelps us choose the best modelling techniques and adjust them for better results. \n \nkey steps in eda \n1. understand the problem and the data \n2. importing libraries \n3. loading/reading the dataset \n4. data inspection \n5. data cleaning \na. checking for missing values \nb. checking for duplicates \n6. analysing the dataset \na. univariate analysis inspecting one variable \nb. bivariate analysis inspecting two variables and their relationship \nc. multivariate analysis inspecting more than two variables and their relationships \n \n1 understand the \nproblem and the data \n● what is the business goal or research question? \n● what are the variables in the data and what do they represent? \n● what types of data (numerical, categorical, text, etc) do you have? \n● are there any known data quality issues or limitations? \n● are there domain-specific concerns or restrictions? \n2 importing libraries \n● numpy (np), pandas (pd), seaborn (sns), matplotlib (plt) \n3 loading/reading the \ndataset \n● df = sns.load_dataset() \n● df = pd.read_csv() \n4 data inspection \n● df.head(), df.tail(), df.info(), df.describe(), df.shape() \n● check data types using df.dtypes \n\n5 data cleaning \n● identify and handle missing values using methods like \ndf.isnull().sum( \n● find and address duplicates with df.duplicated.sum() \n6 univariate analysis \n● analyse single variable at a time \n● use descriptive statistics with df.describe() for numerical data \n● create histograms, box-plots, and density plots \nbivariate analysis \n● explore relationships between two variables \n● create scatter plots, pair plots \nmultivariate analysis \n● interactions between three or more variables in a dataset are \nsimultaneously analysed and interpreted in multivariate analysis \n● use heatmaps, line charts \nexample: insurance.csv \ninsurance = pd.read_csv('/content/insurance.csv') \ninsurance.head()   \n      \n# view top 5 rows \ninsurance.sample(6)      \n# show 6 random rows \ninsurance.info()      \n   \n# check for null values, data types, and total rows \ninsurance.columns    \n   \n # shows us all column names \ninsurance.describe()     \n# shows us summary statistics for numeric columns \ninsurance.describe(include='o')   \n# shows us summary statistics for categorical columns \ninsurance.region.unique() \n  \n# shows all unique values for a particular column \ninsurance.isnull().sum()    \n# shows a sum of all null values per column \ninsurance[insurance.duplicated()] \n \n# shows all the duplicated rows (observations) \ninsurance.drop_duplicates(keep='first', inplace=true) # removes the second duplicate row \n \n# univariate analysis for numerical variables \nplt.figure(figsize=(10,6)) \nsns.distplot(insurance.charges, color='lightblue') \nplt.title('charges distribution', size=18) \nplt.xlabel('charges', size=14) \nplt.ylabel('density', size=14) \nplt.show() \n \n# univariate analysis for categorical variables \nplt.figure(figsize=(10,6)) \nsns.countplot(x='smoker', data=insurance, hue='smoker') \nplt.title('smoker distribution', size=18) \nplt.xlabel('smoker', size=14) \nplt.ylabel('count', size=14) \nplt.legend(['smoker', 'non smoker']) \nplt.show() \n \n\n# bivariate analysis for numerical variables \nplt.figure(figsize=(10,6)) \nsns.scatterplot(x='age', y='charges', color='b', data=insurance) \nplt.title('age vs chargers', size=18) \nplt.xlabel('age', size=14) \nplt.ylabel('charges', size=14) \nplt.show() \n \n# bivariate analysis for categorical variables \nplt.figure(figsize=(10,6)) \nsns.set_style('darkgrid') \nsns.boxplot(x='smoker', y='charges', data=insurance) \nplt.title('smoker vs charges', size=18) \n \n# multivariate analysis for numericalvariables \nnumeric_df = df.select_dtypes(include=np.number) \nsns.heatmeap(numeric_df.corr(), annot=true, cmap='coolwarm') \nplt.title('correlation matrix') \nplt.show() \nfilling missing values \ndf['age'].fillna(df['age'].median(), inplace=true) \ndf['embarked'].fillna(df['embarked'].mode()[0], inplace=true) \nprint(df.isnull().sum()) \ngrouping values \ngrouped = df.groupby('pclass')['survived'].mean() \ngrouped.plot(kind='bar') \nweek 6 \nmachine learning \nmachine learning is a subfield of ai that focuses on developing algorithms that enable computers to \nlearn from data and make predictions or decisions without being explicitly programmed for each \ntask. ml allows machines to improve their performance over time by identifying patterns and \nrelationships within datasets.  \n \nmachine learning examples: \n● banks look for trends in transaction data to detect outliers that may be fraudulent.   \n● email inboxes use text to classify an email as spam or not, and adjust their classification rules \nbased upon how we flag emails.  \n● travel apps use live and historical data to predict traffic, travel times, and journey routes. \n● retail companies using data to recommend products \n\nthe tasks in the given examples above can be classified into at least one of the four broad groupings. \n1. finding trends in data \n2. classifying data into groups and categories \n3. making decisions and predictions \n4. learning to interact in an environment \n \ntypes of machine learning \n1. supervised ml algorithms \n2. unsupervised ml algorithms \n3. reinforcement ml algorithms \n \nclassical learning \n              ↓                                                             ↓ \nunsupervised  supervised \n↓ \n \n            ↓                                        ↓ \nclustering  regression  classification \n↓ \n \n↓ \n \n↓ \n● fuzzy c-means \n● mean-shift \n● k-means \n● dbscan \n● agglomerative \n \n● linear  \n● polynomial  \n● ridge/lasso  \n \n● k-nn \n● naive bayes \n● svm \n● decision trees \n● logistic regression \nsupervised learning algorithm \nsupervised learning needs external supervision to learn, it is trained using the labeled dataset. a \nsla takes a known set of input data and known responses to the data and trains a model to generate \nreasonable predictions for the response to a new data. once training and processing are done, the \nmodel is tested by providing sample test data to check whether it predicts the correct output. you \nshould use sla if you have known data for the output you are trying to predict. \n \neli5: if you see a shape that has four equal sides, label it as \"square\". this is fed into the model for \ntesting, it makes a prediction, and we verify whether it is correct or not. \n \nregression: if you have to predict a continuous variable, use regression algorithms. that is, if there is \na relationship between the input and output variables, it may predict continuous variables, such as \nweather forecasting, market trends, etc. \n \nclassification: classification algorithms are used when the output variable is categorical, which \nmeans there are two classes such as yes-no, male-fale, true-false, etc. \n \n \n\nunsupervised learning algorithm \nin unsupervised learning algorithms, the machine does not need any external supervision to learn \nfrom the data, they are trained using unlabeled dataset and draws inferences from data sets \nconsisting of input data without labeled answers. the model has no predefined output and tries to \nfind useful insights from a huge amount of data. \n \neli5: providing pictures of dogs and cats, the algorithm tries to find features, such as ears, tails, color, \nshape, etc. it will create a classification to provide an output. \n \nclustering: is a method of grouping objects into a cluster based on the most similarities between \ngroups, or no similarities with the objects of another. for example: clustering can help group \ncustomers with similar purchasing behaviours, which can be used for targeted marketing and product \nrecommendations. \n \nsupervised vs unsupervised \nsupervised: if you are training the model to make a prediction such as the future value of a \ncontinuous variable, such as temperature or a stock price, or a classification, such as identifying car \nmakers from webcam video footage. \n \nunsupervised: if you need to explore your data and want to train a model to find a good internal \nrepresentation, such as splitting data up into clusters. such as splitting customers based on different \nfeatures such as frequency, expensive shoppers, etc \nmachine learning workflow \n         \n→ \n' \ntest  \ndataset \nraw data        \n↓ \n         \n1. extract features \n→ \n2. split dataset \n→  \n3. train dataset \n→  \n4. train model \n→  \n5. evaluate \nmachine learning limitations \n● garbage in = garbage out. without data preprocessing or data cleaning, our dataset may be \nskewed and produce meaningless predictions \n● biases due to training data. the performance of an ml system depends on the quality of input data \nused to train it. for example, if we collect data on public transport use from only high \nsocioeconomic areas, the resulting input data may be biased due to a range of factors that may \nincrease the likelihood of people from those areas using private transport versus public options. \n● extrapolation. we can only make reliable predictions about data which is in the same range as our \ntraining data. extrapolation of the training data means we cannot be confident in our results \n● overfitting. sometimes, ml models become overtrained, for example, continuously finetuning the \nmodel to become overtrained. \n● inability to explain answers. ml techniques will return an answer based on the input data and \nmodel parameters, even if that answer is wrong. most systems cannot explain the logic used to \narrive at that answer. this can make detecting and diagnosing problems difficult. \n\nregression \nregression is a method that allows you to estimate how the dependent variable changes as the \nindependent variable(s) change. in this course, we will only focus on linear regression. some \nexamples of applications of linear regression include: how temperature affects ice cream sales, \npredicting the price of a house given house features, or predicting the impact of college scores on \nuniversity admissions. \n \nleast squares regression equations \nleast squares regression can calculate a predictive model. for example, with the simple linear \nequation of y = a + bx, the least squares can be calculated as: \n,  \n푏 = \nσ(푥 − 푥̄) * (푦 − ȳ)\nσ(푥 − 푥̄)\n2\n푎=푌−푏푋\nprediction models have limitations. for example, if we are using least squares regression to predict \nthe test score (0-100) of a student based on their hours spent studying, and we want to cast a \nprediction for for 20 hours spent, using the regression equation y' = 30.18 + 6.49 * x their predicted \nscore would be 160, which does not make sense for a score scale of 0 to 100.  \nregression example \nscikit-learn \nscikit-learn is a python package that provides machine-learning algorithms. \nfrom sklearn.linear_model import linearregression \nregressor = linearregression() \n \nimport seaborn as sns \nsns.relplot(x='scores', y='hours', data=stud_scores, height=3.8, aspect=1.8, kind) \nsns.set_style('darkgrid') \nsplitting the data \nsplitting the dataset is crucial in determining the accuracy of a model. if we were to train the model \nwith the raw dataset and predict the response for the same dataset, the model would suffer flaws \nlike overfitting, thus compromising its accuracy. for this reason, we have to split the data into \ntraining and testing sets. scikit-learn provides a train_test_split function for splitting datasets into \ntraining and testing subsets. we will split the dataset in the ratio of 70:30, where 70% will be the \ntraining set, and 30% will be for the testing set. \n \nx = stud_scores.iloc[:,:-1].values \ny = stud_scores.iloc[:,1].values \n \nfrom sklearn.model_selection import train_test_split \nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=1) \n \nfitting the model \nfrom sklearn.linear_model import linearregression \nregressor = linearregression() \nregressor.fit(x_train, y_train) \n\ny = mx + c \nm m = regressor.coef_ \nc c = regressor.intercept_ \n \nprediction \ny_pred = regressor.predict(x_test) \ny_pred \n \ncomparing the test values and predicted values \ncomparison_df = pd.dataframe({\"actual\":y_test, \"predicted\":y_pred}) \ncomparison_df \n \nfit error \nimport sklearn.metrics import mean_squared_error \nprint(\"mean squared error\", mean_squared_error(y_test, y_pred)) \n \nroot mean squared error (rmse) \nprint(\"rmse\", np.sqrt(mean_squared_error(y_test, y_pred))) \n \nweek 7 \nclassification algorithm: logistic regression \n \nf1 score \nimagine you have a basket of fruits and are trying to find the green apples. the f1 score checks how \ngood you are at picking the right fruits by evaluating the \"precision\" and \"recall\". precision is how \noften we are right, while recall is how many of the green apples we actually find.  \n \nprecision = true +ve's / true +ve's + false +ve's \nrecall = true +ve's / true +ve's + false -ve's \n  \nℎ푎푟푚표푛푖푐 푚푒푎푛 =\n푛\n1\n푥1\n + \n1\n푥2\n + \n1\n푥3\n +... + \n1\n푥푛\nharmonic mean increases as values increase, decreases as values decrease, directly proportional.  \n \n● for a balanced dataset, we should go for an accuracy score, otherwise a precision/recall/f1 \nscore. \n● if you intend that fp should be very small, go for the precision score \n● if you intend that fn should be very small, go for the recall score \n● if you intend that both fp and fn should be very small, go for the f1-score \n \n \n \n \n\ncancer diagnosis  \nfalse negatives are more dangerous \nbecause as a person thinks they are \nnot diagnosed with cancer, they will \nnot take any corrective actions, hence \nwe should use the recall score. \n actual predicted \nfalse positives cancer not detected cancer detected \nfalse negatives cancer detected cancer not detected \n \nspam emails  \nfalse positives are more dangerous \nbecause a person may miss an \nimportant email and will not take \naction, hence we should use the \nprecision score. \n actual predicted \nfalse positives not spam spam \nfalse negatives spam not spam \n \nthe titanic  \nboth fp and fn are dangerous. \nmarking someone as alive when they \nhave died, or marking someone as \ndead when they are alive, hence we \nshould use the f1 score \n actual predicted \nfalse positives not survived survived \nfalse negatives survived not survived \n \nin python \n#from sklearn metrics import accuracy_score,f1_score, precision_score,recall_score,confusion_matrix \n#from sklearn import metrics \n \n#accuracy_score(y_test, y_pred) \n#f1_score(y_test, y_pred) \n#precision_score(y_test, y_pred) \n#recall_score(y_test, y_pred) \n \nfrom sklearn.linear_model import logisticregression \nclassifier = logisticregression(random_state = 0) \nclassifier.fit(xtrain, ytrain) \ny_pred = classifier.predict(xtest) \n \nfrom sklearn.metrics import confusion_matrix \ncm = confusion_matrix(ytest, y_pred) \n \nfrom sklearn.metrics import accuracy_score \naccuracy_score(y_test, y_pred) \n \n\nweek 8 \nk - nearest neighbour (knn) \nis a supervised machine learning distance-based algorithm that can be used for classification and \nregression problems. it makes predictions by finding the 'k' closest data points in the training set to a \nnew data point and then basing its prediction on those neighbours. \n \nexample age vs loan: to predict brianna's default status (yes or no) \ncustomer age loan default \n1. calculate euclidean distance \n d = sqrt(x1 - y1)\n2\n * (x2 - y2)\n2\n \n sqrt(20 - 21)\n2\n + (60000 - 80000)\n2\n  \n \n2. calculate for k \n so if k=3, there is one default = y, two default = n \naulia 21 40000 n \nankita 21 40000 n \nmellisa 20 60000 y \nbrianna 21 80000 ? \n \nknn is a lazy learner as it parses through all data points for each classification, hence it is only useful \nfor smaller datasets, otherwise computational power, time and expense can burden exponentially. \nadditionally, knn fails for variables with different scales, so feature scaling (standardisation and \nnormalisation) are required before applying knn. \n \nin python \n# split dataset into training and test set \nknn = kneighboursclassifier(n_neighbours=7) \nknn.fit(x_train, y_train) \ny_pred = knn.predict(x_test) \naccuracy_score(y_test, y_pred) \n\n\n\n \nbusan302 week 07-12 \n \nweek 07.1.......................................................................................................................................2\n \nlogistic regression...........................................................................................................................2\n \nlinear regression vs logistic regression....................................................................................2\n \nlogistic regression in python.....................................................................................................2\n \nweek 07.2.......................................................................................................................................4\n \nf1 score............................................................................................................................................4\n \nfalse positives vs false negatives.....................................................................................................4\n \nweek 08.1.......................................................................................................................................5\n \nknn - k-nearest neighbour...............................................................................................................5\n \nexample: age vs loan.................................................................................................................5\n \nknn limitations................................................................................................................................6\n \nhow to choose the k-factor.............................................................................................................6\n \nknn in python – iris dataset............................................................................................................6\n \nknn in regression............................................................................................................................7\n \nweek 08.2.......................................................................................................................................7\n \nunsupervised machine learning......................................................................................................7\n \nk-means clustering...........................................................................................................................7\n \nweek 09.1.......................................................................................................................................9\n \nelbow method..................................................................................................................................9\n \nintra vs inter cluster distance..........................................................................................................9\n \nsilhouette score................................................................................................................................9\n \nfeature engineering.......................................................................................................................10\n \nabsolute maximum scaling [normalisation]..................................................................................10\n \nmin-max scaling.............................................................................................................................11\n \nmean normalisation.......................................................................................................................11\n \nstandardisation..............................................................................................................................11\n \nweek 09.2.....................................................................................................................................12\n \nhierarchical clustering....................................................................................................................12 \n \n \n\n \nweek 07.1 \nlogistic regression \ncategorical variables, such as gender (male/female), color (pink, blue, purple, orange), contain \ndifferent classes. while, continuous variables can take on any possible values, such as age, height, \nweight, salary, etc. when the dependent variable is dichotomous (0 or 1), (success or failure), we can \nuse a logistic regression. \n \naim: whether an employee would get a promotion or not based on their performance \na linear graph will not be suitable in this case. as such, we clip the line at zero and one and convert \ninto a sigmoid curve (s-curve). based on the threshold values, the organisation can decide whether \nan employee will get a salary increase or not. \n \nfor logistic regression, use probabilities. for the straight-line equation, y can take the values of 0 to 1, \nas opposed from -inf to inf for a linear regression. hence, to compare the two, we take the log of the \nodds ratio (log-odds), hence if 0 = p / 1 - p, then the log-odds will be . \n푙표푔(\n푝(푥)\n1 − 푝(푥)\n)\n \ntypes of logistic regression \n● \nbinary logistic regression: the target only has two possible outcomes, such as spam or not \nspam, cancer or no cancer. \n● \nmultinomial logistic regression: the target variable has three or more nominal categories \nsuch as predicting the type of wine. \n● \nordinal logistic regression: the order of categories is important, for example, if a variable \nhas three or more categories such as restaurant or product ratings from 1 to 5. \n \napplications of logistic regression \n● using the logistic regression algorithm, banks can predict whether a customer would default \non loans or not \n● predict weather conditions for a particular location (sunny, windy, rainy, humid,etc.) \n● ecommerce companies can identify buyers if they are likely to purchase a certain product \n● companies can predict whether they will gain or lose money in the next quarter, year, or \nmonth based on their current performance \n● to classify objects based on their features and attributes \n \nlinear regression vs logistic regression \nlinear regression: solving prediction problems, estimates the dependent variable when there is a \nchange in the independent variable, and is a straight line. \nlogistic regression: predicting the class of a variable, it helps calculate the possibility of a particular \nevent taking place, and is an s-curve. \n \nlogistic regression in python \n# reading dataset in \ncol_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label'] \ndf = pd.read_csv('/content/diabetes.csv', header=none, names=col_names) # add column names \n\n \n# selecting and subsetting variables from the dataset \nfeature_cols = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree'] \nx = df[feature_cols]  \n# extract the independent variables \ny = df['label']   \n# extract the dependent variable \n \n# splitting the dataset into training and testing sets \nfrom sklearn.model_selection import train_test_split \nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=16) \nthe dataset is broken into two parts in a ratio of 75:25. it means 75% data will be used for model \ntraining and 25% for model testing. random_state=16 is used as a set.seed() function. if you don't set \nrandom_state, the split could change each time you run it. \n \n# running the model and making a prediction \nfrom sklearn.linear_model import logisticregression   \n# import logistic regression \nlogreg = logisticregression(random_state=1, max_iter=1000)  \n# instantiate the model \nlogreg.fit(x_train, y_train)     \n# fit the model with data \ny_pred = logreg.predict(x_test) \nmax_iter=1000 sets the maximum number of iterations the algorithm will try to find the best model. \nlogistic regression is solved by an optimization algorithm, and if it doesn't find the best solution \nquickly, it keeps iterating. if you don't set max_iter, sometimes it will stop too soon (default is 100), \nand you’ll get a warning like \"solver failed to converge.\" \n \n# model evaluation: performance metrics [confusion matrix] \nfrom sklearn import metrics \ncnf_matrix = metrics.confusion_matrix(y_test, y_pred) \na confusion matrix is a table that is used to evaluate the \nperformance of a classification model. confusion matrix is the \nnumber of correct and incorrect predictions summed up class- wise \n \nclass imbalance: in the dataset, out of 768, if 700 rows have the 'label' as 0, and 68 rows have the \n'label' as 1. hence, we have more data for the '0' class and much less for the second class '1'. ideally, \nwe should have two rows that are relatively close to each other for a more acceptable analysis, data \ngiven to train to the model is biased.  \n \nthis is why we cannot use accuracy on its own, as a performance metrics, hence why we need a \nconfusion matrix. \n \n# model evaluation: performance metrics [accuracy] \naccuracy = correct_predictions / total_predictions \n \n퐴푐푐푢푟푎푐푦 = \n푇푃 + 푇푁\n푇푃 + 푇푁 + 퐹푃 + 퐹푁\nfrom sklearn.metrics import accuracy_score \naccuracy_score(y_test, y_pred) \n \n \n\n \n# model evaluation: performance metrics [precision score] \nout of the total predicted positives, how many are actually positive. precision= tp/(tp+fp). \n● total predicted positives →tp+fp \n● actual positives → tp \nfrom sklearn.metrics import precision_score \nprecision_score(y_test,y_pred) \n \n# model evaluation: performance metrics [recall score] \nout of the total actual positives, how many are predicted correctly as positive. recall = tp/(tp+fn) \n● total actual positives → tp+fn \n● predicted correctly as positive →tp \nfrom sklearn.metrics import recall_score \nrecall_score(y_test,y_pred) \n \nweek 07.2 \nf1 score \nf1-score is the harmonic mean of the recall and precision. it is useful when you need to take both \nprecision and recall into account. \nmean (x1+x2+x3+..+xn)/n \nthe harmonic mean will be high if all the values are high and will be low if all the values are low. \n \nfalse positives vs false negatives \ncancer diagnosis \n actual predicted \nfalse positive cancer not detected cancer detected \nfalse negative cancer detected cancer not detected \nfalse negatives are more severe than false positives because it opens a dangerous scenario where a \npatient is not treated for an underlying disease. a false positive might include unnecessarily \nmedicating someone and/or causing stress and anxiety to the patient, although the repercussions of \nthis is less severe as that for a false negative. \n \nspam emails \nfor spam emails, false positives are more dangerous because, declaring valid emails as spam would \nbe sent to the spam inbox and likely not reach the reader, and worse, will be deleted. \n \nimproving the accuracy of the model \nchange the rando_state value and run the model a few times and use the one with which you get the \nhighest accuracy. \n \n \n \n\n \nprecision, recall and f1 scores in python \nfrom sklean.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix \nfrom sklearn import metric \ncnf_matrix = metrics.confusion_matrix(y_test, y_pred) \n \naccuracy_score(y_test, y_pred) \nprecision_score(y_test, y_pred) \nrecall_score(y_test, y_pred) \nf1_score(y_test, y_pred) \n \nweek 08.1 \nknn - k-nearest neighbour \neli5: imagine you just moved to a new school and you're trying to guess what games a kid likes, but \nyou don’t know them yet. so, you look at the 3 kids (or any number k) who sit closest to them at \nlunch, their nearest neighbours. if most of those nearby kids like soccer, then you guess that the new \nkid probably likes soccer too. that’s k-nearest neighbours (knn), it makes a guess based on what the \nclosest “friends” are like. \n \nknn is a supervised machine learning algorithm as it involves a \nlabeled data set. \n \nif we consider k=6, for the star, we would consider all the points \ninside both rings, and the star would be classified as class a. \nalthough, if we consider k=3, then the star would be classified in \nclass b. \n \neuclidean distance \nthe difference between the data point and the consideration of other data points, squared, provides \nthe euclidean distance. other methods include manhattan, minkowski, etc \n \nexample: age vs loan \n1. our dataset has customer, age, loan and default (yes/no), for one datapoint, andrew, we do not \nhave a default value. andrews age is 48 and his loan amount is 142000.  \n2. the euclidean distance between andrew and john is 102,000.  \n3. we then take the euclidean distance for all the data points, and add a new column called \n\"euclidean distance\".  \n4. we then sort by ascending order, and find the five smallest distances. \n5. for k=5, we see that there are two default=n, and three default=y \n6. therefore, we predict andrew's default = yes \n \n\n \n \nknn limitations \n \nknn disadvantages \n- knn is a lazy learner, it requires all data points for each classification, so it's mostly better for \nsmaller datasets. knn performs very slowly. (computational cost) \n- it fails when variables have different scales. feature scaling (standardisation and \nnormalisation) is required before applying the knn algorithm to any dataset. otherwise knn \nmay provide wrong predictions. \nexample: for a computer, the age and salary are just pure numbers, hence it won't \nunderstand the context that someone who is 18 might have a lower salary (40,000) then \nsomeone older, such as 25 with a higher salary (250,000). hence, it does not understand the \ncontext. \n \nif you have these types in datasets, you should use scaling or transformation that brings the features \ninto the same scale. for example, making the range 0 to 1 (log transformations for example). \n \nknn advantages \n- simple to implement, only two parameters are required to implement knn \n- suitable for small datasets \n \nhow to choose the k-factor \n- there is no best way to determine the k value. most times it involves finetuning the k factor \nto compare whether or not the model performs better, until we find an appropriate one. \n- there is a loose rule of thumb that k=5, although it does not really matter too much \n- another loose rule of thumb, take the sqrt(n), where n=number of datapoints, usually an odd \nvalue of k is selected to avoid confusion between two classes of data. \n \nknn in python – iris dataset \n# import required packages \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.neighbors import kneighborsclassifier \nfrom sklearn.metrics import accuracy_score \nfrom sklearn import metrics \n \n# create feature and target variables \ndf = pd.read_csv('/content/iris.csv') \nx=df.iloc[:,:4] \ny = df.iloc[:, 4].values \n \n# split data into train and test sets \nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2,random_state=42) \n \n\n \n# generate a k-nn model using neighbors value \nknn = kneighborsclassifier(n_neighbors=7) \n \n# train or fit the data into the model \nknn.fit(x_train, y_train) \n \n# predict on dataset which model has not seen before \ny_pred=knn.predict(x_test) \n \n# check accuracy and matrix \naccuracy = accuracy_score(y_test,y_pred) \ncnf_matrix = metrics.confusion_matrix(y_test, y_pred) \n \nknn in regression \nexample dataset of height vs age \n \n \n \n \n \n \n \n \n \n \n \nweek 08.2 \nunsupervised machine learning \nmachine learning using unlabeled data (no correct answers, and no output to guide the analysis, \nthere is nothing right or nothing wrong). there are only inputs. the algorithm tries to group data \nbased on similarities itself. for example, running ml on understanding customers of a store. the \nalgorithm goes through the database and tries to form patterns on age, money spent, etc. \n \nk-means clustering \nk-means is an unsupervised clustering algorithm designed to partition unlabelled data into a \ncertain number (k) distinct groups. it finds data points that share important characteristics and \nclassifies them into clusters.  \n \nintra-class similarity: clusters data points within the same cluster are as similar as possible \ninter-class similarity: data points from different clusters are as dissimilar as possible. \n \n \n \n\n \nexample: k-means ml for grouping shapes \nnumber \nof sides \n1. randomly pick three clusters (k=3) and assign each shape to the nearest \ncentroid \n \ncluster 1: 3 3, 3, 2, 3  \ncluster 2: 5 4, 5, 6, 4, 6, 4   centroids: 3, 5, 8 \ncluster 3: 8 8, 7  \n \n2. recalculate centroid: take the average of each cluster’s numbers \n \ncluster 1: 3 11/4 = 2.75  \ncluster 2: 5 39/6 = 4.83   new centroids: 2.75, 4.83, 7.5 \ncluster 3: 8 15/2 = 7.5  \n \n3. repeat assigning shapes to the nearest centroid. eventually, this process \nsettles into stable clusters. \ncluster 1 is probably triangles \ncluster 2 is probably squares, pentagons, or hexagons \ncluster 3 is probably octagons \n3 \n4 \n5 \n6 \n4 \n8 \n6 \n4 \n3 \n2 \n3 \n7 \n \nk-means disadvantages \n- \nsensitivity to initial centroids: k means is sensitive to the initial selection of centroids and \ncan converge to a suboptimal solution \n- \nrequires specifying the number of clusters: the number of clusters k needs to be specified \nbefore running the algorithm, which can be challenging in some applications \n- \nsensitive to outliers: k-means is sensitive to outliers, which can have a significant impact on \nthe resulting clusters \n \nin python \nfrom sklearn.cluster import kmeans \nkmeans_model = kmeans(n_clusters=3) \nkmeans_predict = kmeans_model.fit_predict(x) \n \n \n \n\n \nweek 09.1 \nelbow method \nto find the best value of k, select the optimal number of clusters (k). wcss is the total within-cluster \nsum of squares, it measures the compactness of the clustering and we want it to be as small as \npossible. \n \nthe elbow method finds the best value of k. if we \nstart with 1, run a k-means algorithm, derive a wcss \nvalue, continue with k=2, k=3, etc until we get a \nsmall wcss value. \n \nin the diagram above, the red line shows the \noptimal k value because after that point, the graph \nno longer grows exponentially, hence small changes \nbecome negligible.  \n \n \nintra vs inter cluster distance \n \nintra-cluster distance: the distance between the centroid of a \nspecific cluster and its outermost point. \n \ninter-cluster distance: the distance between the outermost \npoints of each cluster from each other. \n \n \n \n \nsilhouette score \nthe silhouette score for a data point is calculated by  \n(푏−푎)\n푚푎푥(푎,푏)\n1 ideally, close data points within a cluster and far away from other clusters (good clustering) \n0 data points are on the border between clusters, indicating some overlap (average \nclustering) \n-1 datapoints might be assigned to the wrong cluster (poor clustering) \n \nif we took k=1, and got a ss of 0.3, then we took k=2, we got an ss of 0.35, then k=3, ss=0.4, then \nk=4, and we got a ss of 0.6, hence we can iterate over different values of k to find an optimal ss, that \nis, the closer to 1 the better. \n \n\n \nfeature engineering \neli5: imagine you are baking a cake. you don't usually just dump a raw egg or raw wheat into a bowl, \ninstead, you measure the amount of flour to add, and crack the egg into a bowl first. feature \nengineering is about outlier handling, transformations, scaling, etc before we dive into an algorithm. \nit includes remodeling raw data by selecting, combining, remodelling data to improve the quality of \nour data. without data preprocessing, we may not be able to rely on the results from the ml \nalgorithm. \n \nthe steps included in feature engineering \n- data understanding \nthere may be features we need and do not need in a study, decide or select the most \nrelevant features for the problem at hand. (anova) \n- feature extraction \nif we are given dob, we can create a new column of the age to make findings more \ninterpretable. calculating sales price * units sold as a new column \"total revenue\". \n- feature selection \nexcluding or including specific features according to the studies' needs, for example, \nexcluding the user id column is the ml algorithm does not warrant a unique identifier \n- feature creation / transformation \nchanging a text column such as gender into a numeric column by encoding male=1, \nfemale=0 \n- feature scaling and normalisation \nbringing features onto the same scale so each feature can contribute equally. for example, \nscaling age and salary in a dataset that is classified on a boolean predictor: \"purchased\" \nbecause age is in tens, while salary is in thousands. another example could be something \nweighed in grams and a price of 10 dollars representing two different things (the computer \ndoesnt know that). \n \nnormalisation: values are bound between 0 and 1 \nstandardisation: transforming data such that the average of the column is 0 and its variance (sd) is 1 \n \nabsolute maximum scaling [normalisation] \neli5: imagine you have cars of different lengths, the longest car is 10cm long, and the shortest is \n-8cm long. we then take the absolute maximum, and divide every row by that length. for example: \ncar length absolute maximum scaling \n9 9/10 = 0.9 \n8 8/10 = 0.8 \n5 5/10 = 0.5 \n10 10/10 = 1 \n-8 -8/1- = -0.8 \n-6 -6/10 = -0.6 \nit’s a way to shrink values proportionally while keeping their signs and relationships. \n \n\n \nin python \nimport pandas as pd \nmax_vals = np.max(np.abs(df)) \nprint((df - max_vals) / max_vals) \n \nmin-max scaling \n \n푠푐푎푙푒푑 푣푎푙푢푒 =\n푠푐표푟푒−푚푖푛\n푚푎푥−푚푖푛\nimagine you have a bunch of student's grades, with a minimum of 50 and a maximum of 100. this \nscales the grades into 0 to 1, 0 being the worst and 1 being the best. \n \nin python \nfrom sklearn.preprocessing import minmaxscaler \nscaler = minmaxscaler() \nscaled_data = scaler.fit_transform(df) \nscaled_df = pd.dataframe(scaled_data, \ncolumns=df.columns) \nscaled_df.head() \n \nmean normalisation \nsame as the min-max method but here instead of the minimum value, we subtract each entry by the \nmean value of the whole data and then divide the results by the difference between the minimum \nand the maximum value.  \n푠푐푎푙푒푑 푣푎푙푢푒 =\n푋푖−푋푚푒푎푛\n푋푚푎푥−푋푚푖푛\n \nstandardisation \nfirst, calculate the mean and sd of the data then we are to subtract the mean value value from each \nentry and then divide the result by the standard deviation.  \n \nin python \nfrom sklearn.preprocessing import \nstandardscaler \nscaler = standardscaler() \nscaled_data = scaler.fit_transform(df) \nscaled_df = pd.dataframe(scaled_data, columns=df.columns) \n \nlecture 09 example \n# transform gender into a binary outcome of 0 or 1 \nfrom sklearn.preprocessing import labelencoder \nlabel_encoder = labelencoder() \ndata['gender'] = label_encoder.fit_transform(data['gender']) \ndata['gender'].unique() \n \n \n \n\n \n# extract columns of interest and split the data \nx = data.iloc[:, [1,2,3]].values \ny = data.iloc[:, 4].values \nxtrain,xtest,ytrain,ytest = train_test_split(x,y,train_size=0.25,random_state=0) \n \n# standardise the x variables to the same scale \nsc_x = standardscaler() \nxtrain = sc_x.fit_transform(xtrain) \nxtest = sc_x.fit_transform(xtest) \n \nweek 09.2 \nhierarchical clustering \nhierarchical clustering helps you find natural groupings in data — even if you don't know how many \ngroups (clusters) there should be. agglomerative (bottom-up). divisive (top-down) \n \n \nalgorithm of agglomerative hierarchical clustering \n1. initialisation \ntreat each data point as a separate cluster \n2. computer distance matrix \ncalculate the distance between each pair of clusters (the euclidean distance). this results in \nan n x n distance matrix, where the distance between a cluster and itself is zero. \n \n \n \n p1 p2 p3 p4 p5 p6 \np1 0 1     \np2 2.5 0     \np3 4 1.5 0    \np4 5 2.5  0   \np5 4.2 2.3   0  \np6 5.5 6 2.6 1.8 3.2 0 \n \n \n\n \nin the above matrix, the shortest distance is 1. this is where we should merge p1 and p2 into a single \ncluster or group. we then re-calculate the distance matrix with -1 data points after 2 points are \nmerged. \n \nthe method of recalculating the distance depends on the linkage criterion used. \n● single linkage \n● complete linkage \n● average linkage \n● ward's method \n\n",
  "305": "\n\n305\n\ndiscrete random \nvariables\n\na small café sells 0, 1, 2, or 3 muffins per customer.\nx0123\np(x)0.10.40.30.2\nthe probability that a customer buys at least two muffins:\np(x ≥ 2) = 0.3 + 0.2 = 0.5\n0x < 0\n0.10 ≤ x < 1\n0.1 + 0.4 = 0.51 ≤ x < 2\n0.5 + 0.3 = 0.82 ≤ x < 3\n0.7 + 0.2 = 1x ≤ 3\nss = {0}\nss = {0, 1}\nss = {0, 1, 2}\nss = {0, 1, 2, 3}\nss = {}\nthe probability that the customer buys fewer than 2 muffins\np(x < 2) = f(2) = 0.5 \ndiscrete random variables: problem 1\n\na startup tracks the number of sales calls made per day\nx01234\np(x)0.050.150.40.30.1\nfind the expected number of calls per day.\ne(x) = 0(0.05) + 1(0.15) + 2(0.4) + 3(0.3) + 4(0.1) = 2.25\n\nfind the variance of the number of calls per day.\ne(x\n2\n) = 0\n2\n(0.05) + 1\n2\n(0.15) + 2\n2\n(0.4) + 3\n2\n(0.3) + 4\n2\n(0.1) = 0.15 + 4(0.4) + 9(0.3) + 16(0.1) = 6.05\nvar(x) = e(x\n2\n) - (e(x)\n2\n) = 6.05 - 2.25\n2\n = 6.05 - 5.0625 = 0.9875\n\nfind the standard deviation of the number of calls per day.\nsd(x) = √var(x) = √(0.9875) = 0.994\n\ndiscrete random variables: problem 2\n\na manager wants to decide whether to stock 2 or 3 units of a product daily.\nx0123\np(x)0.10.30.40.2\nfind the expected demand.\ne(x) = 0(0.1) + 1(0.3) + 2(0.4) + 3(0.2) = 1.70\n\nrecommend a stocking level (2 or 3 units) that will minimise the expected number of unsold units.\ncase 1: stocking 2 unitscase 2: stocking 3 units\n\n\n\n\n\n\ne(x\n2 units\n) = 2(0.1) + 1(0.3) = 0.5e(x\n3 units\n) = 3(0.1) + 2(0.3) + 1(0.4) = 1.3\nthe recommendation is to stock 2 units as there is less expected unsold units.\n\n\ndiscrete random variables: problem 5\ndemand = 0unsold = 2\ndemand = 1unsold = 1\ndemand = 2unsold = 0\ndemand = 3unsold = 0\ndemand = 0unsold = 3\ndemand = 1unsold = 2\ndemand = 2unsold = 1\ndemand = 3unsold = 0\n\ncontinuous random \nvariables\n\na user spends time on a website uniformly distributed between 0 and 10 minutes\nwhat is the probability a user spends more than 5 minutes on the website?\np(x > 5) = f(5) = 5/10 = 0.5\n\nwhat is the probability that the user spends at most 3 minutes on the website?\np(x ≤ 3) = f(3) = 3/10 = 0.3\n\nwhat is the probability that the user spends between 3 and 5 minutes on the website?\np(3 ≤ x ≤ 5)\n= f(3) = 3/10 = 0.3\n= f(5) = 5/10 = 0.5\n= f(5) - f(3) = 0.5 - 0.3 = 0.2\n\ncontinuous random variables: problem 1\n\ncalls arrive to a call center at a rate of 10 customers an hour on average. the time between calls is\nexponentially distributed.\nwhat is the probability that the second call is received at most 10 minutes after the first call? \nthe rate of calls was given in calls per hour, so 10 per hour = 10/60 = ⅙ hours\np(wait time ≤ ⅙)\n= f(1/6) = 1 - e\n-10(1/6)\n \n= 1 - e\n-1.666\n \n= 1 - 0.18900 = 0.8111\n\nwhat is the probability that there is more than an hour between two calls?\np(wait time > 1 hour)\n= e\n-10(1)\n\n= e\n-10\n\n= 0.00005\n\ncontinuous random variables: problem 2\n\nconditional probability\n\nat a coffee shop, 60% of customers order coffee, and 40% order tea. of those who order coffee,\n25% also order a muffin\nwhat is the probability that a randomly selected customer orders coffee and a muffin?\np(coffee and muffin) = 0.6 * 0.25 = 0.15\n\ngiven that a customer ordered coffee, what is the probability they also ordered a muffin?\np(muffin|coffee) \n= p(muffin and coffee) / p(coffee)\n= 0.15 / 0.6 = 0.25\n\n\n\n\nwhat is the probability that a recipient opens the email and clicks the link?\np(opens email and click link) = 0.7 * 0.2 = 0.14\n\nwhat is the probability a recipient clicks the link given they opened the email?\np(click link | opens email)\n= p(click link and open email) / p(opens email)\n= 0.14 / 0.7 = 0.20\nconditional probability: problem 1 & problem 2\na company sends out promotional emails. 70% of recipients open the email. of those who open\nit, 20% click on the link\n\nthe time a user spends on a website is uniformly distributed between 0 and 10 minutes.\nwhat is the probability a user spends more than 5 minutes?\np(x > 5) = 5/10 = 0.5\n\ngiven that a user has spent more than 2 minutes on the website, \nwhat is the probability they spend at most 5 minutes (total) on the website?\np(x ≤ 5 | x > 2)\n= p(2 < x ≤ 5) / p(x > 2)\n= 0.3 / 0.8  = 0.375\n\ngiven that a user has already spent 2 minutes on the website, \nwhat it the probability that they spend an additional 5 minutes on the website?\np(x > 7 | x > 2)\n= p(x > 7) / p(x > 2)\n= 0.3 / 0.8 = 0.375\nconditional probability: problem 3\n1     2     3     4     5     6     7     8     9    10\np(2 < x ≤ 5)\n1     2     3     4     5     6     7     8     9    10\np(x > 2)\n0.1    0.1   0.1   0.1   0.1    0.1   0.1   0.1    0.1   0.1\n0.1    0.1   0.1   0.1   0.1    0.1   0.1   0.1    0.1   0.1\n3(0.1)= 0.3\n8(0.1) = 0.8\n\nthe time it takes to help customers are a service desk is exponentially distributed with rate 5\ncustomers per hour (or 1/12 customers per minute)\nconditional probability: problem 4\nwhat is the probability it takes more than 6 minutes to help a customer.\np(x > 6) = f(6) = e\n-1/12(6)\n = 0.60653\n\ngiven that an employee has already been helping a customer for 3 minutes, what is the probability they spend more than 6 \nminutes (total)?\np(x > 6 | x > 3)\n= p(x > 6) / p(x > 3)\n= 0.60653 / f(3)\n= 0.60653 / e\n-1/12(3)\n\n= 0.60653 / 0.77880 = 0.7788\n\ngiven that an employee has already been helping a customer for 6 minutes, what it the probability that they spend at least \nadditional 6 minutes on the website?\np(x > 12| x > 6)\n= p(x > 12) / p(x > 6)\n= f(12) = 1 - e\n-1/12(12)\n = 0.36788\n= f(6) = 1 - e\n-1/12(6)\n = 0.60653\n= 0.36788 / 0.60653 = 0.6065\n\na cafe manager is analysing customer purchasing behaviour. they have noticed that 90% of customers \npurchase a drink, and 60% of customers purchase food. 45% of customers purchase both food and drink. \nconditional probability: week 3 quiz\ngiven a particular customer orders a drink, what it the probability that they will also purchase food?\n\np(purchase food| order drink)\np(food and drink) = 0.45\np(orders drink) = 0.9\n0.45/0.9 = 0.5\n\n",
  "309": "\n\nbrianna yeung (byeu337)\ninnovent309: concept mapping & recorded reflection\n\nbackbone\n●responsibility is owning a situation and making it \nright even if you aren't directly to blame for it\n\n●innovation improves or replaces something\n\n●ri is about avoiding harm and doing good\n\n●ri balances responsibility and ethical values with \neconomic value\n\n\n\n\n\nresponsibility and main principles\n\n●we can apply ri in a variety of ways to \nbusiness operations to address \nchallenges & capture opportunities.\n\n\n\n\n\n\n●science and technology are \ninterdependent but distinct\n\n●innovation links science and \ntechnology\n\n●value is what underpins \ntechnology  and innovation \ndevelopment\n\n●business models can \nincorporate responsibility, ethics \nand sustainability in a company\n\n\n\ninnovation and main principles\nnote: some illustrations have been simplified for clarity in displaying direct relationships, but the actual \nrelationships from the map remain unchanged.\n\nunderlying relationship: what makes up responsible innovation\n\n●responsibility and ethics make up values\n\n●values push to avoid harm and do good\n\n●this fuels the responsible innovation process\n\n●this process is two-way with stakeholders and \niterative\n\n●this process captures value out of responsible \nproducts/services\n\nfinal reflections\n\n",
  "316": "\n\ncyber security\n\ncomputer security\n\"measures and controls that ensure confidentiality, integrity, and availability of \nthe information processed and stored by a computer\"\nthe cia triad\n\ndefinitionexamplepurposemitigationopposite of cia\nconfidentiality\ninformation is safe from \ndisclosure.\ni send you a message, and \nno one else knows what that \nmessage is.\ndata is not disclosedencryptiondisclosure\nintegrity\nthe information is safe from \nmodification or alteration.\ni send you a message, and \nyou receive exactly what i \nsend you.\ndata is not tamperedhashingalteration\navailability\ninformation is available to \nauthorised users when \nneeded.\ni send you a message, and \nyou are able to receive it.\ndata is available\nbackups, redundant \nsystems\ndescripción\n\nconfidentiality\ndata confidentiality\nensures confidential \ninformation, such as a \nstudent's grades, is not \nmade available or disclosed \nto unauthorised individuals.\nprivacy\nassures individuals control \nover what information related \nto them may be collected, \nstored and shared, e.g., data \ngenerated by smartphone\n\nintegrity\ndata integritysystem integrity\nassures completeness and \naccuracy of data\nprotection against corruption or \nunauthorised changes\nmethods: eg: checksum, \nbackup, version control\nassures a system performs its \nintended\nfunction w/o deliberate \nmanipulation.\ncounterexample: compromised \nmachine, hacked website\navailability\nassures that systems, applications and data accessible to authorized \nusers.\ncounterexample\n-a system under denial of service (dos)\n-ransomware: criminal encrypt files for ransom\n\n\nadversary or attacker\nhacker\nattack\ncountermeasure\nrisk\nsecurity policy\nsystem resource or asset\nthreat\nvulnerability\nan entity that attacks, or is a threat to, a system.\nblack hat hacker: who attempts to gain unauthorised access or entry into a system.\nwhite hat hacker: an individual who helps organisations to strengthen the security of a system.\nan action that compromises security of the system.\ninside attack: initiated by an entity (i.e., an insider) inside the security perimeter\noutside attack: initiated from outside the perimeter (i.e., an outsider), by an unauthorised entity\nan action, procedure or technique that reduces a threat, a vulnerability, or an attack.\nthe probability that a threat will exploit a vulnerability with a particular harmful result.\na set of security rules and practice guidelines that specify or regulate how a system or organisation\nprovides security services. the goal is to protect sensitive and critical system resources.\ndata contained in an information system, a service provided by a system, and system capability \nsuch as processing power or communication bandwidth.\na potential for violation of security. could breach security and cause harm. a possible danger that \nmight exploit a vulnerability.\na flaw or weakness in a system’s design, implementation, or operation and management that could \nbe exploited.\n\nloss of security\nloss of confidentiality: unauthorised disclosure of information\nloss of integrity: unauthorised modification or destruction of information\nloss of availability: disruption of access to or use of information or services\nlevels of impact due to loss of security\n\nlow\nmedium\nhigh\nminor damage or harm; minor loss\nserious adverse effect; significant damage or \nloss\nsevere or catastrophic adverse effect; major \ndamage or loss\n\nconfidentiality\nintegrity\navailability\nlow\nmedium\nhigh\nlow\nmedium\nhigh\nlow\nmedium\nhigh\n\nnetwork security\n\"network security is the process of taking physical and software preventative measures to protect \nthe underlying networking infrastructure from unauthorised access, misuse, malfunction, \nmodification, destruction, or improper disclosure, thereby creating a secure platform for computers, \nusers and programs to perform their permitted critical functions within a secure environment”\nthe osi* security architecture\nthe framework of protocols, standards, and mechanisms designed to ensure the security of data \nand communications within a network environment following the osi model's layered approach.\nsecurity attack\nsecurity service\nsecurity mechanism\nan action that compromises security of system or exchanged information\na service that enhances security of system or exchanged information\na mechanism that is designed to detect, prevent, or recover from a security attack.\n\npassive security \nattack\nactive security \nattack\nrelease of message content \nan eavesdropper intercepts and reads sensitive email exchanges between executives of a company.\ntraffic analysis\nan attacker monitors the frequency and timing of messages between two parties to infer patterns and \npossibly the importance of the communication without reading the actual messages.\nmasquerade\nan attacker pretends to be a legitimate user by using stolen credentials to access restricted areas of a \nnetwork.\nreplay\nan attacker intercepts a valid data transmission and retransmits it to create an unauthorised effect, such as \nreusing an authentication token to gain access.\nmessage modification\nan attacker intercepts a message, alters its content, and sends it on to the intended recipient, causing \nmisinformation or a change in the message's intended action.\ndenial of service (dos)\nan attacker floods a server with excessive requests, overwhelming the system and preventing legitimate \nusers from accessing the service.\n\nrelease of \nmessage content\ntraffic analysismasqueradereplaymessage \nmodification\ndenial of service\nauthentication\naccess control\nconfidentiality\n(message)\n\nconfidentiality\n(header)\n\ndata integrity\nnon-repudiation\navailability\na process of identifying and verifying \nwhether the communicating entity is \nthe one it claims to be.\na technique used to regulate access \nto resources\nprotection of the data\nensuring received data is not \ntampered by unauthorised entities\nprotection against denial by \ncommunicating entities\nthe property of a system being \naccessible and usable upon demand\nsecurity services\n\nsecurity mechanisms\nencoding messages in such a way that only \nauthorised parties can read it\nencryption\na cryptographic technique that allows recipients to \nvalidate message authenticity\ndigital signature\ntechniques for enforcing access rights\naccess control \nmechanism\na document that has been signed by a notary public \nin order to make it official or legal\nnotarization\na secret word or phrase known to an authorised party\npassword\n\nencryptiondigital \nsignature\naccess \ncontrol \nmechanism\nnotarisationpassword\nauthentication\naccess control\nconfidentiality\n(message)\n\nconfidentiality\n(header)\n\ndata integrity\nnon-repudiation\navailability\na process of identifying and verifying \nwhether the communicating entity is \nthe one it claims to be.\na technique used to regulate access \nto resources\nprotection of the data\nensuring received data is not \ntampered by unauthorised entities\nprotection against denial by \ncommunicating entities\nthe property of a system being \naccessible and usable upon demand\nsecurity services vs mechanisms\n\ncomputer security\n\"measures and controls that ensure confidentiality, integrity, and availability of the information \nprocessed and stored by a computer\"\nnetwork security\n\"measures to protect the underlying networking infrastructure from unauthorised access, misuse, \nmalfunction, modification, destruction, or improper disclosure, thereby creating a secure platform for \ncomputers, users and programs to perform their permitted critical functions within a secure \nenvironment”\ncyber security\n\"the collection of tools, policies, security concepts, security safeguards, guidelines, risk management \napproaches, actions, training, best practices, assurance and technologies that can be used to protect \nthe cyber environment and organization and user’s assets\"\n\ncyber security\noverlaps with information security\nmore specific to cyberspace\nincorporates the human factor\nrelates to the role of human in the security process\ncyber security is not only about the \ncia triad\ncyber crime/cyber bullying/ cyber \nterrorism\nnz’s cyber security \nstrategy\n\n– exercising cyber resilience\n\n– having cyber capabilities\n\n– improving cyber security\n\n– increasing international \ncooperation\n\n\ngdpr\n\ngdpr replaces dpd\ngdpr legally protects personal data of eu citizens outside of the eu.\n\ngdpr extends the definition of personal data to:\n\ndata protection directive (dpd)\ndeals with the protection and processing of personal data excluding citizens outside of the eu\nphotos, audio, videos, financial transactions, social media posts, etc.\ndevice identifiers (ip address, imei number)\nbrowsing history\ngenetic information\n\ntypes of data\npersonal data\nde-identified data\nanonymized data\nany information related to an identified or identifiable\nperson. eg id, gender, address\npersonal data without any personal identifiers\neg anonymous post in fb group\npersonal data without identifiers, where it is impossible to re-identify\neg average age of first home buyers\ntypes of subjects\ndata subject\ndata controller\ndata processor\nany person whose personal data is being collected, held, or\nprocessed eg students and staff\nan entity that determines the purposes and means of personal\ndata processing. eg uoa\nan entity that handles personal data on behalf of data controllers\neg google (gmail)\n\ngdpr principles\n■ accuracy\n-states that data gathered from clients must be stored correctly and updated regularly\n■ lawfulness, fairness, and transparency\n-states that the gathered data must be handled legitimately, impartially, and transparently\n■ purpose limitations\n-states that clients’ data must not be utilized in ways unknown to the client\n■ data minimization\n-states that only data directly relevant to a specified purpose should be collected\n■ accuracy\n-states that data gathered from clients must be stored correctly and updated regularly\n\n\n\n\n■ confidentiality\n-confidentiality states that only those with the requisite authorization are allowed to retrieve clients’ data.\n■ integrity\n-integrity, on the other hand, states that the retrieved client’s data should only be altered by those who have been authorized to carry out \nsuch alterations\n■ accountability\n-accountability principle requires you to take responsibility for what you do with personal data and how you comply with the other principles.\nin terms of the cia triad\n\ngdpr key changes\nincreased territorial scope (extraterritorial applicability)\n-gdpr applies to all companies processing the personal data of eu data subjects, data controllers and data processors.\npenalties\n-maximum fine: up to 4% of annual global turnover or €20 million (whichever is greater)\nconsent\n-the request for consent must be given in an understandable and easily accessible form with the purpose stated.\n-consent must be clear and in plain language\n-consent is easily withdrawable.\n1\n2\n3\n\ndata subject rights\nbreach notificationmust be done within 72 hours after becoming aware of the breach.\n\ndata controllers and data processors are required to notify \ntheir customers\nright to accessa confirmation from data controllers as to whether or not personal data concerning them \nis being\nprocessed, where and for what purpose\nthe controller shall provide a copy of personal data free of \ncharge amd in electronic format\nright to be forgottenalso known as data erasure, entitling data subjects to have the data controller erase \npersonal data.\ndata controllers must check “the public interest in the \navailability of the data”\n\ndata portabilitydata subjects have the right to request and receive personal data that have been \nprovided to data controller in a structured, readable format. also, subjects can request \ndata to transmit from one controller to another. \ndata controllers should accept and send their data. this \nonly applies to personal data and not anonymous data.\nprivacy by designinclusion of data protection from the early stages of the system design. the controller \ncan hold and process only the data exactly necessary for the completion of its duties.\nthe controller shall implement “data protection through\ntechnology design”\n\npasswords\n\nidentification vs authentication\nthe process in which a system \nentity provides its claimed \nidentity\n\neg: upi (unique personal \nidentifier)\nthe process of verifying an \nidentity claimed by a\nsystem entity\n\neg: pin (personal \nidentification number) or\npassword\n\nidentification \nauthentication\n\npassword vulnerabilities\noffline dictionary attack\nspecific account attack\npopular password attack\npassword guessing (single user)\nworkstation hijacking\nexploiting user mistakes\nexploiting same password use\nelectronic monitoring\nattackers gain access to password files (hash values of passwords) and \ncompare these hashes against hashes of commonly used passwords.\nattackers target a specific account, submits password guesses until \nsuccessful.\nattackers try commonly used passwords such as \"qwerty\" or \"1234\" \nagainst a wide range of user id's.\ngaining knowledge about an individual user and knowing system \npassword policies together to guess a password.\nthe attacker waits until a logged-in workstation is unattended\nusers writing their passwords down as they are hard to remember, where \nan attacker uses social engineering to trick user into revealing password.\nattackers learn a password from one source, and this ends up being the \nsame password for multiple services for one user.\ncommunicating a password in plaintext is vulnerable to eavesdropping\nprevent access to hashed password files or rapid \nreissuance of passwords.\ncounter-measures\nlock out mechanism after certain number of attempts or \ndelay subsequent tries.\nenforce complex password policies, intrusion detection, or \nscanning ip's to submission patterns.\neducate users and enforce complex password policies.\nlogging out after a certain period of inactivity or intrusion \ndetection schemes that detect changes in user behaviour\nchanging default passwords such as \"stationery\" or to \neducate users.\neducate users to choose different passwords for all sites.\nnever send a password in plaintext or technical solutions \nfor secure transfer of passwords\n\ncountermeasures\nhash function\npassword\nsalt\n(random number)\n1.the user provides user id and password\n2.lookup the corresponding salt and hash\n3.re-compute the hash based on the retrieved \nsalt and entered password\n4.if the result matches, password is accepted.\n●difficult to guess if one user chooses the same password for \nmultiple services.\n●difficult to guess if multiple users choose the same password for \na single service (or more)\n●makes offline dictionary attack difficult \n●protection against rainbow tables, pre-computed hash values \n(eg with every possible salt). the solution is to use a large salt.\nsalt with hashmulti-factor \nauthentication\npin/password\nsmartcard\nfingerprint\nvoice pattern or \nbehaviour analysis\n\nman-in-the-middle attack (mitm)\nrequests to download file\nplease insert email address\nsends email addressi forgot my password.\nsecurity question what is the name of your best friend?\nsecurity question what is the name of your best friend?\nanswer: bobanswer: bob\nchoose new password\ncountermeasures:\n-educate users\n-use mfa (multi-factor authentication\n-better notifications to users for password reset\n-phone call and reply by voice in case of password reset\n\naccess control\n\nauthorisation\na process of granting rights or permissions to a system entity to provide access to \na given resource.\nreliable inputs\nprinciple of least privilege\nadministrative duties\nrequirements\nauthenticated entities. eg upi and password\ndeals with granting the minimum set of access rights to do a\njob. eg accessing a single course vs all courses\nonly a special entity should be able to manage access \nrights. eg granting, revoking, or updating access rights\n\nelements to access control\ns\no\nsubject: an entity that can access objects.\nobject: an entity that needs to be protected.\nr\nright: describes how a subject can access and object.\nfunction(s, o, r): looks up access right for combination (s,o). on successful match, it grants access, otherwise not.\n\nsecurity administrator: an entity that manages access rights.\nauditor: an entity that inspects the whole authorisation system.\n\naccess control models\ndiscretionary access control (dac)\nrole-based access control (rbac)\n●resource owner decide who can access and level of access.\n●access is granted based on identity of the requester.\n●vulnerable to trojan horses.\n●dac is used in operating systems (linux file permissions: rwxr-x--x, \nread(r), write(w), execute(x))\nfile1file2file3file4\nalice\nown\nread\nwrite\nown\nread\nwrite\n\nbob\nreadown\nread\nwrite\nwriteread\ncharli\nread\nwrite\nreadown\nread\nwrite\n●rbac maps roles (e.g., in an organization) to access\n●rights\n●supports complex access control\n●reduces errors in administration, compared to user- assigned access\n●ease of administration\n-move users in and out of roles\n-move permissions in and out of roles\n-very flexible, compliance within organization\n●least privilege\n-restricts access according to needs\n-separation of duties through constraints\n\nmalware\nmalware is a set of instructions (programs) that run on your computer aiming to compromise \ncia.\nmalicious software\n\nmalware types\nvirusworm\ntrojan horse\nan executable or script that replicates itself\nit appears to have a useful function, but also has \na hidden and potentially malicious function\na computer program that can run independently \nand propagate itself over the network\nspyware\nsoftware that collects information from and \ntransmits it to another system\nransomware\nstealing data and asking for money to get it back\nbotnets\nreceive commands from remote command and \ncontrol (c&c) servers\ncountermeasures\nanti-virus\nphishing-detection \nsystems\nblacklisting ips\nfirewalls\nmalware detection\npatch your machine\nintrusion detection \nsystem (ids)\nintrusion prevention \nsystem (ips)\neducate users\n\neg: document based malware\n●the malware embeds malicious codes into documents, pdfs, spreadsheets, and other files\n●once on your computer, it can perform various malicious tasks such as stealing passwords or infecting your email contacts\n●the worst part is that you can activate the malware through normal day-to-day tasks such as opening your emails.\n●document-based malware overrides your commands, altering your computer, causing damage and further spreading the malicious code\n\n●often, the malware tampers with word processors by adding, deleting, and changing words within your documents\n●it moves text, adds images, and corrupts your hard drive.\n●it can infiltrate your email, sending unsolicited emails to your contact list. unsuspecting recipients will open your emails in good faith, \nspreading the virus into their computers.\n\neg: advanced persistent threat (apt)\n●using a wide variety of intrusion technologies and malware while applying persistently and effectively to specific targets where it runs over an \nextended period.\n\neg: zero-day\n●an unknown flaw that is discovered but does not have a patch or other fix (0-day to prepare a patch) where it can be exploited.\n\neg: backdoor\n●any mechanism that bypasses a normal security check\n●it may allow unauthorized access to data or program functionalities\neg: social engineering\n●psychological manipulation to trick users into doing security mistakes or giving away sensitive information\n●phishing is the most popular type: email and text message aimed at creating a sense of urgency, curiosity or fear in victims\n\n\n\n \nlecture 2 \ncomputer security: measures and controls that ensure cia of the information processed and stored by a computer. \n\"cybersecurity\" is often used interchangeably with \"information security\" because both focus on protecting the cia triad. we \nwant computer security to be delivered inside the cia triangle. hence, we need the security service to either deliver one of the \ndimensions, or a combination of the dimensions. \ncia triad \nconfidentiality \n1) data confidentiality: protection of personal/sensitive information from being revealed to unauthorised users \n2) privacy: assures individuals have control over their information. hence, if anyone else wants data, they require consent \nand a notation of how data will be used. \nintegrity (intact of data) \n1) data integrity: completeness and accuracy of data (maintain fidelity of data upon transfer or temperament, usually \nthrough checksum, backup or version control) \n2) system integrity: assures the system performs its intended function uninterrupted, without manipulation (such as a \ncompromised machine or a hacked website) \navailability \n1) keeping the system accessible to authorised users, and systems are delivered without interruptions such as dos or \nransomware. \nsecurity terms \nadversary or attacker: an entity, individual, agency, state that attacks, or is a threat to a system, such as a hacker or governments \nblack hat hacker: attempts to gain unauthorised access into a system. \nwhite hat hacker: works with organisations to strengthen the security of the system. \nattack: an action, that has already happened, that compromises or breaches security of a system \n1) inside attack: someone inside an organisation that explores something they aren't supposed to. \n2) outside attack: someone outside an organisation that tried to exploit or gain resources and perform harm \ncountermeasure: preventative action or procedure to prevent an attack \nrisk: the possibility that a threat will exploit a vulnerability with a harmful result \nsecurity policy: set of rules that sets up security regulation \nsystem resource or asset: data, a service, a system capability (processing power, communication bandwidth) \nthreat: the potential violation of security principles \nvulnerability: a flaw or weakness in a systems design, implementation, management, or operation that can be exploited \nloss of security: loss of confidentiality, loss of integrity, loss of availability \nlevels of impact due to loss of security \nlow:  minor damage or harm, minor loss \nmedium: serious adverse effect; significant damage or loss \nhigh:  catastrophic adverse effect; major damage loss \neach dimension of the cia, we have a low, medium, and high impact. \nnetwork security: a set of preventative measures that is supposed to prevent network infrastructure from a series of \nmalicious use, to establish a secure environment so functions promised to users are carried out. \nwhen discussing network security, we use osi* security architecture \nosi* security architecture \n1) security attack: an action that compromises security of the network (exchanged information) \n2) security service: a countermeasure for a security attack (a program, algorithm, hardware implementation) \n3) security mechanism: is a program, machine, algorithm that actually implements security service. \ntypes of security attacks \n1) passive \nrelease of message content: intercepting a message and observes or listens to the message. \ntraffic analysis: intercepts a message and observes metadata (frequency of message, packet size, period of send) \n2) active \nmasquerade: impersonating someone else (usually combined with another type of attack in order to gain information \nabout the person they are impersonating, such as a replay attack) \nreplay: capturing a message without altering it for a masquerade \nmessage modification: changing the message to the receiver \ndos: overwhelming servers to disable a server from delivering its intended service by flooding it, resulting in it \ncrashing or slowing down. \ntypes of security services \nauthentication: verifying that someone is who they claim they are \naccess control: regulates the access of resources \nconfidentiality: protection of data \n\n \ndata integrity: keeping data untempered \nnon-repudiation: making system accessible to legitimate users \navailability: non-interruption of service that is supposed to be delivered \neach type of security attack targets one of the security services. \nmasquerade → authentication / access control \nrelease of message content → confidentiality (message) \ntraffic analysis → confidentiality (meta-data) \nmessage modification / replay → data integrity \ndenial of service → availability \ntypes of security mechanisms \nencryption: encoding messages so only authorised parties can read it \ndigital signature: hash values and encryption to deliver authentication services \naccess control mechanism: implemented techniques to enforce assess rights \nnotarisation: using a trusted party to assure safe data exchange \npassword: ensuring you are you and not someone else \neach type of security service targets one or more security mechanism \nencryption → authentication, confidentiality, data integrity \ndigital signature → authentication, data integrity, non-repudiation \naccess control → access control \nnotarisation → non-repudiation \npassword → authentication \ncyber security: a set of tools (policies, safeguards, training, etc) to protect a safe cyber environment. \ncyber security vs information security: \nboth refer to the cia triad, but cyber security is more specific to cyberspace and incorporates the human factor (such as the \npsychological features of humans such as curiosity or laziness). cyber security also involves cybercrime, cyber bullying, and \ncyber terrorism. \nnew zealand's cyber security strategy: \n- exercising cyber resilience \n- having cyber capabilities \n- improving cyber security \n- increasing international cooperation \nlecture 3 \ndpd protects us citizens within the eu. dpd was later replaced by gdpr which protects citizens outside the eu. gdpr also \nextends personal data to include audio, video, financial transactions, social media posts, ip address, imei numbers, browsing \nhistory and genetic information. \ngdpr purposes: \n- harmonises data privacy laws across eu \n- protects and empowers eu citizens data privacy \n- reshapes the way organisations across the region approach data privacy \nin gdpr, data refers to personal, de-identified, and anonymised data \ngdpr terms \ndata subject: any entity that has information that can be processed by someone else. eg: me. \ndata controller: the one that determines the purpose of data collection. eg: uoa \ndata processor: an entity that engages with data on behalf of data controller eg: google \ngdpr principles \nlawfulness, fairness, transparency: gathered data must be handled legitimately, impartially, and transparently. \npurpose limitations: data should not be used in ways unknown to clients \ndata minimisation: only data directly relevant to a specified purpose should be collected \naccuracy: data must be updated regularly \nconfidentiality and integrity: only authorised entities can collect data, and data should be collected in a legal way \naccountability: expectation that the subject has responsibility for what they do with their data. \ngdpr key changes \nincreased territorial scope: applied to all companies that handle eu data subjects \npenalties: enforces monetary penalties (fines), up to 4% of annual global turnover of an organisation, or 20 million pounds \nconsent: request for consent in order to collect data, in a understandable and easily accessible form, and can be withdrawn \ndata subject rights \nbreach notification: within 72 hours of a data breach, data subjects must be informed by the data controllers and processors \n\n \nright to access: confirmation has been to be issued by the data collector, including how and why data is being collected, also, \nthe controller shall provide a copy of personal data at any time upon subject request, for free and in electronic format. (this is a \nchange from gdp to gdpr where data transparency and empowerment of data subjects is highlighted) \nright to be forgotten: entitles subjects to have controller erased, and enforces controllers to check that data is of public interest. \ndata portability: empowers data subjects the right to retrieve their data, and the right to transmit data to another controller. this \nonly applied to personal data, not anonymised data. \nprivacy by design: data privacy should be embedded at the design (early) stage, which also enforces data minimisation, upon \ndesigning software, data collection should be minimised as much as possible. \nlecture 4 \nidentification & authentication \nidentification is the process in which a system entity provides its claimed identity. (upi) \nauthentication is the process of verifying an identity claimed by a system entity (password) \ntypes of password vulnerabilities countermeasures \noffline \ndictionary \ngaining access to a dictionary of common \npasswords hash-converted and trialling and \nerroring these hashes against a password file. \n● prevent unauthorised access to password files \n(intrusion detection to identify a compromise) \n● rapid reissuance of passwords \nspecific \naccount  \nattacker guesses one person's password until it \nis correct \n●  lockout mechanism after a number of failed \nattempts \n● progressively increase delay after each incorrect \nattempt. \npopular \npassword  \nguessing easy / common passwords such as \n\"qwerty\" or \"12345\" against a wide range \nof users. \n● enforcing complex password policies \n● scanning ip address / client cookies for submission \npatterns (dynamic password policies) \n● intrusion detection \npassword \nguessing \nagainst \nsingle user \nhackers know something personal about \nsomeone they know in real life, and use this \nknowledge to make an educated guess. \n● educating users, don't use personal information as \nyour password \n● enforcing complex password policies \nworkstation \nhijacking \nattacker waits until a logged-in workstation is \nunattended \n● logging out after a certain period of inactivity \n● intrusion detection to detect unusual behaviour \nexploiting \nuser \nmistakes \nmistakes such as writing down a password, or \nusing social engineering tricks that lures a user \nto reveal a password, or organisation that used \ndefault passwords such as \"stationery\" \n● change default passwords \n● educate users on social engineering tricks \nexploiting \nsame \npassword \nuse \nusers who use the same password for multiple \nsites, an attacker gains access to one, and \nattacks multiple streams \n● educate users on choosing different complex \npasswords for different sites. \nelectronic \nmonitoring \ncommunicating a password in plaintext is \nvulnerable to eavesdropping \n● never send a password in plaintext \n● technical solutions for secure transfer of \npasswords. \nhow passwords are protected \nloading password: salt with hash salt is a random number, combined with a hashed password to complicate hash \nto plain text comparison. hence to authenticate a user, you need a salt, a userid, and a hash value. \n- counters exploiting user mistakes, specifically, if someone uses the same password for multiple sites. \n- counters offline dictionary as you need more than just a hash value, especially with a large salt increasing the \ndifficulty of guessing. \nmulti-factor authentication: on top of a password, use a smartcard, fingerprint, voice patterns, or behaviour \nanalysis. \ntypes of password attacks \nman-in-the-middle attack: an attacker intercepts communication between a victim and a service where they are trying to \nchange their password. the attacker communicates between the victim and service. victim think they are talking to service, but \nthey're actually giving all their information to the attacker, the attacker masquerades the victim, talks to the service to provide \ncredentials to get a password change. \n\n \nlecture 5 \naccess control: granting access to authorised individuals to provide resources \naccess control requirements \n1) reliable inputs: access control must be given to authorised users, using an id and password. for example, security \nsystems at uoa should authenticate a upi, and assign different interfaces whether a upi belongs to a teacher or a \nstudent, and hence each have different accesses. \n2) principle of least privilege: grants the minimum set of access rights to do a job, just enough to complete a job \n3) administrative duties: only a special person or department should be able to manage access rights including granting, \nrevoking or updating access rights. \naccess control elements r (right) s (subject) o (object) \n1) subject: an entity that can access objects (eg: a user) \n2) object: system resource (eg: files, directories, etc) \n3) access right: the relationship between a subject and their corresponding access of an object, as well as their \npermissions (read / write / execute) \naccess control system \n1) access control function f(s, o, r): looks up r for the combination of o and r. \n2) security administrator: an entity that manages access rights \n3) auditor: does not take a role in access control system, but monitor and inspects operation of access control (s, o and r) \naccess control models \n1) dac: resource owner decides who can access and what level of access. vulnerable to trojan horses, and these \nmechanisms are only good for honest users. \naccess control matrix (rows = entity, columns = how they can access) \n file 1 file 2 file 3 file 4 \nalice orw  orw  \nbob r orw w r \ncharlie rw r  orw \naccess control list (eg: we can extract four lists, one per file) \nfile 1 → specifies all users that have access to file 1 → alice, bob, charlie \ncapability list (eg: we can extract three lists, one per entity) \nbob → read file1, owns/reads/writes file2, writes file3, reads file4. \n2) rbac: maps roles to access rights, so a manager will have more access compared to employees. rbac supports \ncomplex access control and reduces errors in administration. it complies with the principle of least privilege, where we \nassign just enough privilege for each entity to get the job done. \n○ user: a human \n○ permissions: approval of a mode of access to some object \n○ roles: job title (manager, clerk, ceo, etc) \n○ assignments: user-role (relationship between user and role) and role-perm (relationship between role and \npermission) \n○ session: mapping users to roles (not role and permission) \n○ constraints: assigning a user to a role, and containing what different users can access what exactly. \nlecture 6 \nmalware: a set of instructions that run on your computer and make your system do something an attacker wants it to do. \nmalware types \n1) virus: an executable script that replicates itself in a computer \n2) worms: a computer program that runs independently and propagates itself over a network \n3) trojan horse: appears to have a useful function, but has a hidden function \n4) spyware: software that collects information from computer and transmits it elsewhere (monitors keystrokes, network \ntraffic) \n5) ransomware: stealing data and asking money for it to get it back \n6) botnets: receive command from remote command and control servers, usually used for dos \nways of malware types \n1) document-based malware: receiving an email, content is enabled by victim, and malicious code is downloaded and \ninstalled. \n○ malware embeds codes into documents, pdf, spreadsheets, files, etc \n○ can access password or infect email contacts \n\n \n○ overrides commands, damages computer, or further spreads malicious codes \n○ tampers with addings, deleting, and changing words or images in documents or hard drive \n○ infiltrate email, sending unsolicited emails to contact lists. \n2)  advanced persistent threat (apt): requires advanced tools and runs over an extended period, costing a lot to a \nvictim. \n3) zero-day: realising there is an attack but not having time to protect or come up with a viable countermeasure \n4) backdoor: any mechanism that bypassess a normal security check, allowing unauthorised access to data. \n5) social engineering: using inside knowledge about a system and psychological properties of humans, to trick a user to \nmake security mistakes. (eg: email phishing) \nmalware countermeasures \n- anti-virus \n- phishing-detection system \n- blacklisting ips \n- firewalls \n- malware detection \n- machine patching \n- detect and prevent (intrusion detection / intrusion prevention) \n- education for users \nlecture 7 \nhumans: are the weakest link and are chronically responsible for the failure of security systems. humans are incapable of \nsecurely storing high-quality cryptographic keys and have unacceptable speed and accuracy. humans like to bypass barriers, and \neven if password policies are put into place, humans will still find a lazier way to create a weak password. \nstatic password rules \npassword policies that apply to everyone, such as lowercase/uppercase, numeral, punctuation, special characters, a minimum \nlength, etc \ndynamic password rules \npassword policies that involve a process first where they collect personal information such as address, background information, \nfullname, email, etc in order to design a dynamic password, where the policies cannot contain any of the given personal details. \nuser security : interdisciplinary \nuser security involves many theories, frameworks, models and methods from many backgrounds such as psychology, sociology, \nscience, behavioural economics, hci, etc \nsecurity vs usability (hci) \n1) security is not the primary interest of the user.  \n2) many applications that handle security issues interrupt a user's primary task, and users may find it a distraction and \nwould rather ignore it. \n3) hci looks at normal behaviours, while security looks at abnormal behaviours, which is overlooked in hci. \nwhat makes usable security different:  \n1) security and privacy  problems are well-understood \n2) it also looks at both types of users, legitimate and attackers. \nhence, it can pinpoint where users behave in an unpredictable way, can detect stress, pressure, tension, and whether users are \nattentive or not (lazy users). hence, traditional hci itself is insufficient in keeping a system safe. security and usability are better \nused together, to make usable security. it is about creating and designing a system that intersects security and usability. \nsecurity (strict) usability /  hci (optimistic) usable security \nhuman behaviour is \nsecondary to security \nsecurity is secondary to human \nbehaviour. \nboth human and security factors \nare primary constraints \nhumans considered in their \nrole as attackers \nconcerned about human error, but \nnot human attackers \nconcerns about both normal users \nand adversaries \ninvolves threat models involved task models, mental \nmodels, cognitive models \ninvolves threat models and task \nmodels \nfocus on security metric focus on usability metrics considers usability and security \nmetrics together \nuser studies rarely done user studies are common both user and attacker studies \n \n \n \n\n \nuser-selected graphical passwords \na new password policy was introduced where instead of a plain-text authentication, the user selects a picture of face that they \nhave selected as their password. here we are trading off some security to suability. this system is friendly for humans as it is \neasier to remember a face rather than a complicated string.  \nsecurity  usability  usable security \n● what is the space of \npossible passwords? \n● how can i make the \npassword space larger \nto make the password \nharder to guess? \n● how are the stored \npasswords secured? \n● can an attacker gain \nknowledge by \nobserving a user \nentering her password? \n● how difficult is it for a user to \ncreate, remember, and enter a \ngraphical password? \n● how long does it take? \n● how hard is it for a user to \nlearn the system? \n● are users motivated to put in \neffort to create good \npasswords? \n● is the system accessible using \na variety of devices for \ndisabled users? \n● all the security/privacy and \nusability questions \n● how do users select graphical \npasswords? \n● how can we help them choose \na password harder for \nattackers to predict? \n● as the password space \nincreases, what are the \nimpacts on usability for \nhuman selections? \nthe human threat:  \n- malicious humans \n- clueless humans \n- unmotivated humans \n- humans constrained by human limitations (disability) \nwhen applying usability, we need to understand: \n- misleading / unhelpful user interface looks like \n- requiring users to make decisions for which the user is not qualified to make \n- unreasonable amounts of attention / effort \nhumans in the loop: a method to conduct usable security \n- do they know they are supposed to do something? \n- do they understand? \n- do they know how to do it? \n- are they capable? \n- will they actually do it? \nhow to implements usable security \n1) identify points where system relies on humans to perform security-critical functions \n2) find ways to partially / fully automate some of these tasks \n3) identify potential failure models for the remaining tasks (using human-in-the-loop) \n4) find ways to prevent these failures. \nemail phishing red flags \n● an \"urgent\" subject such as 'someone has your password!' \n● email greets the victim with their actual name 'hi john' \n● email provides details such as a date, ip address, and location \n● \nemail lures users to change their email password by clicking a click '<https://bit.ly/1pibsu0>, bit.ly is a shortening \nurl address service. \nso in this example, with the increasing use of technology, attacks become creative and so we must treat it. we first identify that \nthis system relies on humans to perform security-critical functions (1) as scanning for phishing emails can not be conducted in an \nautomated manner (2), we should find ways to identify potential failure models (3), such as the 'game design framework', which \nis a framework that sums costs and benefits of a security implementation, such as perceived threat, safeguard cost and \neffectiveness, to calculate human avoidance motivation and behaviour, to find ways to prevent these failures (4). \nlecture 8 \nvulnerabilities: are features that exist in applications that we can utilise. acts as a prelude of an attack, so it can become \na risk, but it can also be used to construct threats or risks that point to systems which we use to mitigate possible problems. the \nmain source of vulnerabilities are inputs. \nowasp: open web application security project: looks for top 10 issues in web applications, updates the list every 3-4 years to \nsee most common vulnerabilities that we can utilise when building programs to embed security. each year, owasp points can \nchange, stay the same, replace, swap positions, merged, or even just be removed entirely. \n \n \n \n \n\n \ntypes of owasp vulnerabilities \n1) injection: untrusted input gets processed as part of command or query. attacker injects and executes unintended \ncommands and can access unauthorised data. \n○ command injection \n○ code / sql injection \neg: typing 'google.com' to get an ip address. injecting code where the input changes to 'google.com; uname \n-v', hence the lookup will generate the nslookup, and also execute the second part, returning all information \nof sensitive information. \neg: sql = select id from users where username='...' and password = 'password' or 1=1' hence \nthe code will always run and the attacker gains access to the corresponding username. \ncountermeasure: separate data from application logic. use parameterised queries, such as prevention = \nshlex.quote(domain_name) command = \"nslookup{}\".format(prevention) \n2) broken authentication: incorrectly implementing authentication by allowing default, weak, or well-known \npasswords, or application session timeouts not set properly. \ncountermeasure: mfa, do not use default credentials for admin users, enforce strong password policies, limit or \nincreasingly delay failed login attempts, securely use session id and invalidate after logout, idle, and timeouts. \n3) sensitive data exposure: not protecting financial, healthcare and personally identifiable information via \nencryptions, or cipher suites, hashes with salts. \ncountermeasure: identify sensitive data as per privacy laws (gdpr), do not store data unnecessarily, protect it \nvia encryption, delay factors and hashes and salt. \n4) xml external entities (xee): older versions of xml are poorly configured and do not examine xml that \nuse external resources, or xml's that try to direct to external resources. directing external sources can reveal personal \ninformation, it can scan internal systems, remote code execution, and possibly a dos if an endless loop is clicked onto \nunintendedly. \ncountermeasure: patch and upgrade xml processors, disable xee, validate incoming xml \n5) broken access control: bypassess rbac / dac and unauthenticated users access unauthorised resources, \nso non-admin accounts may access admin accounts. \ncountermeasure: create a deny by default list, log access control failures, set api rate limits (restricting the \nnumber of requests to minimise harm from automated attacks) \n6) security misconfiguration: attackers attempt to exploit unpatched flaws, for example, an application that \nhas detailed error messages, which tells the attackers the contents of the flaw. \ncountermeasures: review and upgrade configurations, and an automated process to validate the effectiveness \nof configurations and settings, and test this in all possible environments. \n7) cross-site scripting (xss): an attack in which an attacker injects malicious executable scripts into the code \nof a trusted application or website. attackers often initiate an xss attack by sending a malicious link to a user and \nenticing the user to click it. \ncountermeasure: separate untrusted data from browser content, and use frameworks that automatically escape \nxss by design. \ncve & nvd definitions \ncve: common vulnerabilities and exposures (identifiers for publicly known vulnerabilities) \nnvd: national vulnerability database (scores vulnerabilities using common vulnerability scoring system cvss) \nlecture 9 \nprogrammers face problems with embedding privacy in design because \n- practical issues: difficult to understand terms, such as 'pbd' which is a privacy policy which urges programmers to \nincorporate privacy into design. they do not understand the principle itself, and so there is a gap between privacy \nrequirements and techniques. \n- privacy concepts may not work in software development, the technology requires expensive computational resources. \n- developers have personal opinions (bypassing obstacles) which take precedence over privacy requirements \nwe should educate software developers and enhance their coding behaviour through motivation. \nlecture 10 \nprogram analysis: process of analysing the behaviour of programs for the goal of finding problems in code \nstatic analysis: do not run programs, just read scripts, comments to draw a logic flow and identify the values of data. \ntries to discover any vulnerabilities in execution through full coverage via checking all parses. it is program-centric, \nand is a type of white box testing (you need to understand scripts). it is scalable, does not depend on computational power. it is \nvulnerable to accuracy issues providing false positives by misclassifying behaviour as misbehaviour. \ndynamic analysis: run the program in the actual execution environment, and if vulnerabilities exist, they show up. \ndifficult to generate and test all possibilities. it is input-centric and a type of black box testing.  \nconcolic analysis: static analysis ran, followed by dynamic analysis. \ncontrol flow graph: static analysis based on cfg, which shows all possible paths. \n\n \ntypes of static analysis \n● program slicing: reducing program to the minimum form that still produces the selected behaviour. the \nreduced program is a slice. it reduces cost / time, and is only an approximation of the original program to locate sources \nor errors more easily and faster. generally, finding a slice is difficult and an unsolvable problem. \n● symbolic execution: instead of specific values of inputs, we have symbolic values, such as 'x' which \nrepresents a range. both paths symbolically executed independently, forming an execution tree. \nin static analysis, we are not observing the actual behaviour, we only observe path conditions. although, in dynamic analysis, we \nobserve actual behaviour, there might be too many options which is its limitation. hence why concolic analysis is utilised.  \nlecture 11 \ncode obfuscation: making a program confusing aimed to make reverse engineering harder for attackers. \nlayout obfuscation layout = non-executable part of the program (comments, debugging information, variable \nnames) \n● hide comments / debugging information, rename variables to confuse etc. \ndesign obfuscation making design confusing, such as in a hierarchical, layered way, etc so attackers spend \nmore time trying to understand the code, or not understand it at all. \n● splitting / merging classes to make it hard for attackers to understand the idea of \ncode. \n● reversing a class \ndata obfuscation making exact values of data hard to guess \n● hide value of data via encryption, instead of coding exact input, use a hash \nfunction \n● variable splitting, split v into p and q, such that v = p xor q \n● changing lifetime of variables such as swapping local and global variables \ncontrol obfuscation making the control flow of the code confusing \n● opaque predicates: if/else statements are made harder to guess, for example \ninstead of using if(a>0), use if (a>b^2+b % 2) or if(a>b[0]) \n● control flow flattening: break or change the structure of cfg \nlecture 12 \nthe uncontrollable nature of threats: threats are unavoidable (1) and we have no control over it (2). \nso we can only prevent threats. \ntypes of threats \n unintentional: environmental, human, accidents, failure \n intentional: greed, anger, desire to damage \ncommon attackers \n- criminals \n- advanced persistent threats (apts) \n- vandals \n- saboteurs \n- disgruntled employees \n- activists \n- other nations \n- hackers \nthreat vulnerability pair: occurs when a threat exploits a vulnerability. vulnerability provides a path for the threat \nthat results in a harmful event or a loss. both the threat and the vulnerability must come together in order to result in a loss. \nexample: an ex-employee is unsatisfied by a company and acts a threat as they are thinking of something bad to do. the \nvulnerability is that the system has not removed the ex-employee from the system. the threat-action is that the employee \naccesses proprietary data. \nus government risk management initiatives \nnist: belongs to department of homeland security \nnccic: department in government that discusses how to integrate security \nus-cert: team in charge of spreading different documents issued by nccic \ncve: list of common vulnerabilities \n \n \n \n \n \n\n \nvulnerability mitigation techniques \npolicies and procedures \n● how to assign more systems to more sources \n● gdpr policy compliance \ndocumentation \n● document system resources, plans, etc \ntraining \n● educating and training sessions to teach all employees on recent data breaches and \nteach precautions \nseparation of duties \n● for rbac, separate roles from system resources \nconfiguration management \n● patch softwares as soon as possible, ensure systems are updates \nversion control \n● keep back-ups of databases or documents to restore upon the case of an attack \nincident response \n● when outlining policy, outline step by step how to tackle these attacks \ncontinuous monitoring \n● periodically review policies to ensure they are up to date \ntechnical controls \n● any risk management tools \nphysica controls \n● for example, preventing a computer room with a lock and key \nmanaging vulnerabilities \na threat utilises a vulnerability which incurs a risk which can result in some loss. mitigation techniques control the \nflow from vulnerability to risk, to dampen the pair to reduce a risk, and therefore a loss, or by blocking the source of the loss. \nhence, vulnerability mitigation is about prevention. \nrisk mitigation techniques (for public facing servers) \nremove or change defaults \n● remove or change admin default passwords such as 'stationery' \nreduce attack surface \n● for injection, the attack surface is the code, it is observable by external users, \nthinking of how to reduce this surface \nkeep systems up to date \n● keeping policies, patches, updates \nenable firewalls \n● blocks internal and external data transfers. firewalls sit between transfers and \nobserve packet sizes, and raise alarms when something looks unusual. \n \nbest practices for \nmanaging threats \n- create a security policy \n- purchase insurance \n- use access controls \n- use automation \n- input validation \n- provide training \n- use antivirus software \n- protect the boundary \nbest practices for \nmanaging vulnerabilities \n- identify vulnerabilities \n- match the threat-vulnerability \npairs \n- use as many of the mitigation \ntechniques as feasible \n- perform vulnerability \nassessments. \nbest practices for risk  \n- harden servers \n- use configuration \nmanagement \n- perform risk \nassessments \n- perform vulnerability \nassessments \nprioritising risk \n- organisations have limited funds \n- threats cannot be eliminated \n- all vulnerabilities do not result in a loss \n- identify important risks \n- use resources to identify current risks \n \n \n \n \n \n\n \nlecture 13 \ncomponents of risk management \n1) risk assessment \na) risk identification \nb) risk analysis \nc) risk prioritisation \n2) risk control \na) risk management planning \nb) risk resolution \nc) risk monitoring \nrisk management plan \n1 risk identification \na list of threats, vulnerabilities, and recommendations, and once identified, form pairs of threat-vulnerabilities \n2 risk analysis \ncost of risk + costs of recommendations = cost-benefit analysis \nalso, report all of the above, use risk statements to communicate a risk and resulting impact. \nuseful tool: fishbone diagram: finds all possible causes of a result, such as attacker + no ids + dos + open ports on \nfirewall = system out of service. \nprofitability vs survivability \n3 risk prioritisation \nuseful tool: risk management plan using risk = threat * vulnerability * asset to calculate the weight of the risk in \norder to prioritise each risk. \n4 risk management planning \ndraw a schedule and make a living document of each recommended solution by breaking into steps with a timeline and \ndetailed descriptions of the project. \nuseful tools: milestone chart → shows the dates/times of when project and subprojects should be completed by. \nuseful tools: gantt chart → shows a full project schedule, tasks that can overlap and cannot overlap are outlined. \n5 risk resolution \n6 risk monitoring \n lecture 14 \nstride threat model \ntype description security \ncontrol \nspoofing threat action aimed at assessing and use of another user's credentials, \nsuch as username and password. \nauthentication \ntampering threat action intending to change persistent data, such as records and \nthe alteration of data in transit between two computers over a network \nintegrity \nrepudiation threat action aimed at performing prohibited operations in a system that \nlacks the ability to trace the operations \nnon repudiation \ninformation disclosure threat action intending to read a file that one was not granted access to, \nor to read data in transit. \nconfidentiality \ndenial of service threat action attempting to deny access to valid users such as by making \na web server temporarily unavailable or unusable \navailability \nelevation of privilege threat action intending to gain privileged access to resources in order to \ngain unauthorised access to information or to compromise a system \nauthorisation \ncountermeasures \n1) in-place controls: in place in operational system with documentation \n2) planned controls: specified implementation data, details in planning documents \n3) control categories: procedural, technical, and physical \n \n\n \n \nprocedural → policies, procedures, security plans, insurance, awareness & training \ntechnical → login identifiers, system logs, firewalls \nphysical → locked doors, video cameras, fire detection and suppression \nseven domains \n1) user domain \n2) workstation domain (computer) \n3) lan domain (hub) \n4) lan-to-wan domain (router / firewall) \n5) remote access domain (broadband internet) \n6) wan domain (firewall, mainframe, application & web, system/application domain) \nlecture 16 \nprivacy: privacy is more attached to personal space, to ourselves. it is not about data. \ncontextual integrity: privacy depends on the context \nprivacy as a social construct \nsociety (a group of people) affects people's views, including their views on privacy. privacy therefore differs from society to \nsociety, for example, societies with greater education surrounding privacy would have different views from societies without it. \nthis also holds for societies with regulation differences, or societal norm differences. real life example: a family shared a \nphone, where messages, photos, data were all kept on one phone which was used collectively between family members. \ncontextually, if one member leaves the country, they now hold a weak belief in privacy, thinking it is okay to have private \ninformation shared. \nlecture 17 \npersonal data: any information that is directly or indirectly relation to an identified or identifiable person. \naccording to gdpr, vague data, such as 'age range', is not personal data. although, if data only has one person linked to it, \nsuch as 'job title ceo in company starbucks' or 'home address', it is considered personal data. \ndigital footprint: can be increased by intentionally sharing information about yourself on facebook or linked in for \nexample. hidden data attached to intentional data sharing activities, such as what ads you've clicked, or information you've \nshared to friends. shopping websites can tell if you like to exercise if your browsing history is workout clothes. every click \nleaves a mark, a footprint. \n- read privacy policies \n- use the privacy settings \n- ask to delete your dta permanently if you no longer using a service \n- think about what data to share \n- what technologies can you use to protect (vpn, anonymous web browser, incognito mode, ad blockers) \nproposed privacy by design by ann cavoukian \n1 proactive not reactive, preventative not remedial → plan it from the start \n2 privacy as the default → default settings should be privacy protective \n3 privacy embedded into design → prioritises privacy in the product design \n4 full functionality, positive sum, not zero-sum → should not be a trade off for the user \n5 end-to-end security, lifecycle protection → collection, process, share, store at each stage as long as \nthe data exists. \n6 visibility and transparency → communicate properly about the data handling practices \n7 respect for user privacy, keep it user-centric → respect the use, be empathetic \nthreat modelling \nbefore identifying threats \n1) identify personal data handled by the product or service \n2) understand the flow of identified data \nlinddun threat modeling framework \nlinking  data can be linked to learn more about the person even if it does identify a person. \nidentifying  identity is revealed through leaks, deduction, inference \nnon-repudiation a person can not deny a claim \ndetecting  arrive at a conclusion about a person through observation \ndata disclosure excessively collecting, sharing, processing, storing personal data \n\n \nunawareness & \nunintervenability insufficient transparency, feedback to users or less involvement of the user \nnon-compliance violation of best practices, standards and regulations \nlecture 18 \nprivacy enhancing technologies (pet): if a security breach occurred, if the data was in plain-text, this affects \nprivacy, but if it was encrypted or obfuscated, then privacy is still maintained even after the attack. pet's are technologies that \nallow the utilisation of personal data while at the same time reducing privacy risks (a win-win). \nanonymity: removing the link between data and the data subjects \n- negative side: cannot track for cyber-bullying, or black market activities \nsome types of pets \n- pseudonymisation: processing data in such a way that it is not possible to attribute them to a specific person without \nthe use of additional information. separate the identifiable information in a mapping table, and use a fake identity, such \nas random numbers (rng), cryptographic hashing, encryption,  or a counter (increment per new entry). \n- scalability issue: the more data entries, the larger the table \n- multiple identifiers issue: at uoa, a person can be tutor and student at the same time, we don't want to \nreveal a link \n- linkage attacks: people can be uniquely identified using only zip, dob, and sex. hence we should use \nk-anonymity. \n- k-anonymity: for every combination of identifying attributes in a dataset, there are at least “k minus 1” other people \nwith the same attributes. k = number of people in a group. quasi-identifiers = identifiers multiplied by a factor that \ntogether, identifies a person. \n- cannot use this to identify people, only used for analysing patterns. (eg: census data) \n- can still be breached with background knowledge. someone can get data from all systems, combine, and \npinpoint individuals. \n- differential privacy: ensures that an individual will experience no difference whether they participate in information \ncollection or not. this means that no harm will come to the participant as a result of providing data. \n- cannot use this to analyse numeric or financial data, as adding noise will not preserve its accuracy. \n- federated learning: ensuring that data remains decentralised. apple records voice for siri. we train machines to not \nlearn data, instead, we send models at the phone, hence, data does not leave your phone. \n- homomorphic encryption: the conversion of data into ciphertext that can be analysed and worked with as if it were \nstill in its original form. \n- performance issues: takes time \n- zero-knowledge proof: every bit of information is treated with complete confidentiality. proving something without \nusing a password, for example 'what is the sum of the last 3 digits of your password' \n- synthetic data: artificial data generated to mimic real data. \n\n\n\n[week 8.1 lecture 21] \n0 encryption \nanonymity ❌❌❌ \n1 key cryptography (solves observability but not unlinkability problem) \nanonymity ❌❌ \n2 vpn / proxies  \nanonymity ❌ \n3 cascade of proxies \nanonymity ✅ \n1 mix network (solves unlinkability problem) \nanonymity ✅✅ \n2 tor (improved mix network) \nanonymity ✅✅✅ \n \nkey cryptography & vpn/proxies \nwhen sending data from one person to another, it is encrypted via public/private keys, although the source and \ndestination are not anonymised and therefore pose the issue of linkability. as an attempt to mitigate this, proxies can \nbe used as a \"middle-man\", and hence the receiver does not know who the sender is. \n \nsender → proxy → receiver \na. if an attacker sits between the proxy and receiver, they do not know the source address but they know the \ndestination address. \nb. if an attacker sits between the sender and the proxy, they do not know the destination address but they \nknow the sender address. \nc. if an attacker sits at the proxy, they know both the sender and receiver addresses. \nd. if there is an attacker between sender and proxy, and another attacker between proxy and receiver, the \nattacker can use statistical analysis / traffic patterns to deduce source and destination. \n \ncascade of proxies \nmix network tor \nc = e(k\np\n, e(k\np\n, e(k\np\n, m))) \n \n \n[k\np\n, k\np\n, k\np\n] \n \nmix 1 <k\np\n, k\ns\n> \n \nmix 2 <k\np\n, k\ns\n> \n \nmix 3 <k\np\n, k\ns\n> \n \n \nmessage received \n \nmix 3 <k\np\n, k\ns\n> \n \nmix 2 <k\np\n, k\ns\n> \n \nmix 1 <k\np\n, k\ns\n> \n \n \n<k\np\n, k\ns\n> \n<k\np\n, k\ns\n> \n<k\np\n, k\ns\n> \n \n \nc = e(s\nk\n, e(s\nk\n, e(s\nk\n, m))) \n \n[s\nk\n, s\nk\n, s\nk\n] \n \nmix 1 <s\nk\n, k\np, \nk\ns\n> \n \nmix 2 <s\nk\n, k\np, \nk\ns\n> \n \nmix 3 <s\nk\n, k\np, \nk\ns\n> \n \n \nmessage received \n \nmix 3 <s\nk\n, k\np, \nk\ns\n> \n \nmix 2 <k\np\n, k\ns\n> \n \nmix 1 <s\nk\n, k\np, \nk\ns\n> \n \n \n<k\np\n, k\ns\n> \n<k\np\n, k\ns\n> \n<k\np\n, k\ns\n> \n \n \n \n\n \n \nmix networks \nan attempt to mitigate statistical/traffic analysis is mix networks. mix networks are a chain of anonymised proxies \ncalled 'mixes'. data is encrypted in layers and sent to the next mix and so on until it reaches a destination. this way, \nat any point between source →mix, mix → mix, mix → destination, it will be unlikely for an attacker to identify a \nsource and a destination.  \n● each mix can artificially delay the time of sending (latency overhead) or randomly shuffle a set of different \ndata packets and send them in an arbitrary order to obfuscate an attacker sitting between mixes. this \nrequires a lot of resources \n● dummy/cover traffic can be used where along with the real packet, randomly generated packets are also \nsent to dummy destinations to make the perception of real traffic difficult. requires extensive  computational \nresources \ntraffic mixing \n \ndummy / cover traffic \n \n \ntor - the onion router \ntor is a mix network with improvements, and instead of 'mixes', these proxies are called 'relays' \n1. sender encrypts data in layers using private keys, each layer holding the address of the next relay. \n2. each relay decrypts the outer layer using a public key, revealing the next address. hence each relay only \nknows the previous relay address and the following relay address. \n3. at the final relay, it sends data to the receiver. \n4. receiver encrypts the reply using a public key with the relay's address \n5. each relay passess the data back by adding back their layer. \n6. sender encrypts all layers using their private keys and can view the reply. \n \nmix network vs tor \nmix network tor \nuse long-term static key pairs for encryption. uses ephemeral session keys for each session. \nif a private key is compromised, past communications \ncan be decrypted. \nprovides perfect forward secrecy, protecting past \nsessions even if keys are compromised. \n \n[week 8.2 lecture 22] \ntor's perfect forward secrecy (pfs) \neach time there is a new \"session\" (layer) a new public and private key are negotiated between sender and receiver. \ntemporary (ephemeral) encryption keys are generated for each session. these keys are used only during that \nsession and discarded afterward. attackers can compromise new connections going forward, it's just the prior \nconversations that are safe. \n \ntor is not attack-proof \n   \n \n● source known \n● destination unknown \n● source unknown \n● destination unknown \n● source unknown \n● destination known \n● source known \n● destination known \n \npredecessor attack \nwith n total relays where m of the are controlled by an attacker: \n● m/n chance for an attacker to be in the entry relay. \n● (m-1) / (n-1) chance for an attacker to be in the exit relay. \n● hence, (m/n)\n2\n chances for an attacker to be in the circuit. \n\n \nif n = 8000 and m = 2, there is 2/8000 = 0.00025 change for entry relay, (2-1) / (8000-1) = 0.000125 change for the \nexit relay, and roughly (2 / 8000)\n2\n = 0.0000000625 chance to be in a circuit. \n \nclients periodically build new circuits \nwhile it’s unlikely for the attacker to control both nodes in a single circuit, as more circuits are built, the attacker has \nmore chances to be in both positions. over time, the probability of this happening increases. \n \ncountermeasure to predecessor attack \n● intuitively, we would assume that increasing circuit lifetime can mitigate the increasing chances of an \nattacker's entry, although this is not realistic as people often access new web-pages, and new sessions \nneed to be delivered. \n● instead, we should make a strict criteria for which computers/servers are allowed to act as entry or exit \npoints so only trusted and secure relays can handle sensitive traffic. \n \nrelays \n- relays are located in a publicly available tor directory that is refreshed hourly. \n- not all relays are treated equally, entry/exit relates are specially labeled. \n- tor does not select relays randomly. \n \nguard relays [the first relay] \nguard relays help prevent attackers from parasitising the first relay. \nold approach to guard relays improved approach to guard relays \n● tor selects 3 guard relays and uses them for 3 \nmonths, discards them, and 3 new guards are \nselected.  \n● the problem: if one is compromised, we could \nbe stuck with compromised anonymity for 3 months. \ntor selects relays \n● with high bandwidth for fastest data transfer \n● relative to proximity of subjects (to reduce \nlatency) \n● with good \"uptime\" to ensure stable connection \n \nexit relays  [the last relay] \nare volunteered and manually reviewed and approved by tor. exit relays must be able to handle legal notices and \npotential law enforcement inquiries and cannot be run from your home. ideally, they should be owned by institutions \nsuch as universities or libraries. \n \nonion services / hidden services \nare used to run hidden servers (hidden ip address). usually, when we want to use google, we need to know \ngoogle's ip address. onion services make it possible such that we can use services without knowing their ip \naddress. this can be used, for example, to bypass government intervention in blocking citizens from accessing \ncertain websites. \n \n1. tor grants the user an introduction point to the onion service. \n2. user sets up a \"rendezvous\" (to meet at a particular time and place) point and rendezvous cookie. \n3. the client received the rendezvous point and rendezvous cookie via the introduction point \n4. onion service connects to the rendezvous point allowing the user and onion service to communicate. \n \n \n\ntor bridges \nwe know that there are 3 types of tor relays: entry/guard, middle, exit. although tor directories are publicly \navailable, governments can still restrict ip addresses of tor relays listed in these directories. tor bridges are just \nunpublished proxies that are used to connect clients in censored areas to the rest of the tor network. bridges alone \nare insufficient to get around all types of censorship, hence pluggable transport design is used, which disguises \ntraffic to look like other traffic. \n \nblock chain \nledger: a book or other collection of financial accounts. \nblockchain: blocks of transactions \n- public: open to allow anyone to participate (permissionless blockchain) \n- private: closed and restricts participants (permissioned blockchain) \n \nblockchain does not work without participants (nodes) \n full nodes lightweight nodes publishing nodes \nwhat \nstore a complete copy of the \nentire blockchain. \nkeep just a small part and rely on full \nnodes for information. \nfocus on sending and receiving transactions \nrather than storing the blockchain or \nvalidating it. \nwhy \nverify and validate \ntransactions. \nfaster and use less storage, great \nfor mobile devices and casual users. \nfacilitate communication between users and \nthe network, helping users publish new \ntransactions. \neg \na librarian who has every \nbook in the library and knows \nwhere everything is. \nimagine a student who only has a \nfew reference books but can ask the \nlibrarian (full node) for anything. \na newspaper editor who spreads news but \ndoesn’t keep a full archive of every article \never printed. \n \n \nmultiple full nodes exist, so in the case that one full node goes \noffline or disconnects, other full nodes can still maintain the \nblockchain.\n \n \n \n \n \n \n \n[week 8.3 lecture 23] \nblockchain network \n \nblock header \nhash (previous block header) \ntimestamp \nnonce \nhash of block data \n \nblock data \n(transaction list, etc) \nblock01 \n \nblock header \nhash (block01 header) \ntimestamp \nnonce \nhash of block data \n \nblock data \n(transaction list, etc) \nblock02 \n \nblock header \nhash (block02 header) \ntimestamp \nnonce \nhash of block data \n \nblock data \n(transaction list, etc) \nblock03 \n \n★ block data: actual data (health records, or thousands of transactions of cryptocurrency) \n★ timestamp: the time a publishing node has started its block-creating task. since blocks are created \nsequentially, we can deduce that the timestamp block03 > block02 > block01 \n★ nonce: in blockchain, it acts as a mathematical, computationally expensive puzzle that publishing nodes \nsolve \n★ hash of block data: checks integrity of block data \n★ hash of block header: provides chain functionality, a link between itself and the previous block. hence \nblock02 can never be compromised as its legitimate record is stored in block03. (tamper resistance) \n\n \nmerkle tree: hashes every single transaction in cryptocurrencies. \nthis is an example of how cryptocurrencies are hashed in a block of a \nblockchain using the \"merkle tree\" algorithm, where each transaction (data) \nis hashed individually, paired and hashed again, and again until we reach a \nmerkle root. \n \n \n \n \n \n \nblockchain consensus models (general agreement) \n● proof of work (pow) \n● proof of stake (pos) \n● proof of authority (poa) \n● round robin \n \nproof of work (pow) \n \npublishing nodes are incentivised to compete with other miners to publish and create new blocks. publishing nodes \nmay either be a full node or part of a mining pool. \n1. new transactions are generated \nalice buys bitcoin, signs the transaction with a private key, and sends it to a full node for verification. \ntransactions are \nnot confirmed yet, and can be attached with a transaction fee to gain priority for its \nconfirmation. \n2. transactions are stored in mempools \nfull node verifies authenticity of transaction and stores it in its own mempool and broadcasts transaction to \nneighboring full nodes so miners can try and create a block, and full nodes \ncan remove this transaction \nfrom its mempool\n. \n3. miners compete to create a block \neach miner independently groups transactions to publish valid blocks. their incentive is a newly mined coin \nplus the transaction fees. blocks are mined via solving\n computationally intensive tasks. \n4. a valid block is published to the blockchain \nre-checks the nonce and if everything is good, the blockchain is updated and continues to expand. \n \nminer block reward = newly mined coin + transaction fee \n1 btc is worth usd60,000, after every 210,000 blocks, the reward halves, hence 1 btc will be worth usd30,000 in \n4 years. \nminer conflicts \ntwo miners solve a puzzle at the same time, both will broadcast their solutions to neighboring nodes, once validated \nand published to the blockchain, one block usually finds the chain faster and is successfully added, while the other \nblock becomes stale and essentially disregarded. \nblock creation & difficulty \nbc tries to publish a block every 10 minutes, the difficulty is adjusted every 2016 blocks (2 weeks). so 1 hour = 6 \nblocks, 1 day = 144 blocks, 1 week = 1008, 2 week = 2016 \nbc mathematical challenge \nhashcash was the original puzzle, but increasing/decreasing x leading 0's by1 doubles or halves the difficulty, a \nlimited difficulty change, hence why bc has changed their puzzle to a 'target hash' where you have to find a nonce \nless than x which allows fine-tuning difficulty. \neg: current target hash is 000101 (5d) and it takes 11 minutes to mine, hence we need to decrease difficulty by 1 \nminute. \n \ntraditional hashcash approach new target hash approach \ninstead of 3 leading 0's we will use 2 leading 0's. hence \n000xxx to 00xxxx \npossible answers for 000xxx (2^3) = 8 \n000, 001, 010, 100, 011, 101, 110, 111 \npossible answers for 00xxxx (2^4) = 16 \n0000, 0001, 0011, 0111, 1111, 0010, 0101, etc \nso the number of valid answers is halved, and hence \nthe time to create a block is also halved, not a fair \nadjustment. \ninstead of the target hash being less than 000101, it \nneeds to be less than 000110. \nhence difficulty will be decreased as it was initially less \nthan 5, but now has to be less than 6. hence, time to \ncreate a block has decreased by 1. \n \ndisadvantages: \n● computationally intensive  \n● high energy consumption (bitcoin uses 4x more energy than all of nz) \n● hardware arms race \n \n\n[week 9.1 lecture 24]\n \nattack vectors \nrace attacks mallory has 1btc and makes two transactions: \n1. purchases a product for 1 btc \n2. transfers 1 btc to another wallet \nneither transactions are not confirmed yet, but the merchant delivers the product and \neventually it is transaction 2 that goes through, and hence the merchant is not paid. \n \ncountermeasure: \n- merchant has to wait until a transaction is confirmed. \nfinney attacks mallory has 1btc and makes a transaction: \n1. transfers 1 btc to another wallet, manages to solve the nonce and creates a valid \nblock, but keeps the block to herself (does not broadcast) \n2. mallory then purchases something for 1 btc and the merchant delivers the product \nwithout it being confirmed. mallory then broadcasts the block in (1) \nmallory gets the product for free. similar to race but time is an important component, and \nmallory is the attacker. \n \ncountermeasure: \n- merchant can wait until the transaction creates a block, and waits for 5-6 blocks after \nthis block to ensure this transaction cannot be reversed. \n51% attack / \nmajority attack \nrequires more than 50% of the entire hashing power, takes control of the blockchain network \nby outpacing other miners. \n1. get enough miners to more than 50% the hash rate \n2. make a huge transaction and pocket the goods/services for that transaction \n3. use hash power to shadow mine starting from the block prior to your transaction \n4. with 51% current hash power produce the longest chain with the most work that \nreplaces all the blocks starting from 1 before your big transaction \n5. release this new chain which then replaces the chain everyone else was using and \nundoing all transactions \n6. chaos ensues and you still have the bitcoin from the large transaction plus the \ngoods/services from spending that bitcoin \n \nproof of stake (pos) \nproof of stake is a system where the creator of the next block is chosen based on the amount of cryptocurrency they \nhold. this means that the more cryptocurrency a person holds, the more likely they are to be chosen to add the next \nblock to the blockchain. miners from pow compete to publish the next block but in pos, only one validator is \nresponsible to publish the next block, hence it is more energy efficient and does not require expensive hardware. \nalso, validators that misbehave such as not publishing a block within a certain time, double spend, or invalid blocks, \nthey lose their 'stake' assets.  \n \nround robin \nmore suited for private permissioned blockchain, where nodes know and trust the identities of publishers. \n[week 9.2 lecture 25] \ndenial of service attacks \n3 types of dos \nnetwork \nbandwidth \ntarget has a lower channel capacity connected to the internet, they \nhave 1gb p/s but attacker has 40gb p/s. so they have no choice but \nto drop some incoming packets. for example, an organization hosts \nthe olympics with legitimate users consuming streams. dos \noverwhelms this link, denying legitimate users or degrading the quality \nof olympic streams. for example, watching in 480p instead of 4k. \nexploiting service with not \nenough bandwidth \n- flooding ping commands \n- source address spoofing \n- backscatter traffic \nsystem \nresources \naims to overwhelm network handling software. packet structure \ntriggers a bug, crashing the system, and communication becomes \nunavailable, this attack is an example of a \"poison packet attack\". \nhiding malicious software \nin packets \n- syn spoofing attack \napplication \nresources \ntargets application resources. attackers target specific applications \nsuch as a website or database by sending a lot of requests. \nflooding large numbers of \nrequests \n- distributed dos \n- sip flood attacks \n- http flood attack \n- slowloris \n\n \n \ndos flooding attacks \n★ flooding ping commands: attacker sends millions of packets to the target by exceeding their capacity. \nalthough, sending icmp echo requests means the source is identifiable and the target knows who the \nattacker is. also, sending out a million packets means we receive a million packets back. hence, the \nattacker is negatively impacted as well.  \n○ attacker capacity > target capacity \n○ potential customers of target cannot consume their service \n★ source address spoofing: forges source address making it harder to identify a hacker and target sends \nreplies to the spoofed address to avoid the problem in the 'flooding ping commands' attack. the spoofed \naddress might even send back resplied, further flooding the target. \n○ random spoofing: address chosen is random (real or fake) \n○ subnet spoofing: address is chosen randomly but within same sunset as target system \n○ fixed spoofing: a fixed spoofed source address \n○ possible countermeasure: identify the flow of packets by tracing way back. however this includes \nincorporation of multiple engineers which is time consuming, difficult to carry out, and some \nservicers delete log-history every few days \n★ backscatter traffic: side effect of source address spoofing. backscatter traffic refers to the ping replies from \nspoofed ip addresses. \n○ honeynet took a bunch of unused ip addresses and advertised the path to these addresses, \nmonitored these addresses and collected details on the packets sent to them. since there were no \nreal systems at these addresses, there should be no traffic, but there were packets directed to \nthese unused addresses, and hence network attacks were discovered. \n★ syn spoofing attack: overflows targets the tcp connection request table. attacker tries to fill up the \nrequest table space. hence, legitimate users cannot fit a request into a tcp table and cannot make \nconnections. \n○ table request capacity is usually quite small because it is developed under the assumption that the \nqueue is emptied quickly enough. \n○ 3-way handshake:  \n■ computer a sends a syn packet to computer b saying \"hey, i want to synchronize and \ntalk with you!\" \n■ computer b then replies to computer a with a syn-ack packet that says \"i \nacknowledge you want to synchronize with me\" \n■ computer a then replies with an ack that is effectively \"i acknowledge that you \nacknowledge that i want to synchronize with you\" \n○ attacker generates tcp syn packets with spoofed ip addresses. the target records in the table \nand sends syn-ack packets to spoofed addresses. if a spoofed ip has a system, the system \nsends rst to terminate the connection and if ip has no system, there is no response and the \nsyn-ack is resent several times (ideally what attacker wants). the table entry is removed from \nthe connection table eventually. \n★ distributed dos attack: attacker exploits vulnerabilities in os or install malware on systems, using multiple \nsystems at once. the attacker is the 'master' and compromised systems are called 'zombies', a large \ncollection of zombies is called a 'botnet', where zombies repeat scanning process and self-propagate. \n★ sip flood attacks: sending an invite message that establishes sessions between user agents, these invites \nconsume considerable resources. \n★ http flood attacks: bombarding a web server with http requests to consume considerable resources, for \nexample, downloading a large file from the target server. this attack has a variant where bots recursively \nbrowse all http links. \n★ slowloris: web servers can only have a limited number of connections, attacker opens multiple connections \nto the target system with partial http request headers, sends incomplete requests to prevent connection \ntimeouts, attacker periodically sends partial headers. countermeasures include: rate limit, timeouts, and \ndelayed binding. \n \n[week 9.3 lecture 26] \n★ reflection attacks: use intermediary servers (reflectors) that attack on attackers' behalf. in ddos we have \nzombies but in this attack, we have reflectors, they make requests where they spoof the ip as the target, \nmake requests, and the server responds to the spoofed ip. \n○ variant: amplification attack: generates a large number of response packets to target. \ncountermeasure: block broadcast packets coming from outside the network, or limit outside ping \ncommands. \n○ dns amplification attacks: sends dns requests to overwhelm target \n\ndos countermeasures \ncommercial solutions available: \n● intrusion detection system (ids) \n● firewalls \n● security enhanced routers \nexpensive solutions: \n● allocate excess bandwidth \n● use multiple servers to distribute load (load balancer) \n● in-house vs outsourced \n \nfour lines of defense: \n1) attack prevention and preemption (before attack) \n2) attack detection and filtering (during attack) \n3) attack source traceback and identification (during and after attack) \n4) attack reaction (after an attack) \n \ndos attack prevention \n● block spoofed source addresses on routers as close to source as possible, although this slows down routers \ndue to ip-checking process, hence there will be an extra delay added, even though it significantly decreases \nattacks using spoofed addresses. \n● filters may be used to ensure the claimed source address is the correct one \n● icmp floods or udp floods can be throttled by rate limiting \n● syn cookie against syn spoofing attack. in the tcp request table, we can cryptographically encrypt a \nserver's initial sequence number. when a server sends a syn, it is sent with a cookie, and cookie is sent \nback to authenticate the syn-ack. server does not consume extra memory but takes computational \nresources (cookie and encryption) and it blocks the use of some gcp extensions such as large windows. \n● for amplification attacks, we can block ip directed broadcasts, blocking suspicious services and \ncombinations, manage application attacks with some graphical puzzle (captcha), and use mirrored and \nreplicated servers when high-performance and reliability is required. \n \nresponding to dos attacks \n \n\n",
  "330": "\n\n \nmodel equation distribution assumptions \nlinear μi = β0 + β1xi yi ~ normal(μi, σ2) linear combination of explanatory terms/ independence /normality/ eov \npoisson \nlink: log \nlog(μi) = β0 + β1xi yi ~ poisson(μi) discrete and  non-negative distribution / right-skewed by symmetric when u is big \n/variance increases with mean \nlogistic \nlink: logit \nlogit(p\ni\n) = b0 + β1xi \nvar(yi ) = ni pi (1 − pi ) \nyi ~ binomial(ni, pi) \nyi ~ binomial(ni = 1, pi) \ndiscrete response / non-constant variance \nnum of successes associated with the ith observation must be an int between 0 and ni \nquasi \npoisson \nvar(yi ) = ku\ni \nyi ~ quasipoisson(kμ\ni\n) multiplying the variance of the corresponding \npoisson or logistic regression model by a ‘dispersion parameter’, k. \nnegativc \nbinomial \nvar(yi ) = u\n \n+ u\n2\n / ө \nyi ~ negative binomial(ui,ө) this distribution assumes that there is a quadratic relationship between the mean and the \nvariance, so var(y) > u \nquasi \nbinomial \nvar(yi) = knipi(1 - pi) yi ~ quasibinomial(ni, pi) multiplying the variance of the corresponding \npoisson or logistic regression model by a ‘dispersion parameter’, k. \nbeta \nbinomial \nvar(yi) = nipi(1 - pi)(1 + nip-p) yi = beta-binomial(ni, pi, p) \n \nthe parameter ρ controls the variance such that ρ = 0 implies no overdispersion and 0 < ρ ≤ \n1 accounts for overdispersion \nwhere, for the ith observation, pi is the probability a respondent has a psychiatric illness, ghqi is respondent i's ghq score, mi = 1 if the gender is male, otherwise zero. [ensure \nto include the log off the offset in the equation] \npoisson exp(coef): we estimate that for every 1_ increase in_, the expected number \nof _  is multiplied by _ \nlogistic confint(): a 1 unit increase in _increases the odds _ by a factor between _ to _ \npoisson exp(100*coef): we estimate that for every 100g increase in the_, the \nexpected number of _ is multiplied by _ \nlogistic exp(coef()): we estimate that, for every 1_increase in _, the odds of _ are \nmultiplied by _. \npoisson100*(exp(coef)-1): we estimate that for every 1g increase in_, the expected \nnumber of _ increases by _% \nlogistic exp(10*coef()): we estimate that, for every 10_ increase in _, the odds of having \n_are multiplied by _. \npoisson 100*(exp(100*coef)-1): we estimate that for every 100g increase in the_, \nthe expected number of _ increases by _% \nlogistic 100*(exp(10*coef()))-1: we estimate that, for every 10_ increase in _, the odds of \nhaving _increase by _%. \ncalculating confidence interval β1 ± 1.96*se z-stat = z =   \n푏\n푆퐸 표푓 푏\ntesting null hypothesis t-statistic = (h1 - h0) / se missing coefficient standard error * zval \nfor a t-stat > 2: cannot reject null hypothesis / in ci = do not reject \nfor a t-stat < 2 : reject null hypothesis / not in ci = reject \ncalculating average value of a explanatory variable (x) with a given probability (80%) \nlog(p\ni\n / (1 - p\n1\n)) = b0 + b1x + b2male\ni\n and p\ni \n = 0.8 \nlog() =log(( ) = log(4) = b0 + b1x + b2male\ni\n (rearrange to get x) \n푝푖\n1 − 푝푖\n0.8\n1 − 0.8\nx =  \n푙표푔(4) − 푏0 − 푏2푀푎푙푒푖\n푏1\ncalculating deviance & deviance residual \ndeviance = 2 (log-likelihood of saturated - log-likelihood of unsaturated) \n * d; where d=1 if \n>p else d=-1 \n퐷푒푣푖푎푛푐푒\n푛푢푚푏푒푟 표푓 푠푢푐푐푒푠푠\n푛푢푚푏푒푟 표푓 푡푟푖푎푙푠\ncalculating a probability of i p = exp(lincomb) / (1+ exp(lincomb) ) or p\ni\n = (p\ni\n / (1-p\ni\n)) \n \ncalculating raw residual = y\ni\n - e(y\ni\n) ni = total number of observations / pi = (pi / (1-pi)) \ncalculating residual sum squares (linear) = (y\ni\n - u\ni\n)\n2\n y\ni\n = number of successes \n \n poisson logistic \ndeviance \nyes \ncounts of student attendance in large first-year stats. business, \nbio, maths & psych courses. \nhow many students pass in large first-year stats, business, bio, maths & \npsych courses, where each observation is a semester. \ndeviance no pass/fail data for past stats330 students where each observation \nis a student. \ndata on the count of births per woman (over 16) in toronto canada \ngrouped vs ungrouped data \ngrouped \ndata that has been organised into groups, typically in frequencies \nun-grouped \neach individual observation is recorded separately \ntest scores grouped into intervals: b+: 3 students, a: 5 students, a+: 2 students a list of students' test scores: 85, 90, 78, 92, 88, 76, etc. \nlog(odds\ni\n) = b\n0\n \ny ~ binomial(n\ni\n, p\ni\n) \n\\log(\\text{odds_i}) = \\beta_0 \\\\ \ny_i \\sim \\text{bernoulli}(n_i, p_i) \nlogit(p\ni\n) = b\n0\n \ny\ni\n ~ bernoulli(p\ni\n) \n\\text{logit}(p_i_ = \\beta_0 \\\\ \ny_i \\sim \\text{bernoulli}(p_i) \n\n \nconditions needed to use the chi-squared approximation to assess the goodness of fit with the deviance \nthe distribution of deviance under the null hypothesis is approximately chi-squared if the response of each observation is well approximated by a \nnormal distribution. \nthis holds for poisson random variables with an estimated mean (ui)>=5. \nthis holds for a binomial random variables if the number of trials (n) is large enough \n● when pi is close to 0.5 n>=5 \n● but if pi is close to 1 or 0, ni must be much larger \noffsets \nincluding the log of the variable by fixing its coefficient to 1. it scales the model to compare response per unit of time. \nlog-likelihood \nbig log-likelihood: likelihood of generating the observed data is very low. \nsmall log-likelihood: model producing a much higher probability of the observed data. \na log-likelihood closer to zero means that the likelihood (the probability of observing the data given the model) is closer to 1, which represents a \nnear-perfect match between the model and the observed data. \nresidual plots \nobserved values minus expected values raw residuals \nobserved values minus expected values, divided by standard deviation pearson residuals \nthe signed-square root of the deviance contribution for the observation model deviance residuals \nresiduals with some added jitter for binomial and poisson regression (normally \ndistributed when the model is correct, even if there is sparsity) \nrandomised quantile residuals \ndiagnosing a glm \n1. test gof using deviance statistic (chi-square) \n2. inspect pearson and/or deviance residual plots \na. if there exists patterns (ice-cream cone) attempt to fix by adding explanatory terms or transformations \nb. if there exists sparsity, fit a randomised quantile residual plot \ni. if rqr appears random scatter, the glm passes gof test \nii. else, there is a problem with the model \n3. if the deviance and residual plots look fine, there is not evidence to suggest the model is inappropriate \n4. if the residual plot does not have a pattern but the deviance suggests a lack of fit, the variance of response distribution \nis probably wrong. (we can try and fit a quasi-poisson) \ngrouped binomial un-grouped binomial poisson \nresponse: missed-booking / all bookings \nexplanatory: driving conditions \nresponse: wine quality \nexplanatory: acidivy \nresponse: number of species  \nexplanatory: island type \nglm(cbind(y, n-y) ~ driving conditions) \nlogit(pi) = b0 + b1*driving_conditions \nyi ~ binomial(ni, pi) \nglm(goodquality ~ acidity) \nlogit(pi) = b0 + b1*acidity \nyi ~ binomial(ni=1, pi) \nglm(species ~ island) \nlog(ui) = b0 + b1*island \nyi = poisson(ui) \nglm(cbind(y, n-y) ~ 1) \nlogit(pi)  / log(oddsi) / log(pi / 1-pi) = b0 \nyi ~ binomial(1, pi) / yi ~ bernoulli(pi) \nglm(goodquality ~ 1) \nlogit(pi)  / log(oddsi) / log(pi / 1-pi) = b0 \nyi ~ binomial(1, pi) / yi ~ bernoulli(pi) \nglm(species ~ 1) \nlog(ui) = b0 \nyi = poisson(ui) \ncalculating df \nthere are 71 observations total. \n- enrolment = number of students enrolled at the school \n- type = college (c) or university (u) \n- nv = the number of violent crimes for that institution for the given year \n- enroll1000 = enrolment at the school, in thousands \n- region = region of the country (c = central, mw = midwest, ne = northeast, se = southeast, and w = west) \nmodel: fit ← glm(cbind(nv, enrolment-nv) ~ type + region, offset = log(enroll1000), family = 'binomial', data = campus_crime) \ndegrees of freedom = 71 - 6 = 65 \nexpected counts = enroll1000 * predicted_rate (we model the rate of violent crimes per 1000 students per school) \nyi = binomial(ni, pi) \nlog(oddsi) = b0 + b1typeuniversityi + b2regionmwi + b3regionnei + b4regionsei + b5regionwi \nwhere ni = 1 and is the number of successes of the observation i,  pi is the probability/proportion of observation i being success, and \ntypeuniversity=1 if type=university, and region = 1 if observation i is from the respective region. \nchoosing the best model using dredge() \n● if the difference is less than 2, then both models are similarly supported by the data. \n● if the difference is between 2 and 4, the one with the smaller aic is slightly better supported. \n● if the difference is between 4 and 10, the one with the smaller aic is considerably better supported. \n● if the difference is over 10, the one with the larger aic has essentially no support. \n\n\n\n[handout 15: using regression models for prediction] \nthe purpose of regression models is to (1) explain relationships between variables, and (2) make predictions \nfor new observations. prediction performance is measured based on how well it predicts for new observations. \n \nofcourse, the performance measurement of a prediction will be invalid if the new observation being predicted is \nnot representative of the observations used to fit the model. \n \ncorrelation coefficients \n1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -1.9 -1.0 \n0: no correlation \n0.3 / -0.3 = weak correlation \n0.5 / -0.5 = moderate correlation \n0.8 / -0.8 = moderate-strong correlation \n1.0 / -1.0 = very strong correlation \n \nwe use a correlation matrix to see correlations between the independent variables. \n \nfew estimated coefficients are insignificant as there exists high correlations between some pairs of regressors. \nthis is because correlation inflates standard errors, which increases their p-value when compared to the \nresponse. \nhigh correlations between independent variables mean they are not providing unique, independent \ninformation to the model\n. \n \nvif: variation inflation factors: diagonal inverse matrix \nprovide a measurement of multicollinearity existence. a vif=1 for a variable, for example \"temperature\" \nmeans temperature explains the response well on its own and is not affected by other regressors. a vif > 10 \nindicates strong multicollinearity. for example \"height\" has a high vif, with another predictor, let's say \n\"weight\", meaning both weight and height work together to explain the response non-independently. \n \nmspe: mean square prediction error \nis used for continuous responses where mspe = e(y - ŷ)\n2 \nwhich will quantify how well the model predicts the \nresponse for new observations. we calculate the prediction error for each new observation that is predicted, find \nthe mean of these errors, and square it. we cannot use new samples in real-life applications as it is time \nconsuming, costly, and expensive, so instead, we split the data into a training and test set. \n \nexample 1: \ncv: cross-validation \nfor small datasets, we use cross-validation to split the data. observations: 46 (if we split data in 10 parts; \n10-fold cv) \n[01 02 03 04][05 06 07 08][09 10 11 12][13 14 15 16][17 18 19 20][21 22 23 24] [25 26 27 28][29 30 31 32][33 \n34 35 36][37 38 39 40][41 42 43 44 45 46]  \n \ncalculate prediction error using a different subset each time \ntest set: [01 02 03 04] training set: [05 06 07 08]  prediction error: e\n1\n \ntest set: [01 02 03 04] training set: [09 10 11 12]  prediction error: e\n2\n \ntest set: [01 02 03 04] training set: [13 14 15 16]  prediction error: e\n3 \ntest set: [01 02 03 04] training set: [17 18 19 20]  prediction error: e\n4 \ntest set: [01 02 03 04] training set: [21 22 23 24]  prediction error: e\n5 \ntest set: [01 02 03 04] training set: [25 26 27 28]  prediction error: e\n6 \ntest set: [01 02 03 04] training set: [29 30 31 32]  prediction error: e\n7 \ntest set: [01 02 03 04] training set: [33 34 35 36]  prediction error: e\n8 \ntest set: [01 02 03 04] training set: [37 38 39 40]  prediction error: e\n9 \ntest set: [01 02 03 04] training set: [41 42 43 44 45 46] prediction error: e\n10 \n\nin the example above, we see that the training set only consists of 90% of the data, leading to an overestimate of \nthe mspe. a model trained on a smaller subset has less information to learn, which leads to slightly worse \npredictive performance on the test set. \n \naic(c) / bic to shortlist submodel candidates \nin a multiple explanatory regression, say we have 10 explanatory variables, then we would have 2\n10\n = 1024 \ndifferent subset models to choose from. it would be far too computationally extensive to calculate the mspe for \nevery 1024 submodel, so we use aic(c)/bic to shortlist candidate models, and only calculate mspe for the \nchosen best candidates. \n \naicc models  [168, 040, 247, 552, 695, 183, 680, 184, 104, 520] \nbic models  [\n040, 168, 516, 008, 552, 520, 183, 247, 772, 004] \n \ntake the top 5 models of both aicc and bic \ntop models = [168, 040, 247, 552, 695, 040, 168, 516, 008, 552] \ntop models = [168, 040, 247, 552, 695, 516, 008] \n(overlap in model 168, 040 and 552, so we end up with 7 \n \nmspe for the seven submodels: [51.08 51.36 51.23 51.72 51.80 50.02 52.97] \nmspe for the full model: [65.1] \nhence, the winner is model 06 with the lowest mspe = 50.02; lm(formula = evap ~ avat + avh + wind + 1) \n*still an under-estiamte \n \nexample 2 (poisson): \nanother example of choosing the best model for prediction using mspe (continuous response), but now we \nhave a dataset with a big sample size. we have 4177 observations in the abalone data set.  \n \nfirst we inspect the data and notice that: \n1) there are two unusually large values for height and there is a minimum value for height of 0. \n○ observation 1258 and 3997 have a height of 0 \n2) there is an unusual combination of length and diameter \n○ ##  length  diameter  \n○ ## 1211  0.185  0.375  \n○ one point has length < diameter which should not happen given the way the measurements \nwere defined. \nafter removing observations 1358, 3997, and 1221 we have completed data cleaning and can move on to \nmeasuring the performance of prediction. (we also saw the the full model had a residual deviance < df \nsuggesting under-dispersion hence we fit a quasi-poisson to replace the poisson) \n \nwe run aic and bic, deriving 6 submodels and then fit gam's to each regressor to explore ways of improving \nthe model. the output indicates that the full model is giving the lowest estimated mspe. \n \nmspe for poisson \nit becomes increasingly difficult to make precise predictions for poisson models as the number of counts \nincreases because the mean increases as the variance increases. \n \nexample 3 (logistic): \nfor a logistic regression, measuring performance of prediction is difference because instead of measuring the \ndifference between prediction for observation\ni\n and what observation\ni\n actually was, \nwe classify prediction error \nas the probability of a wrong classification\n. \ntrue -ve false +ve \nfalse -ve true +ve \n\nwe predict a case (malignant) if the estimated probability is ≥ 0.5 and severity = 0 (benign) otherwise. hence, \ngenerally speaking, we predict a malignant case if p ≥ c for some constant c where 0 ≥ c ≥ 1. \n \nc = 0 \nif c = 0, every observation will be predicted as (1) and we \nwill have a sensitivity=1 and specificity=0. this is because \neverything that is truly a 1 will be predicted a 1, but \neverything that is truly a 0, will also be predicted as 1, \nhence there are all true positives, but no true negatives. \n \n \n \nc = 1 \nif c = 1, every observation will be predicted as (0). this \nmeans everything that is truly 0, will be predicted as 0, \nbut everything that is truly 1, will also be predicted as 0, so \nno true positives are captured. so we will have a \nsensitivity=0 and specificity=1 \n \n \nas c varies from 0 to 1, sensitivity goes from 1 to 0, and specificity goes from 0 to 1.  \n \nsay we are creating a pregnancy test. it would be dangerous to have a bigger number of false negatives, so we \nwould adjust the cut-off, c, such that the model predicts less false negatives, and more false positives. hence we \nwould rather have more false positives than false negatives. this means, more predictions will be 1 when they \nare truly 0. hence, a higher cut-off is needed to achieve this. \n \n[handout 16: using regression models for explanation] \nregression models are used for prediction (1), and explanation (2), explanatory models can be thought of as \neither descriptive modeling, or causal modeling. \n \ndescriptive modeling \nperching birds example \n● length mean body length (cm). \n● nesttype: type of nest built. \n● oorc is the nest open or closed? \n● location: location of the nest. note that decide means that the nest is in a deciduous tree and conif \nmeans that it is in a coniferous tree (the remaining levels are self explanatory). \n● eggs average number of eggs. \n● marking 1 indicates eggs have markings and 0 indicates eggs have no markings. \n● incubate mean length of time (in days) the eggs are incubated. \n● nestling mean length of time (in days) the babies are cared for in the nest. \n● totalcare total care time = incubate + nestling \npossible descriptive modeling questions: key work: 'related' do not state causality \n- does the length of birds differ per nest type? \n- does the nest type relate to the average number of eggs found in each? \n- does the mean nestling time relate to the average number of eggs? \neffect modification: be careful about one variable affecting another, fix using an interaction. \n \nanswering descriptive modeling questions \ndata exploration \n- scatter plot (positive/negative, outliers, scatter variability, etc) \n- conditional plot for interactions, splitting data into groups to assess difference in pattern according to a \nspecific condition such as the nestling time for open vs closed nest types. if there is a difference, we \nexplore an interaction between nestling time and open/close. \n- box-cox plots: explore transformations of the response \nλ = -2 λ = -1 λ = -0.5 λ = 0 λ = 0.5 λ = 1 λ = 2 \n(response)\n-2 \n(response)\n-1 \n \n1 / 푟푒푠푝표푛푠푒\nlog(response) \n \n푟푒푠푝표푛푠푒\nnothing \nresponse\n2\n \n\n \nmodel without log-transformation model with log-transformation \n  \n \n \nlinearity: based on the residuals vs fitted plot, \nthere is some skewness as more data saturates on the \nrhs, indicating a need for a log transformation. \nindependence: as far as we can tell from the \ncontext provided, it appears that the birds are \nindependent, though there may be issues if birds \nfrom the same location aren’t independent of each \nother. \nnormality: i am somewhat concerned about the \nnormality assumption due to the clear departure from \nthe line in the qq plot. that said, linear regression is \nrobust to departures from normality. \neov: based on the scale-location plot, it seems \nlike the variance may be non-constant, specifically, it \nseems to be increasing as the fitted values increase. \n \nlinearity: based on the residuals vs fitted plot, \nthere is less skewness. \nindependence: birds are independent, though there \nmay be issues if birds from the same location aren’t \nindependent of each other. \nnormality: departure from line has decreased very \nslightly. \neov: variance is still non-constant, but better than \nthe first fit. \n \n \nwe can see that the model with the logged-response \nis a slightly better model. \n \nfindings \n \n(1) we conclude the relationship between \nincubation time is different for closed and open nest \nbirds. for open nests, incubation time increases as \nnestling time increases, but for closed nests, there is no \nclear indication of an increase. \n \n(2) open nests have a lower nestling time \ncompared to closed ones but as incubation time \nincreases, the difference decreases.  \n \n(3) there is quite a bit of scatter around trend lines \nfor both open nest and closed nest species. \n \n \n \ncausal modeling \nexample 1: smoking and lung cancer \nif we want to investigate the hypothesis that smoking has a causal effect on the occurrence of lung cancer... \n- we assume lung cancer does not cause smoking, smoking does not cause allele. \n\n \nthis is the most complicated causal model, where allele might cause lung cancer \nand smoking. smoking may cause cancer, and lung cancer does not cause either. \n \n \n \n \n \n \n \n \ns-l connection \nl ~ s   direct effect, ignores impact of a. \nl ~ s + a confounding effect of a on l. \n \na-l connection \nl ~ a  direct effect \nl ~ s + a indirect effect of s is removed as s is fixed and we explore a on l. \n \na-s connection \ns ~ a  direct effect \ns ~ s + a + l colliding effect \n \nexample 2: college plans \n \ndirect causal effects \nseniors.glm = glm(cbind(cpyes, cpno) ~ pe + iq + ses, binomial) \n \nfit the logistic regression model where log odds of cp=yes is the response and ses, iq and \npe are explanatory variables. this model closes all the other pathways to cp in the causal \ndiagram. \n \n \n \n \n \nindirect causal pathways \n● the effect of iq on pe \n● the effect of ses on pe \n● the direct effect of pe on cp \n \npe.glm = glm(cbind(pehigh, pelow) ~ iq + ses, binomial) \nwe find moderate evidence of an interaction between iq and ses. \nthe only interaction coefficient that is showing up as having a \nsmallish p-value (0.065) is iqh:sesh \n \n \n \ntotal effects \nseniors3.glm = glm(cbind(cpyes, cpno) ~ iq + ses) \n \nto explore the total causal effects of ses and iq on cp, fit the \nlogistic regression model where log odds of cp=yes is \nthe response and ses and iq are explanatory variables. this \nleaves the indirect causal pathways from iq to cp and \nfrom ses to cp open. \nthe coefficient for iq estimates its total effect on cp and \nsimilarly the ses coefficient estimates its total effect on cp \n \n \n\nexample 3: discrimination data \n \nthe context of this example means that we are focused on \nassessing the causal relationship between gender and salary. \n \ndirect causal effect \nblue blue: direct effect of gender on salary, that is, when all other \nvariables are fixed, is there a difference between females and \nmales in terms of the salary. \n \nlm(formula = salary ~ gender + deg + field + startyr + yrdeg + \nank + admin, data = salary.df) \n \n \n \n \n \n \n \n \ntotal causal effect \nthe combined effect of all causal paths that lead from gender to salary. consider two people, a female and \nmale who are of identical ability and drive. is there a difference in their expected salaries? \n \nthus we should not include any explanatory variable that lies on an indirect causal effect path from gender to \nsalary in our model. each of the other variables lie on at least one indirect causal pathway from gender to salary \nand thus they all must be excluded from the model. \nlm(formula = (1/salary) ~ gender, data = salary.df) \n \n2022 s2 causal diagram question \n \nwhat variables do you need to include to estimate the effect of pfoa on \nheart disease. give brief reasons why you would include or exclude each \nvariable. \n \nto estimate the effect of pfoa on heart disease, we explore the direct \neffect of pfoa on heart disease. that is, what will be the relationship \nbetween a person's pfoa exposure on chd if all other variables related \nare fixed. \nwe need to block the effects of education, ethnicity, age, and smoking \nas they are confounding paths. we should include gender as it is a \npredictor of the outcome. and we will not include the intermediate \ncausal variables about blood pressure and diabetes. \n \n2021 s1 causal diagram question \n \n(a) list all of the variables that have a direct causal effect on g. \n(b) suppose we wish to estimate the direct causal effect that a has on f. list all of the \nexplanatory variables that should be included in the model.  \n(c) suppose we wish to estimate the total causal effect that a has on f. list all of \nthe explanatory variables that should be included in the model.  \n(d) consider the direct effect of d on f. (i) list all of the variables that are confounders \nfor this effect. and (i) list all of the variables that are colliders for this effect. \n \n \na. f, d, e \nb. a, c, d \nc. a, c \nd. a, c \ne. g \n\n2024 s1 causal diagram question \n \na) suppose we want to estimate the direct effect of a on d. list the variables that should be included as \nexplanatory variables in the model we use for this purpose. a, c, e \nb) suppose we want to estimate the total effect of a on d. list the variables that should be included as \nexplanatory variables in the model we use for this purpose. a, e \nsuppose that a variable f is missing in the above causal diagram. the variable f affects d directly and does not \naffect any of the present variables. \nc) how does the inclusion of f as an explanatory variable in the model from question 2(a) affect the \ndirect effect estimation of a on d? inclusion of f does not affect the direct estimation of a on d. \nd) how does the inclusion of f as an explanatory variable in the model from question 2(a) affect the \nmodel predictability? it will improve predictability as f will capture variability that is not explained by \nthe other variables. \n2019 s1 causal diagram question \na) an effect modifier is a variable in a model, as an explanatory variable that affects the effect between \ntwo other variables. for example, if the effect of x and y differ depending on z, then z is an effect \nmodifier. \nb) casual relationships can be explored by exploring the direct causal effects, or total causal effects. \nc) direct: a and b: my.glm ← (y ~ a + b, family=poisson, data = my.df) \nd) indirect: my.glm ← (b ~ a, family=poisson, data = my.df) \ne) total: my.glm ← (y ~ a, family=poisson, data = my.df) \ndirect effects include factors that could confuse the relationship but are part of the direct link. for \nexample, if exercise → energy levels → health, then energy levels are in the direct line. \ninclude energy but ignore any outside factors that don’t connect directly on that path. if we \nthink diet affects both exercise and health separately, it’s not in that direct line and can be \nignored for this estimate. \ntotal effects to get the total effect, include any factor that affects both exercise and health in any way, \neven if it’s not directly in the middle. example: diet affects both exercise and health \nseparately, so include it to see the bigger picture. \nconfounders these are variables that affect both the cause and the effect independently, creating an \nadditional connection between them. (directly or indirectly) \ncolliders these are variables that are caused by both the starting point and the endpoint variables. \neffect \nmodifier \nif the effect between two variables, say exercise and health, varies depending on a third \nvariable, that third variable is likely an effect modifier. \n \n\n",
  "335": "",
  "345": "\n\ncompsci 345 - human-computer interaction \nbrianna yeung \ncoursebook: https://www.sciencedirect.com/book/9780128053423/the-ux-book  \n \nweek 1 | lecture 1..........................................................................................................................2\n \nl1.1 - introduction to hci and ux..............................................................................................2\n \nl1.2 - design walkthrough.........................................................................................................2\n \nl1.3 - understanding persona....................................................................................................2\n \nweek 1 reading: \"the ux book\" chapters 1, 2, 9.4 and 25.2....................................................3\n \nweek 2 | lecture 2..........................................................................................................................4\n \nl2.1 - heuristic evaluations........................................................................................................4\n \nl2.2 - the human and fitts law cs3...........................................................................................6\n \nweek 2 reading: \"the ux book\" chapters 25.5, 32.3, 30.3.2.5 and 32.7.2...............................7\n \nweek 3 | lecture 3..........................................................................................................................8\n \nl3.1 - contextual inquiry and brainstorming.............................................................................8\n \nl3.2 human computer interaction............................................................................................9\n \nweek 3 reading: \"the ux book\" chapter 7, section 8.7.1, and chapter 14..............................9\n \nweek 4 | lecture 4........................................................................................................................10\n \nl4.1 - visual design (perception and aesthetics).....................................................................10\n \nl4.2 - visual design - typography and reading........................................................................12\n \nweek 4 reading: \"the ux book\" chapter 17 and chapter 20..................................................13\n \nweek 5 | lecture 5........................................................................................................................14\n \nweek 5 slide pre-reading........................................................................................................14\n \nl5.1 - visual design - color.......................................................................................................15\n \nl5.2 - hsl with css...................................................................................................................16\n \nweek 6 | lecture 6........................................................................................................................16 \nweek 6 slides pre-reading.......................................................................................................16 \nl6.1 - intro to inclusive design.................................................................................................16 \nl6.2 - applying inclusive design...............................................................................................16 \n \n1 \n\ncompsci 345 - human-computer interaction \nbrianna yeung \nweek 1 | lecture 1 \nl1.1 - introduction to hci and ux \nhuman-computer interaction: in 1963 ivan sutherland discovered a device that allows direct manipulation of virtual \nobjects, building the \"first ipad\" called the \"sketchpad\". this is described as \"usable\" as he was using a stylus, which comes \nwith a precise tip, quite forward-thinking for a system in the 60s. \n \nux: more popular term for the practice of hci in the field, focusing on the usability of hci. \nease of use \nperformance & \nproductivity \nefficiency error avoidance learnability retainability \n \ncomponents of ux: \n1. usability  productivity, efficiency, ease of use, learnability \n2. usefulness ability to use system or product to accomplish goals of work \n3. emotional impact affective component of user experience and feelings, satisfaction \n4. meaningfulness long-term personal relationship with product \nl1.2 - design walkthrough \ndesign walkthrough example: pawshake \n1. \nprovive a channel for gathering early feedback from various stakeholders. presenting key scenarios, storyboards \nand prototypes while showing functionality such as creating a user profile. \n2. \nexplore the design from a user-centric perspective, simulating how pet owners might interact with the platform. \n3. \ntry to foresee potential usability issues that a user might encounter. for example, what if there are no sitters \navailable in an area? or what if payment fails? avoid infinite buffering or loading screens for example. \n4. \nuse it as a communication tool. there is always a gap between engineers and designers, tending towards a clash. \nhence, using personas and sketches explains what can be and cannot be done, streamlining a team. ensure \neveryone is on the same page and encourage open discussions. \n5. \nfinally, prepare to adjust a scenario to meet specific needs, keeping the design flexible and scalable so the \nplatform can iterate quickly. \nl1.3 - understanding persona \nrobot example: does not engage well with alexa because it roleplays as someone with a specific personality and behaviour \nbased on an observation of the real world (it is not a made-up persona). creating a character that can roleplay a \nstakeholder as close as possible is ideal in design. a persona that is more representative with real life people, provides \nbetter feedback to hypothesise possible user interactions. \n \nfor example, suzie is a design persona. her primary goals are to find relevant and up-to-date university information, and \nneeds to access it quickly. she stays in timaru, has a family of 3 children, is a school care advisor, and has weak tech \nconfidence. hence, this persona was designed as if it was a real person, including hobbies, personal information, etc. \n \n● \npoint to a direct one-to-one relation with an observed user behaviour \n● \nmake them 3d, as though a real person \n● \nit can be a synthesis observed over multiple users \n● \npair a persona with a scenario (goal, conditions, activities, outcomes) \n \npact framework \npeople activities context technologies \nalthough personas can be difficult to create if target users are too diverse, having too many makes the work difficult, and \nthere is the risk of incorporating unsupported designer assumptions. \n \n2 \n\ncompsci 345 - human-computer interaction \nbrianna yeung \nweek 1 reading: \"the ux book\" chapters 1, 2, 9.4 and 25.2 \nchapter 1 \n1.1: the concept of 'interaction' has expanded from how people use computers, to a wide variety of communication and \ncollaboration between a human  and an artifact in an ecology. the user experience  refers to the totality of effects felt by \nthe user as a result of interaction. interaction = [desktop, smart car, laptop, smartphone, smart devices, smart watch]. \n1.2: \"ui\" means user interface, while \"ux\" design includes the design of the interaction, conception design, the ecology, \nbut not the ui software. hci meaning \"human-computer interaction\" refers to the whole field of study, while ux is the \nmore popular term for the practice of hci in the field. \n- ux is a result of interaction, whether direct or indirect \n- ux is about the totality of the effects \n- ux is felt internally by a user \n- uc includes usage context and ecology \n1.3: there is no longer a difference between business strategy and design of the user experience. ux has become a \nmission-critical consideration for companies in every industry. bad ui/ux designs cost money and lives, for example, \ndistractions due to bad ux design for operating cars can lead to traffic accidents, injuries and even death. \n1.4: the components of ux \nusability components: [ease of use, user performance and productivity, efficiency, error avoidance, \nlearnability, retainability (ease of remembering)]. usability is important but sometimes overlooked \nby the glamorous parts of ux. for example, the flat design looks and feels visually attractive but \nmight make the screen hard to see which elements are clickable and which are not, \nusefulness is the power and functionality of backend software that provides the ability to get work done, \nemotional \nimpact \nuser experience is all experiences internally by the user, factors that are felt up close and personal \nduring the usage of technology. users are no longer satisfied with just the efficiency and \neffectiveness of usability; they are also looking for emotional satisfaction \nmeaningfulness a personal relationship that develops and endures over time between human users and a product \nthat has become a part of the user's lifestyle \n \nchapter 2 \n2.1: building usability into a system requires more than knowledge of what is good. without guidance from a ux design \nprocess, practitioners are forced to make it up as they go along, and may be limited by their own experience using their \nown favorite ways to do things while other important process activities fall through the cracks. you should use a ux lifecycle \nthat follows this framework: \n1. understand needs (of users) \n2. design solutions \n3. prototype candidates (for promoting designs) \n4. evaluate ux \n \nchapter 9.4 | user personas \na persona is a narrative of a specific design target of a ux design for a user in one work role. a hypothetical character in a \nspecific work role. as a technique for making users real to designers, a persona is a story and a description of a realistic \nindividual who has a name, life and personality, allowing designers to limit design focus to something specific. the goal of \npersonas may be to help designers focus on the needs and goals of specific types of learners, creating a more tailored and \neffective learning experience. a team is designing a mobile app for managing personal finances may gather information on \nusers' spending habits, financial goals, and attitudes towards budgeting, gathered through interviews and surveys \n \nchapter 25.2 | design walk-throughs and reviews \nwalk-throughs are an informal way to get initial reactions to design concepts by scenarios, storyboards and screen \nsketches. design reviews are more comprehensive, usually done with click-through prototypes to demo. the primary goal \nof a design walk-through may be: to simulate a cyclist using the app in various scenarios and identify potential usability \nissues or areas for improvement. the primary goal of a design review may be to:  explain what the user will be doing, what \nthe user might be thinking, and how the task fits in the work practice, workflow, and context \n3 \n\ncompsci 345 - human-computer interaction \nbrianna yeung \nweek 2 | lecture 2 \nl2.1 - heuristic evaluations \naugmented reality technology (ar tech) \nin electrical engineering, so much of what is built is invisible to us, such as electric plugs and cords behind walls. ar \ntechnology provides a greater ability to be accurate and undertake processes that would be difficult otherwise. ar is used \nwith medicine as well, like an mri, providing visibility of bones and organs. ar provides better ways of understanding what \nwe cannot directly see. \n \nheuristic evaluations \nwe take an expert in hci (me) who understands usability, design and experience of a system, and guide the design of a \nsystem, providing feedback from an hci expert point of view. \n \nnielsen's heuristics \n1. visibility of system status \n2. match between system and the real world \n3. user control and freedom \n4. consistency and standards \n5. error prevention \n6. recognition rather than recall \n7. flexibility and efficiency of use \n8. aesthetic and minimalist design \n9. help users to recognise, diagnose, and recover from errors \n10. help and documentation \n \nin an hci report, we often write what the website does well, along with the issues \n \n1. visibility of system status \nwe want people to know where they are in the system and what is happening in the system. is the system doing a \ncalculation and how do we know it's happening? meaning we cannot do anything while the system is doing those \ncalculations. do we know we are logged in? is it visible that we have a shopping basket? do we know our assignment has \nbeen submitted successfully? often the system knows, but it isn't made known to the user. \n- before an update, the uoa website did not signal whether a user was logged in as staff or student (same \ninterface), this has changed where system status is made obvious. this is good! \n- breadcrumbs exist as they show us the path we have taken to get to a particular place. without breadcrumbs, we \ncannot go back to where we were before, forcing the user to navigate back using the back button which isn't ideal \nas it breaks convention and makes navigation difficult. not a major problem as it doesn't stop people from \nachieving functionality, but might be something to raise \n \n \n2. match between system and the real world \nare we using language that is appropriate for the other users? for example, in hospitals, forms are completed on paper. if \nwe were to replicate this, we would simply copy the form into a computer system, the same things in the same palace \n(name, id, medication) in the same layout to what people are used to doing.  \n- would a first year student know what a upi is? it is not best practice to use acronyms. as someone who has \nnever been to uoa, we don't want them to guess \n- 'velocity' is an entrepreneurial group that runs at the university. a lot of people won't know what velocity is. we \nwant to understand everything from the title itself \n \n3. user control and freedom \nwe want to ensure people know how to get out of a system (do not feel trapped in a system). similarly, undo/redo buffers \nshould be offered, allowing people to redo and undo what they have just done. as well as this, we want clearly marked \nexits. \n4 \n\ncompsci 345 - human-computer interaction \nbrianna yeung \n- uoa applications look good in terms of telling us how many steps we have left and which steps we have done and \nsystem status. it saves the application as we go, and we can exit without losing anything. \n- we can go back, so no pathway is forced, providing the user with a level of control, which is great. \n \n4. consistency and standards \nwe know what shopping basket icons relate to, and we look for common symbols when we use our systems. we want \nsystems to be understandable, follow platform conventions, and terminology that stays constant. \n- underlined items are clickable, making it easy to tell which things are clickable and which aren't. although, some \nclickable items aren't underlined, hence we should ask why there is inconsistency with convention of what should \nand shouldn't be clickable. \n \n5. error prevention \nwe don't want people to make mistakes as it may slow down a system. for example, drop down lists for birthdays for each \nmonth, day and year. we can type it faster than drop-down lists that may cause more errors. \n \n6. recognition rather than recall \nis about giving people options to choose between putting things into lists, pull-downs, selections by making all sorts of \nactions visible. for example, if i can put the item on a dropdown list, that i should. why make the user type it in and maybe \nchoose an option that isn't available? essentially, use menus and lists instead of relying on blanks. \n- in the uoa application, we type in romansh for the language (swiss language), and the system seems happy \nabout that. but going back to where it asked for language, it has 76 languages that it understands. why does it \nallow romansh if it deems it an unknown language? and what's going to happen with this input? \n \n7. flexibility and efficiency of use \naccelerators are mainly in developer environments, instead of using a mouse to click a menu and selecting something, we \ncan use an accelerator for it, and we usually look for this in other platforms. hence, websites should include this for expert \nusers. \n \n8. aesthetic and minimalist design \nhow much information do we really need for a website? back in the day, people never scrolled after the first screen, \nnowadays, people are happy to scroll. remove irrelevant, or rarely needed, information. \n \n9. help users to recognise, diagnose, and recover from errors \nwe don't put system eros up, we put in messages to users to help direct them to fix the issue instead. errors should be in \nplain language, suggesting solutions. \n \n10. help and documentation \na common convention is to include help pages at the bottom on websites. easy to search, focussed on the task, details \nconcrete steps to carry out \nl2.2 - the human and fitts law cs3 \nfitts law \nis the classic performance measure. where the time to target depends on target width (w) and distance (d) to move the \npointer. the wider the target is, the faster we can attain the target, and the thinner the target, the easier it is to overshoot, \nand ofcourse, if the target is further away, even with the same width, it takes longer to reach it.  \n푇 = 푎 + 푏 푙표푔2 \n2퐷\n푊\n \nobtaining a target is different when using a pen on paper, mouse on computer, and tapping a phone screen. \n \nwhen planes crash, it's human error. paul fitts found, yes it was human error, but the design of the cockpit contributed to \nthat error. hence, when a pilot made a decision to choose to turn off an engine, where the switch is placed, contributed to \nhow long it took for the pilot to respond. so human error is impacted by the design of the cockpit.  \n \nlog\n2\n component is the index of difficulty (id);  as id increases, time to reach the target increases. \n퐼퐷 = 푙표푔2 \n2퐷\n푊\n \n5 \n\ncompsci 345 - human-computer interaction \nbrianna yeung \nprime pixel notion \nwhere the mouse is, is the prime pixel as it takes no time to get to where the mouse is. hence, right-clicking opens a \nmenu, it's fast and efficient. hence, if we know where the prime pixel is, we can take advantage of that. \n- we can get to the corner and edges of a screen really fast because the mouse stops at the edge and doesn't keep \ngoing into infinity, hence we don't need to slow down to reach the edge. \n \nfitts law's influences \n- drop down lists should be short; long lists will take long times to traverse \n- rick-click pop-up menus should be at the prime pixel \n- pie menu rather than dropdown list. hence, the same distance for every item in the menu. \nfor example, in league of legends pings \n- large targets: increase size until error rates drop off \n- adding labels to icons \n- related targets should be organised close together \n \ncalculations [which calculations can we do with fitts law?] \nmaintaining the same time to a target but we want to change something on the screen, such as changing the button size. \nhence, doubling the width of a button means doubling the distance to it. if we halve the width of the button, we should \nhalf the distance. \n \nhick-hyman law \nthe time it takes for a person to make a decision as a result of the n possible choices.  \n푇 = 푏 * 푙표푔2(푛+1)\nwe don't make decisions in linear time! we make them in log time. this is particularly important for menus, although log2 \nonly holds if the menu is sorted in a logical order (alphabetical) otherwise search time is linear! other factors that may \naffect decision time include recognition time for an icon or a word. \n \nlog2 time linear time \nlogically sorted items randomly sorted items \n \nthe human [the model human processor] \nwe have small and short working memory for processing. if we want to remember things forever, we have to move the \ninformation into long-term memory. once we learn something, it's permanent. long-term memory is also infinite. getting \npeople to fixate at different times comes with a cost. this is because the human eyes move very slowly relative to what the \nbrain processes. the cost is a quarter of a second per fixation. \nhuman memory \nsensory memory: is being overwritten all the time. if we want to use something from the sensory memory, we pay \nattention to what's in the sensory memory and store it in the short-term memory. cognition occurs in the short-term \nmemory, so we can rehearse and put it into the long-term memory, where it lasts forever. \n \nweek 2 reading: \"the ux book\" chapters 25.5, 32.3, 30.3.2.5 and 32.7.2 \n25.5. heuristic evaluation, a ux inspection method \n6 \n\ncompsci 345 - human-computer interaction \nbrianna yeung \nheuristic ux evaluation methods are based on expert ux inspection in which the evaluator compares aspects of the design \nagainst a set of heuristics (processes). nielsen states the heuristic evaluation method is inexpensive, intuitive and easy to \nmotivate practitioners to do. \n \nten heuristics \nvisibility of system status system should always inform users on what is going on through appropriate \nfeedback within reasonable time. \nmatch between system and real \nworld \nsystems should use users' language (words, phrases, concepts) familiar to the user, \nfollowing real world conventions to make information appear in a natural and \nlogical order. \nuser control and freedom when a user chooses system functions by mistake they need a clearly marked \n\"emergency exit\" to leave unwanted states without having to through an extended \ndialogue. support undo/redo \nconsistency and standards users should not have to guess what different words/situations/actions mean \nsomething \nerror prevention  even better than good error messages is designs that prevent problems from \noccurring in the first place. eliminate error-prone conditions or check for them and \npresent users with confirmation options before committing to the action. \nrecognition rather than recall minimise the users memory load by making objects, actions, and options visible, \nthe user should not have to remember information, and instructions should be \nvisible / easily retrievable \nflexibility and efficiency of use accelerators (unseen by the novice user) should be available for expert users, \nhence catering a system for both experienced and inexperienced users. \nhelp users recognise, diagnose \nand recover from errors \nerror messages should be expressed in plain language (no codes), indicate the \nproblem precisely, and suggest a solution constructively. \naesthetic and minimalist design dialogues should not contain irrelevant information. \nhelp and communication provide help documentation that is easy to search and not too large \n \n32.3. human memory limitations \nhuman memory is used in ux analysis because (1) it applied to most of the interaction cycle parts, and (2) its one of the \nnew areas of psychology that has solid empirical data supporting knowledge that is directly usable in ux design. \n \nshort-term memory / \"working memory\" \nhas a duration of 30 seconds and is a buffer storage that carries information of immediate use in performing tasks, aka \n\"throw-away data\" because it's only useful and desirable short term. the typical capacity of human short-term memory is \nabout seven, plus or minus two items; often it’s less. \n \nchunking \nitems in short-term / working memory are chunks of memory units containing one piece of data. random strings of letters \ncan be divided into groups which are remembered more easily. if the group is pronounceable, it’s even easier to remember, \neven if it has no meaning \n051594737 → 0 \n515 9 4 737 \nm,p,e,l,s,h,a → s,h,a,m,p,e,l \n \n30.3.2.5. manual dexterity and fitts’ law \na large object is obviously easier to click on than a tiny one. and location of the object can determine how easy it is to get \nat the object to manipulate it. \n● proportional to log2 of distance moved. \n● inversely proportional to log2 of target cross-section normal to the direction of motion. \n7 \n\ncompsci 345 - human-computer interaction \nbrianna yeung \n \n32.7.2. help user in doing physical actions \nthe \"physical actions\" part of the interaction cycle is about making user interface object manipulation physically easy. \nissues relevant to supporting physical actions include awkwardness and physical disabilities, manual dexterity and fitts law. \n \nweek 3 | lecture 3 \nl3.1 - contextual inquiry and brainstorming \nunderstanding what it is like to be the client to build something they can use (ar dementia example). \ncontextual inquiry: a form of usage research to understand how something is used. it is more about what they need rather \nthan what they want, hence asking them \"what do you require?\" rather than \"what do you want?\". this happens at the \nbeginning of the development cycle, when writing new software or when evaluating software products. \n \ndata elicitation steps \n1. conduct a field visit \n○ observe and interview people while they use the existing product \n○ log research notes as you encounter research data points, gather artefacts and make sketches. \n2. identify \n○ who is involved, things they use, processes involved, information required, constraints imposed, \nimputed required and outputs created \n3. model the information \n○ create descriptions of the people who do the work, document main use cases, create stories about how \nvarious aspects of work are done and create formal diagrams of the interaction. \nbefore the visit \n- learn about the subject domain, don't go into it being completely clueless. this includes understanding language, \nslang, jargon, environment, etc \n- domain exploration: learn from other design solutions, assess the position and negative aspects, respect \ncopyrighted material and don't build new systems when you can just buy one. \ndata collection \n- direct (interviews and focus groups) \n- indirect (logs and questionnaires) \nduring the visit \n- set the stage, rapport with client, explain purpose and approach \n- observation vs interviewing \n- look for user work roles, user persona, work practices, and information flow \n \nl3.2 human computer interaction \ngenerative design \nideation:  spawning ideas out of nowhere \nsketching:  capturing ideas \ncritiquing: analysing ideas \nrefining:  adopting, modifying or discarding ideas \n \nweek 3 reading: \"the ux book\" chapter 7, section 8.7.1, and chapter 14 \nchapter 7 - usage research data elicitation \nusage research is not about asking users what they want, they often don't know what they want. usage research is more \nabout understanding user work practice and work activities and reducing needs. \n \n7.2.1 - concepts of work, work practice, and work domain \n8 \n\ncompsci 345 - human-computer interaction \nbrianna yeung \nuser work identify what users are trying to accomplish (work). \nuser work practice understand how they actually do it (work practice). \nwork domain consider the bigger environment they work in (work domain). \n \nto understand people's work practice, we have to understand their needs, how they do their work so we can think of ways \nto improve effectiveness of that work, we need to know what is needed in a design to fit user needs, and finally, often \ndetails that drive work are hidden beneath the surface, hence intentions, strategies, motivations and policies should be \nconsidered, making barriers to problems less visible. \n \n7.2.5 - are we studying an existing product/system or a new one? \nanalysts and designers can become strongly biased toward thinking ahead to the new system, but almost everything we do \nin usage research starts with an existing system and work practice. for example, the ipod, when first released, was thought \nof as a unique innovation, but it started with the phonograph, essentially, reproducing recorded sound. hence the ipod was \nalready building on an existing product. \n \nchapter 14 - generative design: ideation, sketching, and critiquing \nthe overarching objective of design creation is to formulate a plan for how the system will be structured to satisfy the \necological, interaction, and emotional needs of users. \n \ngenerative design is an intertwining of ideation (brainstorming), sketching, critiquing and refining. \n1. ideation / brainstorming: the activity where ideas are spawned \n2. sketching: an externalization activity that captures those ideas in concrete representations \n3. critiquing: an analysis activity to evaluate the emergent design ideas for tradeoffs \n4. refining: an activity (usually iterative) where ideas are adopted, modified, or discarded. \n \n14.2 - ideation \nthe process of creating various and innovative proposals for ecological, interaction and emotion designs, this is a hugely \ncreative and fun phase. we can also get users and clients to participate too. \n- ideation informers: provide information on usage, requirements, targets and goals. you don't use them as \nbuilding blocks though, rayther they inform by pointing to design-oriented aspects such as personas, to consider \nor take into account the design. \n- ideation catalysts: are design inspires which precipitates an event or change without itself being affected or \nchanged \n- ideation techniques: something a designer can do to foster the spawning of a design idea, such as brainstorming, \nframing and storytelling. \n \n14.3 - sketching \nis the rapid creation of freehand drawings and expressing preliminary design ideas, focusing on concepts rather than \ndetails. it is a conversation about user experience, not art. they should be open to interpretation because if they are \ninterpreted in different ways, they may foster different and new relationships to be seen within them. \n \n14.4 - critiquing \nis the activity where the design ideas are assessed to identify advantages, disadvantages and constraints and tradeoffs are \nevaluated for each idea. users should be involved in this stage, and your job is to listen. here are some critiquing questions \n- meets design goals? \n- fits well with the ecology? communicates seamlessly with other devices in the environment? \n- supports the interaction needed with other devices? \n- provides good usability? \n- evokes positive emotional impact? \n- provides meaningfulness for users? \n \n \n \n9 \n\ncompsci 345 - human-computer interaction \nbrianna yeung \nweek 4 | lecture 4 \nl4.1 - visual design (perception and aesthetics) \nat the foundation of visual design, is that our eyes have evolved with us as organisms to keep us alive. the eyes are \ndesigned to see motion and to see order in a visual confusion. we have a lot of automatic processing, from the eye to the \nbrain, through all sorts of signal processing, before applying cognition. \n \ngestalt principles of perception \nthe example perceptions show three pacman converging, we notice a triangle even though there isn't a triangle drawn. or \nthat there is a sphere even though there isn't explicitly one present. hence, the visual system has evolved to help us see \ncertain objects. as designers, we should manipulate how humans see items, based on what we know. \n \nproximity principle objects that are close to each other will be seen as belonging together. we are \nprogrammed to see groups by proximity \n \nsimilarity principle picking differences and commonalities based on visual characteristics, such as size or \ncolor. \n \ncommon fate principle objects that move together (sharing a beginning and/or a direction and/or an end) are \nseen as related. for example, information aligned on one side so they appear to be \ngrouped.  \n \ncontinuity principle when we see lines that are all continuous but disrupt what we are able to see, then our \neyes are programmed to complete the story even if it's visually not there. when we see a \ncross, we call it a cross and not four lines coming to meet in the middle. there is a real \nnaturalness for continuity. \n \nclosure principle the black splotches, has its body leaking into the page, but the black arcs cause us to \nclose up shapes naturally. we tend to see things as complete objects even though there \nmay be gaps in the shapes. \n10 \n\ncompsci 345 - human-computer interaction \nbrianna yeung \n \n \nthe area principle we have images and text on screen. smaller objects might be seen as a figure rather than \nthe ground, also called the smallness principle. in the first image, the text on the images \nmakes the image look like a background. making something big enough, makes it no \nlonger the figure, and rather the background. it is not meant to hold the same role as a \nfigure. \n \nsymmetry principle symmetrical figures tend to be complete figures that form around their middle. \n \nsurroundedness principle objects we want users to focus on are in the center, surrounded by less interesting \nnavigation controls. \n \nprägnanz principle all together, the most concise solution is the joke below. we have the number 1 and dots \nand brackets on a yellow background to return a face. implementing common fate is by \nthe similar length of shadow. this makes it look like they're the same height off the yellow \nsurface. the yellow surrounds the back object in the middle with a greater area. \n \n \n\"what is beautiful is usable\" \nwhether we like it or not, aesthetically pleasing objects can be perceived as more error tollerations, more usable, and more \ntrustworthy. for example, apple is obsessed with ensuring artifacts are aesthetically pleasing. \n \nthree principles from aesthetics \n1. balance \nit's a metaphor for the control of distribution of optical weight in an interface. dark text is perceived as heavier \nthan the light text, whether or not the pixel actually does. this also holds for larger objects, clusters of small \nobjects, and objects with strong, intense colors. the balance on screen is achieved by providing equal weight of \nscreen elements. \n \n11 \n\ncompsci 345 - human-computer interaction \nbrianna yeung \nbalance can be formal or informal. formal, when there is symmetry, or informal where there is no symmetry. \n2. emphasis/dominance \nemphasis asks for an easily recognisable focus. such an object has dominance, others may have successive levels \nof emphasis to create a hierarchy. emphasis can be manipulated in many ways, such as isolation, color, size and \norder. \n \nfor dominance, objects of the same lightness create a static design with all objects equal in visual importance. if \nwe have three dots, one dark, ones lighter, and one very light. we create an emphasis hierarchy, that the first is \nthe darkest and the last and the lightest dots. \n3. unity \nall parts of our design have some commonality so that they are perceived as a whole. in an interface, aligning \nfonts and borders. for example, a curvy font with a box with curved edges, while serif fonts have more angles so \nunify them using smaller border radius. \n \nl4.2 - visual design - typography and reading \neye movement: saccades and fixations \nwe read by having our visual focus 'jump' between spots, a very common practice in animal vision generally. as designers, \nwe want to make text readable. so we can give clear emphasis signals, indicating to the users where to start reading, and by \nconvention it should be at the top left, we can also make it bold or make use of colors. we also should give clear targets for \nbig saccade jumps. left centred text is best for this as opposed from middle-aligned. \n \nfonts for title vs. body \nputting the whole body copy into the same font creates a challenging situation, the font is using small caps rather than \nproper lowercase, making it less readable. for the wedding example, we see a contrast in title and paragraph, while if the \ntext matches, we have existing expectations of wedding invitations, hence it doesn't have much to say. so it's okay for this \nspecific case. \n \nfont size \ntypically given in ('pt') which is about 1/72nd of an inch. it's rather small. the 'em-box' is the entire height of text in the \nfont, from bottom of the descender to the top of a cap or ascender. otherwise, it can always give size in terms of pixels. \n \nvisual design and interaction design \nusually start with low-fidelity prototyping because it's fast and replaceable, as a conversation with stakeholders in the \ndesign process. high-fidelity prototypes show full appearances in terms of size, spacing, colors, fonts and images. these \nallow us to test whether we are on track and whether or not we will deliver a good user experience. \n \nlow-fidelity: hand-drawn on paper, sticky notes. alternatively, use wireframing tools, tidier but a bit more work \nhigh-fidelity:  \n \nweek 4 reading: \"the ux book\" chapter 17 and chapter 20 \nchapter 17 - designing the interaction \n12 \n\ncompsci 345 - human-computer interaction \nbrianna yeung \nthe interaction design is about how people use the system to perform tasks within the broader work practice and covers all \npoints where the user interacts with the ecology. different ecologies require different factors, for example, the interaction \ndesign for android platforms would be different for those of ios platforms. \n \n17.3.4: leverage existing designs, for example if you're creating an email interface, you might want to include three \nelements, one side bar, and email list, and a space for the email to show up. leveraging designs with already established \npatterns should be embraced, except for if your goal is to set yourself apart in the market. \n \n17.3.5: establishing the information architecture for each device is also important, for each device, phone, laptop, tablet. \nyou should consider the system and contextual data and envision a conceptual design in the interaction perspective. this \ncan be communicated via storyboards and wireframes. \n \nchapter 20 - prototyping \nthe idea of prototypes is to provide fast and easily changed early views of an envisioned ux design. because it can be easily \nand quickly changed.  \n \n20.2.1: slicing a system's features and functionality by breadth, you get a horizontal prototype. it is very broad and offers \ndepth of functionality. it provides a simple overview. \n20.2.2: a vertical prototype contains more depth of detail for some functionality, but not all. vertical prototypes are ideal \nfor times when you need to represent completely the details of an isolated part of an individual workflow to understand \nhow those details play out in actual usage. \n20.2.4: \"t\" prototypes combine the advantages of both horizontal and vertical prototypes. \n \n20.4.4: wireframe prototypes focus on more detail into the user workflow and navigation. the flow model (a simple \ngraphical representation) giving an overview of how information flows among user work roles, is a starting point.  \nweek 5 | lecture 5 \nweek 5 slide pre-reading \ncolor: is a powerful design feature that controls energy and emphasis, impact legibility and expresses branding. color can \nbe broken down into a hue, saturation, and lightness. \n \nhue \nspectral color: roy g biv, frequency = 1 / wavelength \ncolor wheel: our perception of color can be wrapped from 0 to 360 degrees, magenta is extra spectral as there is no such \nthing as pure magenta light, or our brain synthesized a perception of a color between red and violet in hue when the retina \nis stimulated by blue and red light without much green light. \n \nmaking color \nadditive color (rgb – red, green, blue): think of tv screens or phone displays. they create colours by mixing different \namounts of red, green, and blue light. when all three are combined at full intensity, they turn white. \nsubtractive color (pigments – paint, ink): this is how painting works. when you mix paints, each colour absorbs (subtracts) \nsome light and reflects the rest—that’s the colour you see. if you mix too many, they absorb too much light and turn black \nor muddy brown. \n \nphysical reception \nthe eye receives light and transforms it into electrical energy, while light reflects off objects or is produced from a light \nsource (like a computer display). humans have three types of cone cells in their retinas. cones basically correspond to r,g \nand b with lots of overlap. \n \ncontrolling saturation and lightness \nhsl and hsv cylinders give us much more relevant control than rgb \n13 \n\ncompsci 345 - human-computer interaction \nbrianna yeung \n \n \ncolour schemes \n \nmonochromatic (one single  \nhue) \n \nanalogous (colors that are \nadjacent on the color wheel) \n \ncomplementary (colours \nthat are located opposite on \nthe colour wheel) \n \n \nl5.1 - visual design - color \nhue \nwe bend the colors into a wheel, it appears seamless. although magenta is extra spectral, it's something we experience \nwhen we see both red and blue light. which is our first hint, of our brain synthesising information. \n \nas designers, we generally make use of additive color, because most display devices have light emitted diodes of red, green \nand blue light (rgb). although, for most of history, we didn't have diodes, instead we had pigments. printing on paper, we \nuse cyan, magenta and yellow ink/toner, hence still works for painting and pigments, while the rgb color model is more \nnew and relevant to technology. \n \nretina cells located closely to the brain, where color processing happens there. on the retina, we interpret images upside \ndown, where the brain does the heavy lifting. retina contains \nrods for low light vision and cones for color vision. we have \nthree different color receptors, some respond most to green, others to red, and others to blue light. these receptors dont \nhave a precise spectrum of how they respond to color, so our perception of yellow is when the red and green cones work. \nwe also have less blue cones compared to red. \n \ncomputer colour pickers \ncolours are usually 24-bit but some systems might add 32-bit colors to add transparency, which isn't adding or changing a \ncolor, but rather tinting or shading. \n14 \n\ncompsci 345 - human-computer interaction \nbrianna yeung \nhsl & hsv \nthe rgb model makes sense, but isn't harmonial in terms of picking colors from a scheme. what is, is the hsl and hsv \nmodels.  \n● for hsl, at 0.5 lightness, we have a pure color, as we move lightness to the maximum, any hue turns to white, or \nany darkness moves towards black. \n● hsv provides rainbow colors on the top where full saturation is at a value of 1. \n \nsaturation \nto increase the brightness of a color, all r,g,b should be turned to 255 when using a color picker that uses both hsl and \nrgb. the rgb can also tell us if a green, for example, is closer to red than to blue. \n \nlightness \nlightness can be manipulated to increase or decrease contrast. using an inverse saturated photo as a small object on a \nscreen might not be the best. stretching dark colors to the entire screen might be better as the user can focus on the \ncontent rather than the boundaries of an object. \n \nthe more complex we make a color scheme, the harder the time the user will have to understand what you're trying to do \nwith all those colors. \n \nl5.2 - hsl with css \ncss selectors: each html tag (body) gets one set of styling detectives, in each case, we get one or two attribute-value pairs, \nincluding a font size, background color, color, etc. in all these cases, we use hsl (hue, saturation, luminance) such that 210 \ngets a true blue, with a saturation of 100%, and a luminance directive of 20% to shade it down alot to get a darker blue \nbackground. \n \nhigh fidelity prototypes \nshould convey the entire look of the system, or is built out of materials of the final products. for example, for a mobile app, \nthe prototype should be the size of a mobile phone. look out for the idea that the stakeholder thinks it's the final complete \nsystem. important to manage expectations \n \nlow fidelity prototypes \nis there for the low development cost, takes less effort, time and resource than a high-fidelity prototype. allowing us to \nmore effectively explore and evaluate other design concepts, a useful communication device, stakeholders can see and \nimmediately engage with feedback on post-it notes for example. \n \ncss selectors \nselectors specify which html elements you want to style \n \ncss 'box' model: padding, border, margin \nthe box model is the idea that content has a border, but padding specifies how much space there is between the border \nand the content, while the margin specifies how much space there is between the border and other html contents. \n \ncss cascades \nthey are designed for you to override what you have initially imported.  \n \nweek 6 | lecture 6 \nweek 6 slides pre-reading \ninclusive design is methodology that enables and draws on the full range of human diversity, that is, including and learning \nfrom people of different perspectives. naturally, as designers, we design for our own abilities and biases. although we need \n15 \n\ncompsci 345 - human-computer interaction \nbrianna yeung \ninclusive design to create products for the greatest number of people, it should reflect diversity and should not act as a \nbarrier to participation. \n● accessible refers to an attribute, while inclusivity refers to a method, and is more focused on disability. \n● assistive refers to specific tools or devices for disability \n● usable refers to the ease of use of a product or service \n \nprinciples of inclusive design \nprinciple 1: recognise exclusion \nprinciple 2: learn from diversity \nprinciple 3: solve for one, extend to many \n \nl6.1 - intro to inclusive design \ndesigners often design based on our own set of abilities and biases. \n \nps5 example \nthe ps5, as we all know and love, is designed for people who have two hands. ofcourse, we cannot make a \nsingle product accessible to absolutely everyone, that is, someone who is blind, would probably encounter more \nchallenges with design accessibility, but take for example, the ps5 access controller, which was designed for \npeople who only have one hand. \n \nour biases as designers \nlet's say we were tasked, as a designer, to create controllers for a game. at first, it is hard to imagine a case of \nan access controller. this leads to the creation of products that are great for some people, but inaccessible to \nothers. this creates a narrow demographic for the product. another example of companies designing outside of \nthemselves as baselines, is google home which can detect a diverse range of accents in english, while their \ncompetitor, alexa, could only detect two different accents.  \n \nexclusion \nwe should understand that there is no such thing as a \"normal\" user, just like there is not one single \"average\" \nstudent. if we make certain assumptions, we would be ignoring a big range of humanity. we should customise \nproducts so more people can use that product or service. \n \ndifferent terms: universal, assistive, accessible, usable \nthese terms are interchangeable, but have different contexts that may change their definitions \nuniversal the design for everyone in the broadest concept. for example, stairs with ramps. \nhence, wheelchair users, walkers, bikers, etc can all use the ramps. \naccessible it is an attribute, it is focused more on disability. for example, having raised sidewalks for \nblind people to navigate easily. \nassistive it is more so, specific tools or devices for disability. for example, a walking stick for \nsomeone who is blind. it is something the user uses. \nusable ease of use. how we modify to augment the product to make it easy to use. \n \nprinciples of inclusive design \n1. principle 1: recognise exclusion \nas a human being, we are naturally biased, however exclusion happens when we solve problems with \nbiases. exclusion can be temporary or a situation, either way, it still exists. \n2. principle 2: learn from diversity \nwe should constantly evolve and adapt. people, agenda, perspectives and contexts always change, \nhence we should change with these. the key is empathy, not just in terms of understanding their current \ncontext, but also how they adapt. \n3. principle 3: solve for one, extend to many \nstart with a target user, and think about how to include others into our target user. for example, movie \nsubtitles can be used for people with hearing disabilities. but, there are other use cases of subtitles, \nsuch as eating chips, or watching late at night. \n16 \n\ncompsci 345 - human-computer interaction \nbrianna yeung \npersona spectrum \nthere is a continuum from permanent disabilities to situational impairments. \n \n1. permanent: long-term or lifelong impairment \n2. temporary: short-term for temporary limitation \n3. situational: from specific environment or context an individual is in \n \n permanent temporary situational \ntouch one arm arm injury new parent \nsee blind cataract distracted driver \nhear deaf ear infection bartender \nspeak non-verbal laryngitis heavy accent \n \nactivity (in-class) \n1. present a common product or digital interface for an everyday activity \nthe notes app \n2. brainstorm potential points of exclusion for individuals with different permanent, temporary and \nsituational limitations related to at least one sense. \n permanent: people who are blind \n temporary: people who have cataract infections \n situational: someone who has a heavy accent \n3. think about mismatches between the users ability and the design of the interface \nthe notes app does not have a voice feature which can read out notes, or allow someone to speak into \na microphone to create entries. although there is a voice memo option under the keyboard, this option is \nvery small and not easy to navigate to for someone who isn't familiar with the apple notes app interface. \n \nl6.2 - designing for diversity of participation \nthe \"i-n-g\"s at the playground \nsusan goltsman proposed the \"i-n-g\"s framework, where she asks:  \n1. \"what i-n-g is most important to this environment?\" what actions are important, such as running, \ndigging, swinging, climbing, sleeping.  \n2. secondly, she asks, \"how many ways can human beings engage in that activity?\" participation does not \nrequire a particular design. but a particular design can prohibit participation. \n \ndesigning for interdependence \nthe blind marathon is where a blind person runs with another runner as an assistant, who has complementary \nskills. hence, we design with people, not for people. this is the idea of co-design to acknowledge diversity. of \ncourse the main dilemma is that we cannot make a 100% foolproof design. \n \nbenefits: \n1. increases access: more users can use your design \n2. reduced frictionl less barrier to entry for users \n3. more emotional context: your design can be empathised by many \n \ndiversity vs inclusion \nit is like the difference between hearing and listening. for example, hearing is like your ex-boyfriend. who would \nphysically be there to hear your bare minimum wants and needs then goes to fuck off and overstep your \nboundreis anyway. listening is what your ex-boyfriend could never be, that is, actually paying attention to your \nneeds and wants, and understanding that and changing behaviour to create a sound relationship.  \n \ndiversity is a community of different people, while inclusion is the culture that creates a sense of belonging, \nparticularly among diverse people. inclusion implies a diverse group of people feeling they belong. diversity \nmeans a group of different people, but not necessarily feeling like they belong. \n17 \n\n\n\ncompsci 345 - human-computer interaction \nbrianna yeung \ncoursebook: https://www.sciencedirect.com/book/9780128053423/the-ux-book  \n \nweek 7 | lecture 7.1 accessibility...................................................................................................................2 \nimportance of accessibility..............................................................................................................................2 \nweek 7 | lecture 7.2 senses...........................................................................................................................3 \ncycle times and task performance...................................................................................................................3 \nvision...............................................................................................................................................................3 \nhearing.............................................................................................................................................................3 \ntouch (haptics).................................................................................................................................................3 \nweek 8 | lecture 8.1 intro to qualitative analysis..........................................................................................3 \napproaches to qualitative analysis collection.................................................................................................4 \ngrounded theory.............................................................................................................................................4 \ncontent analysis..............................................................................................................................................4 \nthematic analysis............................................................................................................................................4 \ninterviews.................................................................................................................................................5 \nfocus groups............................................................................................................................................5 \ncase study.................................................................................................................................................5 \nethnography / observational studies.......................................................................................................5 \nweek 8 | lecture 8.2 the coding process..........................................................................................................................5\n \nvalidity vs reliability.........................................................................................................................................6 \ntwo types of coders.........................................................................................................................................6 \nweek 9 | lecture 9.1 intro to quantitative analysis........................................................................................6 \n \n1 \n\ncompsci 345 - human-computer interaction \nbrianna yeung \nweek 7 | lecture 7.1 accessibility \nimportance of accessibility \nthe who estimates that 285 million people are estimated to be visually impaired worldwide, 466 million \npeople with disabling hearing loss, and over 1 billion people to experience physical disability. mozilla mdn is \nworking on building accessible sites to benefit everyone, for example, semantic html to improve accessibility \nand search engine optimisation to make searches more findable. \n \nthe history of accessibility guidelines \nwcag (web content accessibility guidelines) were first published by the www in 2008, there are three wcag levels, a, \naa and aaa, with level aa being the most commonly used by industry. a decade later, in 2018, the w3c published version \n2.1 which added additional guidelines. \n \nweb font size \ngoal: readability of font in a typical usage scenario \nchallenge: differences in devices, interaction of user settings, for example, zoom settings, and font size.  \nsolution:  \n- explicit font size setting (16px as recommendation) \n- using standard settings \n- font size setting \n \ncolor \nchallenge: many users cannot read text if there is not sufficient contrast between the text and background \nsolution: using a contrast ratio of at least 4.5:1, using 1px, then .75pt \n- if text <18pt or <14pt (bold), then 4.5:1 contrast \n- if text >=18pt or >=14pt (bold), then 3:1 contrast \n \nfunctional color \nfunctional colour use is where colours transmit important information (e.g., traffic light colours). or, for example, in \nmarkers on these slides we use the cross and tick where the colours are only repeating the information already given by the \nsymbols: ✅❌ \n  \ntexture \ntexture aids the readability for colour blind people, instead of categorising by color, we can categorise by color and pattern \n/ texture \n \nkeyboard accessibility \nsome users with mobility impairments rely on the keyboard or on assistive technologies. every link, control, and feature \nthat can be operated with a mouse must be accessible using only a keyboard. hence why we need clear visual indication of \nthe current element in focus \n \npage title \ngood page titles are particularly important for orientation, hence there should be a title that adequately and briefly \ndescribes the content of the page. for example, using login system vs source.html \n \naccessibility of forms \nlabels and clear instructions are important for forms accessibility. each field should have an associated label or an \nappropriate title. all form controls are keyboard accessible (in the expected order) \n \nvisual clues and color \nfor example, a password entry should have red alerts (text and color) to indicate to the user that their password has been \nentered incorrectly. \n \nalt text \ntext alternatives (“alt text\") convey the purpose of an image, including pictures, illustrations, charts, etc. text \nalternatives are used by people who do not see the image. this can happen in 2 cases: when people who are \nvisually impaired rely completely on screen readers or when the link to the picture is broken. alt text should be succinct, \nand not replicate the caption of the image (if there is one) \n2 \n\ncompsci 345 - human-computer interaction \nbrianna yeung \nweek 7 | lecture 7.2 senses \ncycle times and task performance \nhow we can calculate and what tells us about our abilities, by cycling between two lines, and how many oscillations we can \ndo in 5 seconds. the human processor tells us that we should expect about 71 oscillations. also, often we would move \noutside of the lines, notice this and make a change, on average, with about 20 corrections in 5 seconds. \n \nsenses \nvision \nvision happens in two ways, (1) physical reception of stimulus and (2) processing and interpretation of \nstimulus. \n- visual angle indicates how much of the view object occupies \n- visual activity to perceive detail, which is limited \n- familiar objects perceived as constant size \n- cues like overlapping help perspective of size and depth \n \nbrightness: \n- subjective reaction to levels of light \n- affected by luminance of object \n- measured by just noticeable difference \n- visual activity increases with luminance as does flicker \ncolour: \n- made up of hue, intensity, saturation \n- cones sensitive to colour wavelengths \n- blue acuity is lowest \n- 8% males and 1% female are color blind \n \nvisual perception \nperception involves the intervention of representations and memories. visual systems constructs \na model of the world by transforming, enhancing, distorting and discarding information. the \necological approach says that, if we see a handle, there is probably a door there. the affordances \nprovide us information to make those inferences.  \nhearing \nhearing is involuntary, unless we are listening to what someone is saying, we are processing it. hence, we \nhave to make focus between what goes into the buffer, and what stays, as a coping mechanism. a baby \ncrying would grab our attention even if we try to ignore it, or someone saying our name, etc.  \n \nauditory icons \n- sounds that relate to something in the real world, for example, the sound of filling a bottle with \nwater to match moving a large file. \nearcons \n- artificial sounds where people have to learn them, more abstract relationship to action or purely \nconvention. representing other states, such as error states, or on a mac, putting something into \nthe rubbish bin (deleting a file), makes a sound that has no real relation to the real world, but it is \nlearned relatively quickly. \ntouch \n(haptics) \ntouch provides important feedback about the environment. it might be key sense for someone who is \nvisually impaired. stimulus received via receptors in the skin include pressure, heat or cold, etc. some areas \nare more sensitive than others, such as the fingertips. \n \nweek 8 | lecture 8.1 intro to qualitative analysis \n\"qualitative analysis\" is an analysis method that explores ideas based on data that cannot be quantified. qualitative data \ninclude text (document), observations, audio/visual data, and artifacts. \n \n1. looks for critical incidents (focuses on key events to analyse using specific techniques) \n2. identified themes (emergent from data depending on observational frameworks) \n3. categorises data (pre-specified categorisation schemes) \n3 \n\ncompsci 345 - human-computer interaction \nbrianna yeung \napproaches to qualitative analysis collection \n1. direct: interview \n2. direct: focus group \n3. indirect: case study \n4. indirect: ethnography \n \nthe most popular approaches to analysis is grounded theory and content analysis \ngrounded \ntheory \na method grounded in data systematically gathered and analysed. \nstage 1 \nopen \nopen coding: reading text and analysing them to identify patterns and opinions \n- open to new theories as we are not basing off of anything from the past \n- using the words interviewees directly use. for example, if a participant \nsays the ui is clumsy, you can use \"clumsy\" in the analysis. \nstage 2 development of concepts: codes that describe similar content are grouped \n- merging concepts into something that represents several interviews \nstage 3 \naxial \ngrouping concepts: into categories based on axial coding (breaking data down) \n- look at code, merge and group them (axial coding - see\n figure 1) \nstage 4 formation of theory: creating correlations between concepts, selective coding \n- merging many concepts into one key theory, so all categories are \nconnected into one \nadvantage: grounded theory provides a systematic approach with theory backed by evidence \ndisadvantage: it's a complex and contextual process (iterative), and can be overwhelming with coding \ndetails, with potential research bias. \n \nexample: qualtrics is just like a high quality google forms (on steroids). let's say a company uses the \nground theory to understand why people want to leave. qualtrics returns quotes, which translate into \nmanagerial issues, culture of long hours, and lack of travelling routes. this translates to lack of recognition \nand having too much work, which translates into being overworked and under-appreciated. \n \nfigure 1 \n \ncontent \nanalysis \na method that classifies data into themes or categories based on frequencies. for example, counting how \nmuch a specific word shows up in an interview. \n1. define the data set \n2. define the population \n3. understand the context \nthematic \nanalysis \nfocuses on patterns and themes. not coming up with a theory, just listing key themes. \nexample: how do university students experience academic stress during exams? \ngrounded theory: “pressure to perform”, where theory suggests it could be due to internalised \nexpectations and peer comparison. \ncontent analysis: 80% mention time management, 40% mention lack of sleep. \nthematic analysis: \"fear of failure\", \"coping strategies\", \"feeling isolated\" \n \n4 \n\ncompsci 345 - human-computer interaction \nbrianna yeung \n \ninterviews \nthe lesser the structure, the greater the difficulties, but it gives more opportunity for unique insights. the interviewer skills \nbecome more important. \n● fully structured (scripted) \n● semi-structured (pre-specified questions where digression is allowed) \n● unstructured (open-ended exploration) \n \nfocus groups \nyou run them as groups of gathering for discussing specific topics, and usually requires a moderator to keep the discussion \non track. they are spontaneous with a clearly defined outcome, and encourage peer relationships among participants. you \ndon't have your boss sitting next to you so you don't feel restricted. they're basically a group interview. \n \ncase study \nan in-depth study of a specific instance within a specific real-life context. it involves a variety of data types including text, \nobservation, video and artifacts. a good example is that google maps changed from using 700+ colors to only 25, which \nmakes it look more simple and cleaner. \n \nethnography / observational studies \nalso known as \"observational studies\" where we observe humans in social settings and activities, and have the observer \nimmerse themselves in the user's environment. your presence affects the observation (hawthrone effect).  \n \nweek 8 | lecture 8.2 the coding process \nanalysing text content involves assigning categories and descriptors to blocks of text, called coding. it is not just \nparaphrasing or counting keywords; you interact with data, make comparisons and derive concepts. \n \nthe coding approach \n1. emergent coding: qualitative analysis without prior theory, noting concepts and refining them into a \nmodel. it is appropriate for new topics with limited literature, often based on grounded theory. is a \ntype of inductive analysis (data without preconceived theories). code is purley emerged from the data \nwith no previous study done on this topic before. \n○ \nbenefit: open-ended, allowing theories to emerge with data \n○ \nlimit: can be more fainting as you need to be open-minded \n2. a priori coding: using an established theory to guide the selection of coding categories. uses existing \ntheoretical framework to guide coding categories (deductive analysis, data with preconceived \ntheories). it requires doing your due diligence before, and refer to this for the coding process \n○ \nbenefit: simpler to a user with a relevant framework \n○ \nlimit: limits broader insights \n \ngrounded theory \nit uses emergent coding to \"emerge\" new theories, this involved three processes \n1. open \n2. axial \n3. selective \n \nopen \nread through data, such as interview notes, identify patterns, opinions and behaviours. assign a distinctive \nname (code) to each unique phenomenon or try \"in vivo coding\" where you borrow terms from participants \ndirectly. \n \n5 \n\ncompsci 345 - human-computer interaction \nbrianna yeung \naxial \ngrouping interview terms, and coming up with a new term, which is higher-level, that represents all the four \nelements below. \n \nabusing the robot \n→ \n \n \n \nnegative treatments of the robot \nyelling at the robot \n→ \nimpersonating the robot \n→ \nnaming names \n→ \n \nselective \nregardless of your approach, you need to organise the code into a code list. in emergent coding, it develops as \nyou code, and in priori coding, it comes from your theoretical framework. \n \nvalidity vs reliability \nincrease validity \n- data organisation \n- chain of evidence \n- data source triangulation \n- accounting for all observed data \n- consider alternative explanations \n \nincrease reliability (consistency of results) \nk = p\n0\n - p\ne\n / 1 - p\ne\n \np\n0\n = probability of agreement observed \np\ne\n = probability of agreement by chance \n \nexample: two coders independently coded interview data from 50 children about their computer usage. they \nagreed on 30 of the responses. the probability of agreement by chance is estimated to be 0.4. what is the \ncohen's kappa? \n \nk = 0.2 / 0.6 = 0.333 \n \ntwo types of coders \n1. subjective coder: those who designed the study, have a good understanding on domain knowledge, \nstrong biases \n2. objective coder: not involved in the study at all, they are more open but lack domain knowledge and \nrequire training. \n \nweek 9 | lecture 9.1 intro to quantitative analysis \nquantitative analysis focuses on numerical data to evaluate user experience, focusing on \"how long\", \"how \nmany\" or \"how much\", to provide measurable insights, and complements qualitative analysis. \n \nqualitative: why do users hesitate? \nquantitative: how many hesitate and for how long? \n \n6 \n\ncompsci 345 - human-computer interaction \nbrianna yeung \nweek 10 | lecture 10.1 design affordance \naffordance: an opportunity provided by the thing for the user to interact with it correctly. it is an element of \ndesign that helps the user do something, for example, button design. a simple ui button design might give us a \nfeeling of a button being more \"clickable\" than another, based on design considerations, such as text, borders, \ncolor, shading, icons. \n \naffordance: knowing what something does without the designer telling you what it is. for example, good \nbutton affordance would be, a user knowing its a button without the designer saying its a button. \n \ntypes of affordance \nsensory: seeing the doorknob, i see the button \ncognitive: turning the doorknob, the button is easy to click \nphysical: touching the doorknob \nemotional: an emotionally impactful design of doorknob, the button plays a cute animation when i click it \nfunctional: understands the doorknob opens the door, the button does what its suppose to \n \ncognitive affordance \ncan be used as feed forward and feedback.  feedforward can help with a priori knowledge used to predict the \noutcome of an action, while feedback may help users know what happens after an action, such as visual \nfeedback. for example, for door knobs, they all show some degree of cognitive affordance.  \n- they should also help the user understand something they are new to, such as slides with \"double \nclick to edit\". \n- a simple reminder to remind users if there is an issue completing their task, such as message has no \nattachment reminders when sending an email. \n \nphysical affordance \nby looking at something, we should understand what is prioritised by the designer at the time. we should \nunderstand what items are clickable and what aren't, and in what sequence priority. \n- unlike cognitive affordance being towards more inexperienced users, physically affordance is more for \nexperienced users as they become more efficient with the interaction. \n- a physical keyboard offers better physical affordance compared to a touch screen keyboard, but \ntechnology has shifted to a touch-screen phone, hence sacrificing physical affordance for a cost (more \nscreen real estate, nicer aesthetic design, smarter predictive features such as changing language, \nemojis, etc) \n \nsensory affordance \nseeing, hearing, feeling, tasing and smelling are all aspects of sensory affordance, but we will focus on visual \naffordance. it's about the presentation of user interfaces, including visibility, discernibility, noticeability, \nlegibility, distinguishability, color and presentation. \n- visibility: the most important object shouldnt be blocked, hidden or require more actions to get to it \n- noticeability: seeing something but doesn't grab attention, it is therefore visible but not noticeable. \n- discernibility: can the user make out, detect or recognise the object, its shape, colors \n- legibility: how readable is text? can people with poor vision see? \n- distinguishability: are similar functions or objects distinguishable? \n- color: making something stand out more to influence behaviour \n- auditory: standard notifications \n- haptic and tactile: on smartphones,  \n \n7 \n\ncompsci 345 - human-computer interaction \nbrianna yeung \nfunctional affordance \ndesign features that help users employ a product or system to accomplish work, it is focused on the backend of \nthe system. \n \nemotional affordance \nfeatures that add impact to the user's emotional experiences and are often linked to aesthetic design. \n \nconsolidating affordance \n \n \nuser-created affordance \naffordance added to an original design because it was not good enough. \n8 \n\n\n\nprocess components of ux \nux design lifecycle: a cycle of the life of a ux design, from inception to deployment and beyond. \nux lifecycle activities: (1) understand needs of users. (2) design solutions. (3) prototype candidates (for promising designs). (3) evaluate ux. \nthe wheel: each rotation closer to destination. blueprint for process to any design;  applies if design scope small piece (chunk) or the whole system. \nlifecycle subactivities: (1) data elicitation, (2) data analysis, (3) data modeling, (4) requirements extraction \nux methods: a way one can carry out the whole or part of a given lifecycle activity or subactivity, for example, usage research \nux techniques: use to perform a step within an activity, subactivity, or method, for example, user interviews, observation of users at work \n \n(1) understand needs | method: usage research | sub-activity: elicit usage information | method: observe and interview | technique: manual note-taking \n(2) design solution | generative design: ideation, sketching, low-fi, criquiqing | conceptual design: mental, system models, storyboards, lo-fi | intermediate design: ecological, \ninteraction, emotional design for promising candidates, illustrated scenarios, wireframes, trade offs, medium-fidelity \n(3) prototype candidates | paper prototypes, wireframes, wireflows, click-through wireframe prototypes, physical prototypes \n(4) evaluate ux | collect evaluation data | analyse evaluation data | propose redesign solutions | report results \n \ncontextual inquiry: usage research data elicitation / understand user word and needs \ndata eliviation steps: (1) field visit (observe, interview, log, gather artefacts, sketches, diagrams) (2) identify people, things, processes, information, constraints, inputs, \noutputs. (3) model information using descriptions, document main use cases, create different stories, create formal diagrams \nbefore the visit: learn culture, vocab, slang of subject domain, learn company goals, competitors, proposed system, decide data sources and visit logistics/parameters \n \ngenerative design: ideation informers: task description, personas, user work roles, flow models, activity based interaction, task structure, sequence model, artifact models, \nsocial models, requirements | ideation catalysts: source of inspiration from observing world around you | ideation techniques (brainstorming, framing, reframing, magic wand, \nreuse successful solutions, seek opportunities for embodies, tangible interactions) \ndesign thinking: inductive: observation pattern → hypothesis -> theory | deductive: theory → hypothesis → observation → confirmation \n \ndesign walkthrough: informal reactions | scenarios, storyboards, sketches, wireframes | exploration on behalf of users through experts eye, user-centric | early feedback, \nanticipate problem | communication |flexibility and scalability | key functionalities and common tasks | open discussion with stakeholders \ndesign review: more comprehensive | click-through wireframe workflow, navigation | leader-driven | explain what user does, thinks and how the task fits in the work practice, \nworkflow, and context \npersona: help designers focus on the needs and goals of specific types of users, creating a more tailored and effective learning experience | narrative description of a specific \ndesign target in one work role | hypothetical but specific “character” | mbodies a story and description of a realistic individual | not an actual user | derived from user \nresearch | defined by user goals | consistent model | makes user \"real\" | difficult for diverse | too many bad |risk of incorporating unsupported designer assumptions \n \nheuristic evaluation: 50% hit-rate | may prejudice study design | can be stand-alone or follow scenarios \nvisibility: users shouldn't guess | appropriate feedback timely, control, transparency, trust | something in shopping cart, logged in, assignment uploaded \nmatch: vocabulary, real-word conventions, workflow match (required information, copying paper to screen, interruptions is \"edit mode\" a button? \nuser control and freedom | undo/redo | clearly marked exit \nconsistency & standards | follow platform conventions, constant terminology \nrecognition / recall | minimise memory load | option vs type it in \nflexibility | acc accelerators for expert users \naesthetic: remove irrelevant or rarely needed information \nhelp recognise: error messages and suggest solutions | \"are you sure you want to exit?\" \n \nfitts law: performance measure for time to complete the task of pointing at an object | depends on width and distance to move pointer \nhick-hyman: time to make decision as result of n possible choices |\"that's the object i want!\"| log2 if menu is sorted logically (alphabetically, days of week, manager, senior, \njunior), or if brain can predict what's next, otherwise linear \n \nmemory: sensory memories + attention → short-term + rehearsal → long-term | iconic: visual, echoic: aural, haptic: tactile stimuli \nstm: rapid access, duration 30s, limited capacity (7 +/- chunks) | experience moves recognition to recall, recognition can become barrier \nretroactive interference: new information replaces old | proactive interference: old may interfere with new \nltm: slower access, slow decay, unlimited capacity | episodic: serial memory of events, semantic : structured memory of facts, concepts, skills \nsemantic ltm derived from episodic ltm \nreading text calculation: 60 sec/min)(0.23 sec/saccade)(5 saccade/word) = 52 words/min \nmatching symbol calculation: presented with 2 symbols once. user presses a yes key if they match | tp + 2tc + tm = 100 + 2*70 + 70 = 310[130-640] msec \ncycle times and task performance: sum of cycle time 100 + 70 + 70 = 240 msec, maximum number of corrections will be about 20 (5000/240) \n \ngestalt: our visual system has evolved to perceive objects \nproximity: objects close will be seen as belonging together | whitespace can communicate your intended logical grouping of a screen the user \nsimilarity: objects with similar visual characteristics will be seen as a group and therefore related \ncommon fate: objects that move together are seen as related \ncontinuity: lines that are continuous with disruption (grass, leaves in the way) we complete even though its visually not there | \"i imagine how it continues or flows.\" \nclosure: see things as complete objects even though gaps might exist |\"i see the whole thing, even if parts are missing.\"  \narea: objects with smalls area seen as figure not the ground | if you make something big enough, it becomes the background and isn't meant to hold your attention \nsymmetry: symmetrical figures tend to be seen as complete and as groups \nsurroundedness: an area that is surrounded will be seen as the figure and the area that surrounds will be seen as the ground \nprägnanz:  we tend to perceive things based on the simplest and most stable or complete interpretation | olympic rings, face made out of numbers, complex shapes \n \nmonochromatic: one hue | analogous: hues near each other | complementary: hues opposite to each other \n \nlo-fi adv: lower cost | evaluated multiple design concepts | useful communication | addresses screen layout issues | useful identify market requirements | proof of concept \nlo-fi dis: limited error checking | poor detail spec tp code | facilitator driver | limited utility after requirements established | limited usefulness testing | nav and flow limit \nhi-fi adv: complete functionality | fully interactive | user-driven | clearly defines nav scheme | exploration and test |serves as living specification | marketing and sales tool \nhi-fi dis: more resource intensive | time consulting |inefficient for proof-of-concept | not effective for requirements gathering \n\n \ninclusive design: universal: broaden concept, design for everyone | accessible: attribute focused on disability | assistive: specific tools for disability (walking stick) \nuniversal (everyone can use / everyone can get through the door) → usable (everyone can use easily / the door handle is easy to grasp) → inclusive (gender, culture, religion, \nlang / door dimensions fits diverse people) → accessable (vision, hearing, physical, cognition / door is automatic) → assistic tools (glasses, hearing aid, wheelchair / long \nreach door opener) \npermanent: one arm, bline, deaf, non-verbal \ntemporary: arm injury, cataract, ear infection, laryngitis \nsituational: new parent, distracted driver, bartender, heavy accent \n \ninclusive design: recognize exclusion, as this helps us generate new ideas for inclusivity, aim to first solve for one target audience, then extend it to many \ncontextual inquiry for inclusion: interviews, focus groups, surveys, only difference is you work with marginalised groups \n(1)understand stakeholder difficulties, do they need safe space? assistants present?  (2) let them choose how they wish to express themselves (draw, story tell, speaking) \n \nweb font size: the general consensus for font size is 16px as a frequent recommendation, using standard settings (established frameworks). \ncolor contrast: a contrast ratio of at least 4.5:1 for normal sized text is recommended. (21:1, 19.55:1) \nlarger text size: if text < 18pt or <14pt (bold), then use a 4.5:1 contrast ratio. if text >=18pt or >14pt (bold), then use a 3:1 contrast ratio. \nfunctional color: colors can transmit information (traffic light colors), and should be used alongside other information (don't use functional color just by itself) \ntexture aids: make information of different types clear to people, for example for color blindness, patterns/texture can be an additional differentiator. \nnavigation: give people ways to navigate through information we provide, such as menus, headings, subheadings, etc especially for screen-readers \nkeyboard: some users with mobility impairments rely on keyboard assistive technologies, so ensure everything is keyboard accessible (ordering, presenting information, visual \nindications of current element in focus etc) \npage title: good titles that adequate and briefly describe the content of the page \nform fields: add names to fields so users know what goes into these fields. \nvisual clues: provide alternative information for screen-reader users. incorrect passwords shouldn't just have a red outline, it should also describe the mistake \nalt text: identifies elements for people who cannot see it, such as broken links, or people with visual impairments \naudio content: for deaf users, provide another channel of information such as audio transcripts and closed captioning on content that relies on sound \nskip navigation: allows mouse-less users to move keyboard focus to different areas of a web page without forcing them to press the tab key repeatedly \n \nquantitative data: expressed as numbers | quantitative analysis: numerical methods to ascertain size, magnitude and amount | numerical, like speed, error rate, distance \nprovides measurable evidence (numbers). expressed as numbers numerical methods to ascertain size, magnitude and amount \nqualitative data: difficult to measure as numbers, though counting words are possible | qualitative analysis: expresses nature of elements through themes, patterns, stories \ninterviews, focus groups, case study, ethnography (observational studies) | grounded theory & content analysis \nrich descriptions, motivations, \"the story.\". expressed as themes and excerpts expresses the nature of elements through themes, patterns and stories \n \ngrounded: derives theory from systematic analysis of empirical data based on a categorization approach \nstage 1: open coding: read text and analyze to identify patterns, opinions, not based on established theory; be open to all possibilities!  (in vivo coding) \nstage 2: development of concepts | codes that describe similar content are grouped together to form higher-level concepts \nstage 3: grouping of concepts | further grouping to form categories based on axial coding. axial coding: breaking down data into categories to reveal deeper insight \nstage 4: formation of theory | correlation between concepts. selective coding: identify core category. sounds linear, but complex and contextual. iterative review is key \n\"i find myself checking slack every few minutes, even when i’m trying to focus. it totally ruins my flow.\" → distraction from notifications → distractions \n\"if i don’t set a proper routine in the morning, i just drift through the day and don’t get much done.\" → lack of structure = low productivity → energy levels \n \ncontent: classifying data into themes or categories based on frequencies (both qualitative and quantitative) \n“i didn’t feel motivated without in-person classes.” → lack of engagement → lack of engagement: 25 mentions \n \nemergent coding: qualitative analysis without prior theory, noting concepts and refining them into a model. \na priori coding: using an established theory to guide the selection of coding categories. \naxial coding: identifying and defining the relationships between the concepts and categories developed from the initial codes. \n \ncohen's kappa: two coders independently coded interview data from 50 children. they agreed on 30 of the responses. the probability of agreement by chance is 0.4.  \np0 = 30/50 = 0.6 (observed agreement) \npe = 0.4 (the given chance of agreement) \ntherefore, k = (0.6 - 0.4) / (1 - 0.4) = 0.2/0.6 = 0.33 \n \nquantitative data use \nbenchmarking: comparing against goals, previous versions, or competitors. | tracking: monitoring performance/experience over time.| comparing designs: rigorously \nevaluating different solutions. | validating: providing numerical support for observations.| data-driven decisions: making informed choices based on evidence. \nmeasuring what users do (objective performance): task completion time, counting user errors, count number of tasks completed within a certain period, success rates \nasking users what they think or feel (subjective perceptions via questionnaires): labels, anchors, points, granularity \nbias in measurement instruments; a broken ruler \nbias in experimental procedures; instruction wording difference: \"hurry up\" vs \"take your time\"; participants might worry about speed rather than depth \nbias in participants: giving a questionnaire to your grandmother vs a computer science student \nbias in experimenter behaviour: giving a questionnaire to your friend \nbias in experimental environment: loud noisy, uncomfortable temperatures, poor lighting, someone watching over you \nsensory: i see the button | cognitive: i understand what button does | physical: the button easy to click | functional: the button does what its meant to do \nethics importance: design decisions can significantly impact people's rights and well-being \ndark pattern: a deceptive user interface that manipulates users into actions they didn’t intend \nfor example, for a id question where d = 8 and w = 1, you will have to do log(2*8/1) / log(2). step by steps in the calculator would be 2*8/1 -> = -> log -> / -> 2 -> log -> = \na = 0.278, b = 0.197, d = 11, t < 1, w = ? \nw = 2d / 2^[(t-a)/b] ---> 2*11 / 2^[(1-0.278)/0.197] = 1.73442445 \nthis width result is only when t = 1 though, and we want t < 1. the question also asks for only 1 digit after the decimal.  so we increase the width to 1.8 (as an increase in width \ndecreases the time taken to attain target), where now t < 1, which you can check if you insert 1.8 as the width back into the equation. \n\n",
  "380": "\n\nstats380 semester 1 2025 | final exam \n \ncode development: introduction....................................................................................................2 \nthe youth justice indicator report...................................................................................................2 \na manual approach...................................................................................................................2 \ncode development: functions.........................................................................................................3 \nfunctions to generalise a solution...................................................................................................5 \nmerging tables.................................................................................................................................5 \nfixed constants................................................................................................................................6 \nwriting higher-level functions........................................................................................................6 \ncode development: style................................................................................................................8 \ncode development: refactoring....................................................................................................10 \ncode development: divide & conquer..........................................................................................12 \nbuilding simple from complex.......................................................................................................12 \nbuilding complex from simple.......................................................................................................13 \ncode development: version control..............................................................................................15 \nmanual version control..................................................................................................................16 \ngit version control.........................................................................................................................16 \ncode development: testing...........................................................................................................19 \nchecking for weird.........................................................................................................................20 \ntest-driven development...............................................................................................................20 \ntesting for valid input.....................................................................................................................21 \nregression testing..........................................................................................................................21 \ntest scripts......................................................................................................................................22 \ncode development: debugging.....................................................................................................22 \n1. read the error message.............................................................................................................22 \n2. print the call stack......................................................................................................................22 \n3. simplify the problem..................................................................................................................23 \n4. add print statements.................................................................................................................23 \n5.  debug the function...................................................................................................................23 \ncode development: graphics........................................................................................................24 \ncode development: consolidation-1.............................................................................................25 \nrefactoring gettable()....................................................................................................................25 \nregression testing..........................................................................................................................25 \ncode development: consolidation-2.............................................................................................26 \ncode development: serialization...................................................................................................27 \ncode development: diffing r code................................................................................................27 \ncode development: convert xlsx to csv......................................................................................28 \nall together...................................................................................................................................29 \n \n\ncode development: introduction \nthe youth justice indicator report \nram raids have been recently reported to be featured regularly in the media amongst youth near the \nend of 2022. although, a couple of sources claim otherwise. we will evaluate this for ourselves. \na manual approach \nsince we only need cells x20 and ah23, we can simply go to the excel workbook, copy and paste the \ncontent of these specified cells into a .txt file and use this for our analysis. \n \n \n↓ \n \nratemanual <- read.csv('x20_ah23.txt', sep = \"\\t\") \nmatplot(t(ratemanual), type=\"l\") \n \nmanual approach using x20 - ah23 youth justice indicators summary report \n  \n \nas seen from the yji summary report vs our manual approach, the graphs look similar and the \nclaims appear to hold. although, the copy and pasted .txt file only includes children of ages 10 to 13, \nwhile there is a separate sheet for children ages 14-16. as statisticians, we know better, that other \nvariables may influence our hypothesis. we need to, therefore, dig deeper. \n \nmanual approach code-based approach \n● no record of what i did (collaboration) \n● costly, tedious and error prone \n● working with a mouse is limiting \n● provides code record of what i did \n● easier to repeat, reproduce, modify, debug \n● code is accurate and expressive \n \n\ncode development: functions \nthe 'yji 1.1 children' and 'yji 1.1 young people' sheets are populated with a messy collection of \ntables, hence we need to take subsets of each sheet. \nnaive <- read.csv(\"csv/2021-1.1-children.csv\") \nhead(naive) \n \nas seen from the above data, the sheet contains a lot of information we can ignore. cells x20 to \nah23 are what we are actually interested in, hence, we should extract the 20 to 24th rows of the \n24th or 34th column. \n \nratecols <- 24:34 \nratebyethnicgroup <- read.csv(\"csv/2021-1.1-children.csv\", skip=19, nrows=3)[ratecols] \nratebyethnicgroup \n \nyearnames <- 2011:2021 \ncolnames(ratebyethnicgroup) <- yearnames \n \n \nwe have now completely replicated our initial manual approach in a code-based approach, achieving \nthe same outcome. we now also have a record of the process, making it easier to replicate further. \nfor example, if we wanted to extract the number of distinct offenders, two sub-tables to the left of \nthe rate, we would need the same rows, only changing columns from 24th or 34th, to 2nd to 12th. \n\ncountcols <- 2:12 \ncountbyethnicgroup <- read.csv(csvfile, skip=20, nrows=3, header=false)[countcols] \ncolnames(countbyethnicgroup) <- yearnames \n \nrate by ethnic group count by ethnic group \n \n \n \nnow, to differentiate between the two graphs above, we should use the previous column, before the \n2010/11 columns as indicators to label our tables. \n \nratebyethnicgroup <- read.csv(csvfile, skip=20, nrows=3, header=false)[c(1, ratecols)] \ncolnames(ratebyethnicgroup) <- c(\"group\", yearnames) \n \n \nin the above code, [c(1, ratecols)], \nis essentially a vector of 1, 24, 25, 26, ..., 34. hence, the code \nextracts everything below row 19 (but stops at row 23 because nrows=3), for columns 1, 24, ..., 34. \nmoreover, typically, data frames are stored \"normalised\" or in \"long-format\", so using \nreshape2::melt() function in base r makes it simple to do this. side note, the melt funcion turns \n'year\" into a factor, so converting it back to numeric is useful. \n \nratebyethnicgrouplong <- reshape2::melt(ratebyethnicgroup, id.vars=\"group\",  \n     variable=\"year\", value.name=\"rate\") \nratebyethnicgrouplong$year <- as.numeric(as.character(ratebyethnicgrouplong$year)) \n \n \n \n \n \n \n→ \n \n \n \n\nfunctions to generalise a solution \nthe final two complete sets of code are provided below. although, between the two functions, the \nonly we changed were the column to keep and the name of the variable we were extracting, \ntherefore, we can generalise to create a function as opposed to a block of code. the third example \nshows how we can combine into a function. \nratebyethnicgroup <- read.csv(csvfile, skip=20, nrows=3,header=false)[c(1, ratecols)] \ncolnames(ratebyethnicgroup) <- c(\"group\", yearnames) \nratebyethnicgrouplong <-  reshape2::melt(ratebyethnicgroup, id.vars=\"group\", \n                                                  variable=\"year\", value.name=\"rate\") \nratebyethnicgrouplong$year <- as.numeric(as.character(ratebyethnicgrouplong$year)) \ncountbyethnicgroup <- read.csv(csvfile, skip=20, nrows=3,header=false)[c(1, countcols)] \ncolnames(countbyethnicgroup) <- c(\"group\", yearnames) \ncountbyethnicgrouplong <- reshape2::melt(countbyethnicgroup, id.vars=\"group\", \n                                                   variable=\"year\", value.name=\"count\") \ncountbyethnicgrouplong$year <- as.numeric(as.character(countbyethnicgrouplong$year)) \ntablebyethnicgroup <- function(keep, name) { \n    cols <- c(1, keep) \n    df <- read.csv(csvfile, skip=20, nrows=3, header=false)[cols] \n    colnames(df) <- c(\"group\", yearnames) \n    dflong <- reshape2::melt(df, id.vars=\"group\",  variable=\"year\", value.name=name) \n    dflong$year <- as.numeric(as.character(dflong$year)) \n    dflong \n} \n \nratebyethnicgrouplong <- tablebyethnicgroup(ratecols, \"rate\") \ncountbyethnicgrouplong <- tablebyethnicgroup(countcols, \"count\") \npopbyethnicgrouplong <- tablebyethnicgroup(popcols, \"pop\") \nmerging tables \nif we wanted to merge the rate and count into a single dataframe we use merge(), alternatively, to \ncombine all three, count, population and rate, we could use cbind() \nchildrenbyethnicgroup <- merge(ratebyethnicgrouplong, countbyethnicgrouplong, \n by=c(\"year\", \"group\")) \n \nchildrenbyethnicgroup <- cbind(countbyethnicgrouplong, popbyethnicgrouplong[3], \nratebyethnicgrouplong[3]) \nmerge() cbind() \n \n \n\nfixed constants \nafter combining the three variables, population, count and rate for each ethnicity in the above \ncbind(), we notice that the excel workbook contains more tables we can work with. for example, the \nnumber of distinct offenders and the percent of total (a proportion). we could use our final function \ntablebyethnicgroup(), although it contains fixed constants, for example see the comparison below. \nthis indicates we can generalise our function further, but this means more parameters are required \nfor the function input. \n \nwriting higher-level functions \nlooking further down the excel workbook, we see the rate, count and population broken down by \ngender. if we use our gettable() function defined above, we could skip 75 rows, extract only 2 rows \nand specify the other parameters as shown below. \n \n \nratebygenderlong <- gettable(75, 2, ratecols, \"rate\", \"gender\") \ncountbygenderlong <- gettable(75, 2, countcols, \"count\", \"gender\") \npopbygenderlong <- gettable(75, 2, popcols, \"pop\", \"gender\") \nchildrenbygender <- cbind(countbygenderlong, popbygenderlong[3], ratebygenderlong[3]) \n \n \n \n \n\ngetrateby <- function(skip, nrows, by) { \n    rate <- gettable(skip, nrows, ratecols, \"rate\", by) \n    count <- gettable(skip, nrows, countcols, \"count\", by) \n    pop <- gettable(skip, nrows, popcols, \"pop\", by) \n    cbind(count, pop[3], rate[3]) \n} \ngetpropby <- function(skip, nrows, by) { \n    number <- gettable(skip, nrows, numbercols, \"number\", by) \n    prop <- gettable(skip, nrows, propcols, \"prop\", by) \n    cbind(number, prop[3]) \n} \nchildrenbyage <- getrateby(85, 4, \"age\") \nchildrenbyethnicgroup <- getrateby(20, 3, \"group\") \nchildrenbygender <- getrateby(75, 3, \"gender\") \nchildrenbyethnicity <- getpropby(61, 7, \"ethnicity\") \n \nobject what it contains sheet region read \nchildrenbyage  counts, population, rates by age rows 85‑88 \nchildrenbyethnicgroup  counts, population, rates by ethnic group rows 20‑22 \nchildrenbygender  counts, population, rates by gender rows 75‑77 \nchildrenbyethnicity  numbers and proportions by ethnicity rows 61‑67 \n \n \n\ncode development: style \nindenting \nf <- function(x) { \n expr1 \n expr2 \n expr3 \n} \nfor (i in values) { \n expr1 \n expr2 \n expr3 \n} \nif (condition) { \n expr1 \n expr2 \n expr3 \n} \n \nwhitespace \nif (condition) { \n call(a = 1, b = 2, c = 3) \n} \n \nbreak long lines \ncall(a=1, b=2, c=3, d=4, e=5, f=6, g=7, h=8, i=9, j=10, k=11, l=12, m=13, n=14, o=15, p=16, q=17) \ncall(a=1, b=2, c=3, d=4, e=5, f=6, g=7, h=8, i=9, j=10, k=11, l=12,  \n       m=13, n=14, o=15, p=16, q=17 \n \ncomments \n● code should have comments \n● every function should comment its purpose, arguments and return value \n● constants should be explained \n● comment assumptions or any limits to generality of the code \n \nchecking assumptions \nalong with comments to explain assumptions, there should be code to check such assumptions. \nratebyethnicgroup <- gettable(20, 3, 24:34, \"rate\", \"group\") \n \nin the above, the assumptions are that the gettable() function assumes the first three arguments \nare numbers and the last two are character values. \ngettable <- function(skip, nrows, keep, name, by) { \n    if (!(is.numeric(skip) && is.numeric(nrows) && is.numeric(keep) &&  \n         is.character(name) && is.character(by)))  \n        stop(\"'skip', 'nrows', and 'keep' must be numeric and 'name' and 'by' must be character\") \n  ... \n ... \n} \nthe stop() function throws a meaningful error and halts execution, used for error validation. \n\ngettable <- function(skip, nrows, keep, name, by) { \n    cols <- c(1, keep) \n    df <- read.csv(csvfile, skip=skip, nrows=nrows, header=false)[cols] \n    colnames(df) <- c(by, yearnames) \n    dflong <- reshape2::melt(df, id.vars=by, \n                             variable=\"year\", value.name=name) \n    dflong$year <- as.numeric(as.character(dflong$year)) \n} \nratebyethnicgroup <- gettable(20, 3, 24, \"rate\", \"group\") \nlooking at the code, keep = 24. so cols = x(1, 24). therefore, in the following line, when reading in \nthe csv, it subsets column 1 and column 24. \n \nthen, we try to set the column names in this line: colnames(df) <- c(by, yearnames) \nerror in names(x) <- value: 'names' attribute [12] must be the same length as the vector [2] \nthe problem is we have 2 column names v1 and v24 when it's looking for 12, hence trying to \nassign column names when only 2 column names  are given. this means we assumed that the \nkeep argument is the same length as years. we can adjust the gettable() function accordingly. \n   if (length(keep) != length(yearnames)) \n        stop(\"'keep' and 'yearnames' must have the same length\") \nthis does not prevent the error, but at least we know what is going wrong when the function runs \ninto this error. \nerror in gettable(20, 3, 24, \"rate\", \"group\"): 'keep' and 'yearnames' must have the same length \n \nglobal variables \nideally, we should avoid using global variables. instead, make them local variables within a \nfunction or make them as parameter input to the function. \n \nscope and the search path \nin a function, r first searches local variables (within a function), then global variables, which can \nbe found using ls() command, and finally the loaded packages in the global workspace, found \nusing the search() command. self-contained functions are easier to debug, reuse and test as they \ndon't depend on an external state, therefore, it's best to limit hidden dependencies by passing \neverything your function needs to the best of your ability. \n \nmodular functions \n● a function should perform a single, well-defined task. \n○ avoid too difficult to write a comment that describes its purpose \n○ avoid a function with too many arguments. \n● the behaviour of a function should only depend on its arguments. \n○ avoid global variables \n● a return value should be consistent, the result should always be the same data structure and \nthe same inputs should always produce the same output. \n○ sapply() is not consistent as sometimes it outputs a vector, matrix or list \n \n\ncode development: refactoring \ngetrateby <- function(skip, nrows, by) { \n    rate <- gettable(skip, nrows, ratecols, \"rate\", by) \n    count <- gettable(skip, nrows, countcols, \"count\", by) \n    pop <- gettable(skip, nrows, popcols, \"pop\", by) \n    cbind(count, pop[3], rate[3]) \n} \ngetpropby <- function(skip, nrows, by) { \n    number <- gettable(skip, nrows, numbercols, \"number\", by) \n    prop <- gettable(skip, nrows, propcols, \"prop\", by) \n    cbind(number, prop[3]) \n} \n \ngettable <- function(csvfile, skip, nrows, keep, yearnames, name, by) { \n    cols <- c(1, keep) \n    df <- read.csv(csvfile, skip=skip, nrows=nrows,  header=false)[cols] \n    colnames(df) <- c(by, yearnames) \n    dflong <- reshape2::melt(df, id.vars=by, variable=\"year\", value.name=name) \n    dflong$year <- as.numeric(as.character(dflong$year)) \n    dflong \n} \n↓ \n## extract three tables (counts, populations, and rates) \n## \n## 'csv' is the name of the csv file to read. \n## 'skip' is the number of rows to ignore. \n## 'nrows' is the number of rows to read. \n## 'years' are the names for the year columns. \n## 'by' is the label for the variable in the first column. \n## \n## returns the combined contents of the three tables \n## as a data frame in long form. \ngetrateby <- function(csv, skip, nrows, \n                      years, by) { \n    nyears <- length(years) \n    countcols <- 1:nyears + 1 \n    popcols <- countcols + nyears \n    ratecols <- popcols + nyears \n    rate <- gettable(csv, skip, nrows, ratecols, years, \"rate\", by) \n    count <- gettable(csv, skip, nrows, countcols, years, \"count\", by) \n    pop <- gettable(csv, skip, nrows, popcols, years, \"pop\", by) \n    cbind(count, pop[3], rate[3]) \n} \n## extract two tables (numbers and proportions) \n## \n## 'csv' is the name of the csv file to read. \n## 'skip' is the number of rows to ignore. \n## 'nrows' is the number of rows to read. \n## 'years' are the names for the year columns. \n## 'by' is the label for the variable in the first column. \n## \n## returns the combined contents of the two tables \n## as a data frame in long form. \ngetpropby <- function(csv, skip, nrows, \n                      years, by) { \n    nyears <- length(years) \n    numbercols <- 1:nyears + 1 \n    propcols <- numbercols + nyears \n    number <- gettable(csv, skip, nrows, numbercols, years, \n\"number\", by) \n    prop <- gettable(csv, skip, nrows, propcols, years, \"prop\", by) \n    cbind(number, prop[3]) \n} \n## extract a single table that consists of a contiguous set  \n## of columns and a contiguous set of rows. \n## \n## 'csvfile' is the csv file to read from. \n## 'skip' is the number of rows to ignore. \n## 'nrows' is the number of rows to read. \n## 'keep' are the columns to keep. \n## 'yearnames' are the labels for the years (the columns other than the first). \n## 'name' is the label for the variable within the table. \n## 'by' is the label for the variable in the first column. \n## \n## returns the contents of the table as a data frame in long form. \ngettable <- function(csvfile, skip, nrows, keep, yearnames, name, by) { \n    cols <- c(1, keep) \n    df <- read.csv(csvfile, skip=skip, nrows=nrows, \n                   header=false)[cols] \n    colnames(df) <- c(by, yearnames) \n    dflong <- reshape2::melt(df, id.vars=by, \n                             variable=\"year\", value.name=name) \n    dflong$year <- as.numeric(as.character(dflong$year)) \n    dflong \n} \n\nthe difference \ngetrateby <- function(skip, nrows, by) { \n    rate <- gettable(skip, nrows, ratecols, \"rate\", by) \n    count <- gettable(skip, nrows, countcols, \"count\", by) \n    pop <- gettable(skip, nrows, popcols, \"pop\", by) \n    cbind(count, pop[3], rate[3]) \n} \n \n \n \n \n \n \n→ \n## comments \ngetrateby <- function(csv, skip, nrows, \n                      years, by) { \n    nyears <- length(years)                                                                          . \n    countcols <- 1:nyears + 1                                                                      . \n    popcols <- countcols + nyears                                                             . \n    ratecols <- popcols + nyears                                                                . \n    rate <- gettable(csv, skip, nrows, ratecols, years, \"rate\", by) \n    count <- gettable(csv, skip, nrows, countcols, years, \"count\", by) \n    pop <- gettable(csv, skip, nrows, popcols, years, \"pop\", by) \n    cbind(count, pop[3], rate[3]) \n} \n   \ngetpropby <- function(skip, nrows, by) { \n    number <- gettable(skip, nrows, numbercols, \"number\", by) \n    prop <- gettable(skip, nrows, propcols, \"prop\", by) \n    cbind(number, prop[3]) \n} \n \n \n \n \n \n→ \n## comments \ngetpropby <- function(csv, skip, nrows, \n                      years, by) { \n    nyears <- length(years)                                                                     . \n    numbercols <- 1:nyears + 1                                                             . \n    propcols <- numbercols + nyears                                                   . \n    number <- gettable(csv, skip, nrows, numbercols, years, \"number\", by) \n    prop <- gettable(csv, skip, nrows, propcols, years, \"prop\", by) \n    cbind(number, prop[3]) \n} \n   \ngettable <- function(csvfile, skip, nrows, keep, yearnames, name, by) { \n    cols <- c(1, keep) \n    df <- read.csv(csvfile, skip=skip, nrows=nrows,  header=false)[cols] \n    colnames(df) <- c(by, yearnames) \n    dflong <- reshape2::melt(df, id.vars=by, variable=\"year\", value.name=name) \n    dflong$year <- as.numeric(as.character(dflong$year)) \n    dflong \n}\n \n \n \n \n \n \n→ \n## comments \ngettable <- function(csvfile, skip, nrows, keep, \n   yearnames, name, by) { \n    cols <- c(1, keep) \n    df <- read.csv(csvfile, skip=skip, nrows=nrows, \n                   header=false)[cols] \n    colnames(df) <- c(by, yearnames) \n    dflong <- reshape2::melt(df, id.vars=by, \n                             variable=\"year\", value.name=name) \n    dflong$year <- as.numeric(as.character(dflong$year)) \n    dflong \n} \n  \n\ncode development: divide & conquer \nour functions, getrateby() and getpropby() both have degrees of manual approaches, for example, \nto extract the offending rate or proportion by ethnic group means manually scrolling through the \nexcel workbook to count the specified rows and knowing how many rows to skip. we should instead \nwrite code to inspect the workbook sheet and determine which rows contain tables of data. that \ntask seems far too big and complex to conduct, hence why we should divide and conquer by \nbreaking down the complex problem into smaller chunks. \n \nchildrenbyethnicgroup <- getrateby(csvfile, 20, 3, yearnames, \"group\") \nchildrenbyethnicity <- getpropby(csvfile, 61, 7, yearnames, \"ethnicity\") \nbuilding simple from complex \nfirst simplification: for each table, find one table \nsecond simplification: for each table, find the table start and find the table end \nfor the second simplification, first we need to identify existing structure in the worksheet, something \nthat differentiates the tables. we see that the first 30 values in the first column of the csv are \nirrelevant, but a table starts where there is one na value, followed by a non-na value. \n \nfirstcolumn <- read.csv(csvfile, header=false)[[1]] \nfirstcolumn[1:30] \n \n \n \n\nthird simplification: for each table, for each row, if the next two rows aren't empty, this is the table \nstart, and we should then find the end of the table. \nfor (i in 1:21) { \n    if (!is.na(firstcolumn[i])) { \n        if (!is.na(firstcolumn[i + 1])) { \n            cat(\"table starts on row\", i, \"\\n\") \n        }}} \ntable starts on row 21 \n \nwe have now identified where a table starts. although, if we run this code for 1 in 1:24 instead of 1 \nin 1:21, we get the following output. our solution does not yet work for the larger problem. \ntable starts on row 21  \ntable starts on row 22  \ntable starts on row 23  \ntable starts on row 24 \nbuilding complex from simple \nnow that we have identified where the table starts, by the above code definition, a table starts when \nthere are two non-na values, which isn't true since the table contents will be non-na values. \ntherefore, we can use logical vectors to hold the stats we are in, that, if we are intable or not. \n \nintable <- false \nfor (i in 1:24) { \n    if (!intable && !is.na(firstcolumn[i])) { \n        if (!is.na(firstcolumn[i + 1])) { \n            cat(\"table starts on row\", i, \"\\n\") \n            intable <- true \n        }}} \ntable starts on row 21 \n \nnow we need to find out where the table ends. looking at the data, it appears that a table ends \nwhen there is a row containing \"total\". \nintable <- false \nfor (i in 1:30) { \n    if (!intable && !is.na(firstcolumn[i]) &&  \n        !grepl(\"ratio\", firstcolumn[i])) { \n        if (!is.na(firstcolumn[i + 1])) { \n            cat(\"table starts on row\", i, \"\\n\") \n            intable <- true \n        } \n    }  \n    if (intable && firstcolumn[i] == \"total\") { \n        cat(\"table ends on row\", i, \"\\n\") \n    }} \ntable starts on row 21  \ntable ends on row 26 \n \nnow that our code is starting to look more complex, we should break down into smaller functions. \n\n \ntablestart <- function(column, intable, i) { \n    !intable &&  \n    !is.na(column[i]) &&  \n    !grepl(\"^ratio\", column[i]) && \n    !is.na(column[i + 1]) \n} \ntableend <- function(column, intable, i) { \n    intable && column[i] == \"total\" \n} \nintable <- false \nfor (i in 1:30) { \n    if (tablestart(firstcolumn, intable, i)) { \n        cat(\"table starts on row\", i, \"\\n\") \n        intable <- true \n    }  \n    if (tableend(firstcolumn, intable, i)) { \n        cat(\"table ends on row\", i, \"\\n\") \n    } \n} \n \nnow we can keep a record of the location of all the tables in the workbook sheet, we have the skip \nand nrows values for all tables.\n \nintable <- false \ntableskip <- numeric() \ntablerows <- numeric() \nnumtables <- 0 \nfor (i in 1:length(firstcolumn)) { \n    if (tablestart(firstcolumn, intable, i)) { \n        numtables <- numtables + 1 \n        tableskip[numtables] <- i - 1 \n        intable <- true \n    }  \n    if (tableend(firstcolumn, intable, i)) {  \n        tablerows[numtables] <- i - tableskip[numtables] - 1 \n        intable <- false \n    } \n} \n \nnow we should wrap this entire logic into a function. \ntableinfo <- function(csvfile) { \n    csv <- read.csv(csvfile, header=false) \n    column <- csv[,1] \n    intable <- false \n    tableskip <- numeric() \n    tablerows <- numeric() \n    numtables <- 0 \n    for (i in 1:length(column)) { \n        if (tablestart(column, intable, i)) { \n            numtables <- numtables + 1 \n            tableskip[numtables] <- i - 1 \n            intable <- true \n        }  \n        if (tableend(column, intable, i)) {  \n            tablerows[numtables] <- i - tableskip[numtables] - 1 \n            intable <- false \n        } \n    } \n    list(skip=tableskip, nrows=tablerows) \n} \n \n \n \n\ncode development: version control \n \n \nchildrenbyethnicgroup <- getrateby(csvfile, info$skip[1], info$nrows[1], yearnames, \"group\") \nchildrenbyethnicity <- getpropby(csvfile, info$skip[2], info$nrows[2], yearnames, \"ethnicity\") \n \n \n \nwhen code works well but it needs some changes, we should implement version control to keep \nhistories of different versions of code. now we want to repeat the above process for all the tables in \nthe workbook sheet, however there is still some manual intervention required. \n● call getrateby() or getpropby() depending on whether the table has counts or proportions \n● explicitly provide the by label (\"group\" or \"ethnicity\") \n● explicitly specify the yearnames \n \nassume we have refactored the tableinfo() function to now return the by, type and years. \n \n \n\nnow that we have a version of tableinfo() that works, we might also explore a vectorised version that \ndoes not need any loops. after exploring, we find that the vectorised version produces the same \nresult as the version that is based on loops. the changes to vectorise the function involve almost the \nentire function. if we did not keep the old version, and wanted to revert to the old version, we would \nhave to start again from scratch. \nmanual version control \nthe manual version control approach involves simply copy and pasting our old code into another file \nand naming them based on the version information. although, this only works in limited situations \nand if more changes are made, keeping track can get confusing. \n \n \n \ngit version control \nwe will work with the git2r package.  \n1. import git2r \nlibrary(git2r) \n2. initialise a repository (creating a director and calling git2r::init()) \ndir.create(\"tableinfo\") \nrepo <- init(repodir) \n       2.1. if this is the first time you have used git2r, you need to provide configuration details \nconfig(repo, user.name=\"brianna\", user.email=\"brianna@stat.auckland.ac.nz\") \n       2.2. if a repository exists, you can open it in r using repository() function \nrepo <- repository(repodir) \nsummary(repo) \n3. store functions, in separate files in the repository we just created, call status() to see \nuntracked files \nstatus(repo) \n \n4. when we want to add a file, we first add(), then commit() \nadd(repo, c(\"tableend.r\", \"tableinfo.r\", \"tablestart.r\")) \ncommit(repo, \"initial version\") \n\nnow after running status(repo), we can see a working directory clean message, and running \nsummary(repo) will show information on the repository, such as our branches, commits, etc. \n \n \nin our last example, we vectorised the function, meaning we no longer need the functions \ntablestart() and tableend(), therefore, we can remove these files from the repository. \nrm_file(repo, c(\"tablestart.r\", \"tableend.r\")) \nstatus(repo) \n \nonce we have added and committed these changes, we have a repository with three different \nversions of our code (three “commits”) recorded. in this add(), rather than specifying explicit files to \nadd, we just say add all changes to the repository (\"*\"). \nadd(repo, \"*\") \ncommit(repo, \"vectorised tableinfo(); removed tablestart() and tableend()\") \nstatus(repo) \nworking directory clean \n \nbecause we have a saved history, we can go back and view a previous version of a file \nreflog(repo) \n[abb4fca] head@{0}: commit: vectorised tableinfo(); removed tablestart() and tableend() \n[5719574] head@{1}: commit: added 'by' and 'type' information \n[3026d64] head@{2}: commit (initial): initial version \n \nviewcommit <- function(repo, commit, filename) { \n    cat(content(git2r::tree(commits(repo)[[commit]])[filename]), sep=\"\\n\") \n} \nviewcommit(repo, 2, \"tableinfo.r\") \n\nwe can even view a file that used to exist, but has now been removed. \nviewcommit(repo, 2, \"tablestart.r\") \n \nit is also useful to view the files involved in a commit and to view the difference between two \ncommits. the following functions provide a convenient interface for those tasks. \n \nlscommit <- function(repo, commit) { \n    ls_tree(git2r::tree(commits(repo)[[commit]]))[c(\"path\", \"name\")] \n} \n \ndiffcommits <- function(repo, commit1, commit2) { \n   commits <- commits(repo) \n   cat(diff(git2r::tree(commits[[commit1]]),  \n            git2r::tree(commits[[commit2]]), as_char=true)) \n} \n \nuntracked \nwhen you create a new file, git doesn’t know about it yet,  it’s untracked. \n↓ \nstaged \nwhen you add(), you tell git: “i want to include this file in the next commit.” \n↓ \ncommitted \nrunning commit() saves a snapshot of all staged changes. the commit is stored in the git history \nwith a unique id \n \n \n\ncode development: testing \nthe following function tableby() uses the type information to decide whether to call getrateby() or \ngetpropby(). \n \ncsvfile <- \"csv/2021-1.1-children.csv\" \n \ntableby <- function(csv, skip, nrows, years, by, type) { \n    if (type == \"rate\") { \n        getrateby(csv, skip, nrows,  \n                  years, by) \n    } else { \n        getpropby(csv, skip, nrows, \n                  years, by) \n    } \n} \n \ninfo <- tableinfo(csvfile) \ntables <- mapply(tableby, info$skip, info$nrows, info$by, info$type, \n                 moreargs=list(csv=csvfile, years=info$years),  \n                 simplify=false) \npar(mfrow=n2mfrow(length(info$skip)), mar=c(2, 3, 2, 0)) \ninvisible(mapply(plottable, tables, info$by, info$type)) \n \n \n \nhow do we check that our code is producing the correct result? \n\nchecking for weird \nthis approach acknowledges that we cannot check if the output is correct, but we can verify if the \noutput is definitely incorrect. this may include basic summaries or plots to quickly check for things, \nsuch as negative counts. we can treat the output of our code as data and explore it for unusual or \nunexpected values. \nsummary(tables[[1]]) \ntest-driven development \nanother approach is to create test-cases with known expected outputs. this can be achieved by using \nsynthetic data which we manufacture the output, we will have a valid test case and expected output. \nfor example, we have the tablestart() function that determines whether the ith value in the column \nis the start of the table. the intable argument is a logical value telling us where the ith values are \nalready within a table. we can generate our own sets of character, logical, and integer inputs for \nwhich we know the answer. \ntablestart <- function(column, intable, i) { \n    !intable && !is.na(column[i]) && !grepl(\"^ratio\", column[i]) && !is.na(column[i + 1]) \n} \n \nthese tests should return true if the value at position i is the start of a table, and false otherwise. \ntest 1: we're not already in a table test 2: current value column[i] is not missing \ntest1 <- rep(\"table\", 2) \n[1] \"table\" \"table\" \n \ntablestart(test1, false, 1) \n[1] true \ntablestart(test1, true, 1) \n[1] false \nthe first value is fine, and next is also fine. \ntest2 <- test1 \ntest2[1] <- na \n[1] na      \"table\" \n \ntablestart(test2, false, 1) \n[1] false \ntablestart(test2, true, 1) \n[1] false \ncan’t start a table with a missing value. \ntest 3: the next value is not missing test 4: the value is not something like \"ratio\" \ntest3 <- test1 \ntest3[2] <- na \n[1] \"table\" na    \n \ntablestart(test3, false, 1) \n[1] false \ntablestart(test3, true, 1) \n[1] false \ncan’t start a table if the next value is missing. \ntest4 <- test1 \ntest4[1] <- \"ratio\" \n[1] \"ratio\" \"table\" \n \ntablestart(test4, false, 1) \n[1] false \ntablestart(test4, true, 1) \n[1] false \n“ratio” rows are ignored,  not valid start. \n \nnow, we cannot say that our function is error-free because in order to make that claim, we would \nneed to exhaust 100% of possible inputs, all we can deduce is that our function works for all our tests \n\ntesting for valid input \nas above, we have tested that our function produces the right output for the given inputs. now we \nshould test if the function handles incorrect inputs. \n \nthe tablestart() function assumes that intable and i are both single values; what happens if we \nprovide more than one value for those arguments? \n \nyes. it does, but we can modify our function so it provides a more helpful error message. \nbefore after \ntablestart <- function(column, intable, i) { \n    !intable &&  \n    !is.na(column[i]) &&  \n    !grepl(\"^ratio\", column[i]) && \n    !is.na(column[i + 1]) \n} \ntablestart <- function(column, intable, i) { \n    if (!is.logical(intable) || length(intable) != 1 || \n        !is.numeric(i) || length(i) != 1)                        . \n        stop(paste(\"'intable' should be logical;\",      . \n                   \"'i' should be numeric;\",                        . \n                   \"both should be length 1\"))                  . \n    !intable &&  \n    !is.na(column[i]) &&  \n    !grepl(\"^ratio\", column[i]) && \n    !is.na(column[i + 1]) \n} \n \ntablestart(test1, c(false, true), 1) \nbefore: error in !intable && !is.na(column[i]): 'length = 2' in coercion to 'logical(1)' \nafter: error in tablestart(test1, c(false, true), 1): 'intable' should be logical; 'i' should be numeric; both should be length 1 \nregression testing \nwhen we refactored code into a function (or created a new version of a function), we have ensured \nthe new function produces the same result as the code that it came from (or the previous version). \nthis is known as regression testing.  \n \nwhenever we modify our code to make it better in some way, it is vital to check that we have not \nmade our code worse in some other way. \n \nif we obtain results for a data set that we believe are correct, we can save the correct results and \nthen, whenever we modify our code, we can check that the code still produces the same correct \nresults. \n \nsameoutput <- sapply(tableindex, \n                     function(i) { \n                         all.equal(tables[[i]],  \n                                   read.csv(testtables[i], check.names=false)) \n                     }) \nsameoutput \n[1] true true true true true true true true true \n\ntest scripts \nif a function is supposed to crash (error), we check that it does crash. we can write code that will \ngenerate an error if we do not get the correct result, we can do this using the stopifnot() function. \nthis checks if a condition is true. if it's not true, it stops the code and throws an error. \n \nstopifnot(sameoutput) \n \nwe want the test to produce an error when there is an error, hence we use tools::asserterror(). for \nexample, the following code will stop (with an error) if the tablestart() call does not generate an \nerror.  tools::asserterror() checks that a piece of code does generate an error. \n \ntools::asserterror(tablestart(test1, c(false, true), 1)) \n \nthere is also a tools::assertwarning() function for checking that a piece of code will generate a \nwarning. \ncode development: debugging \n1. read the error message \nerror in seq.default(yrange[2], yrange[1], length.out = length(order)): 'to' must be a finite number \nthe above error message suggests that it lies in the seq.default function, and 'to' must be a finite \nnumber suggests that the input could be na, nan or inf.  \n2. print the call stack \ntraceback() \n \n \nwe see that the error was generated by the yrange[2], yrange[1], length.out = length(order) section \nwhich is stored in labelpos. labelpos is then called in the plot. the section is being called by seq(). \nhence, we have clues as to something went wrong in seq() within plottable() \n \n\n3. simplify the problem \none of the things that we have learned from traceback() is that the problem occured within a call to \nplottable(), but that in turn occurred within a call to mapply(). the mapply() function called \nplottable() several times; our debugging task will be simpler if we can isolate which plottable() call \nwas causing the problem. \n \nbecause mapply(plottable, ...) was used (calls plottable() multiple times), you need to: \n1. switch to a for loop \n2. find out which table caused the crash \n \nfor (i in seq_along(tablesyoung)) { \n  plottable(tablesyoung[[i]], infoyoung$by[i], infoyoung$type[i]) \n} \nthis will print the iteration number before crashing. now we know table 5 is the issue. \n4. add print statements \nedit plottable() and add: \nprint(yrange) \nprint(order) \n[1] \"_\"  \"984.76444887148273\" \nthis means non-numeric data snuck in, like an underscore (\"_\"). \n5.  debug the function \nusing debug() on the plottable() \ndebug(plottable) \n \nnow, when the function is called, you can: \n1. step line by line (n) \n2. inspect any variable just by typing its name (e.g. yrange, data[[yvar]]) \n3. quit the browser with q \nthis helps us see the problem live as it unfolds. instead of manually debugging, you can tell r to \ntrigger debugging automatically on error: \n \noptions(error = recover) \n \nthis will show a numbered list of the call stack and let you enter any frame to inspect variables when \nan error occurs. \n\ncode development: graphics \n1. splitting the childrenethnicgroups  df by group and plotting the first group \nchildrenethnicgroups <-  \nsplit(childrenbyethnicgroup, childrenbyethnicgroup$group) \nplot(rate ~ year, childrenethnicgroups[[1]], type=\"l\") \n \n2. running a loop to produce a plot for each group \npar(mfrow=c(2, 2)) \nfor (i in childrenethnicgroups){ \n    plot(rate ~ year, i, type=\"l\") \n} \n \n3. use empty plot, iterate over df to add line for each group as opposed to separate plots per group. \nxrange <- range(childrenbyethnicgroup$year) \nyrange <- range(childrenbyethnicgroup$rate) \nplot.new() \nplot.window(xrange, yrange) \nbox() \naxis(1) \naxis(2) \nfor (i in seq_along(childrenethnicgroups)) { \n    lines(rate ~ year, childrenethnicgroups[[i]], type=\"l\") \n} \n4. add labels and color to plot so we can tell which line refers to which group \ngroupnames <- names(childrenethnicgroups) \npar(xpd=true) \nplot.new() \nplot.window(c(min(xrange) - .25*diff(xrange), max(xrange)), yrange) \nbox() \naxis(1) \naxis(2) \nfor (i in seq_along(childrenethnicgroups)) { \n    lines(rate ~ year, childrenethnicgroups[[i]], type=\"l\", col=i) \n    text(xrange[1], childrenethnicgroups[[i]]$rate[1], groupnames[i], \n         pos=2, adj=1, col=i) \n} \n\ncode development: consolidation-1 \nrefactoring gettable() \nproblematic \"_\" values in the gettable() produced errors. with modular functions, it's easier to \nextract and refactor. the code below shows how we can view where the error starts. instead of \nerror-handling the plottable() function itself, where the error resides, we can go lower-level to when \nwe read the data in, to account for special cases such as \"_\" to read as na. \n \ngettable <- function(csvfile, skip, nrows, keep, yearnames, name, by) { \n    cols <- c(1, keep) \n    df <- read.csv(csvfile, skip=skip, nrows=nrows, header=false, na.strings=c(\"na\", \"_\"))[cols] \n    colnames(df) <- c(by, yearnames) \n    dflong <- reshape2::melt(df, id.vars=by, variable=\"year\", value.name=name) \n    dflong$year <- as.numeric(as.character(dflong$year)) \n    dflong \n} \nbefore after \n \n \nregression testing \nsince we have altered the gettable() function and this function also has dependencies such as \ngetrateby(), getpropby(), and getby(), these functions are now at risk of being broken. we should \ntest that the changes in gettable() have not broken anything. \n \ntables <- mapply(tableby, info$skip, info$nrows, info$by, info$type, \n                 moreargs=list(csv=csvfile, years=info$years),  \n                 simplify=false) \n \ntestdir <- \"yji-2021\" \n \ntableindex <- seq_along(tables) \ntesttables <- file.path(testdir, paste0(\"table-\", tableindex, \".csv\")) \n \nall(sapply(tableindex, \n           function(i) {all.equal(tables[[i]], read.csv(testtables[i], check.names=false)) \n           })) \n[1] true \n \n\ncode development: serialization \nthis topic focuses on storing r build data into external files. the following code takes the jyitables list \nand, for each original csv file, writes out each individual table to its own csv file. \n \noutpath <- \"serialize\"  \n# creates a folder path where the csvs will be saved. \nfor (i in seq_along(yjitables)) { \n# loops over each sheet which has two elements: info and tables. \n    sheet <- yjitables[[i]]  \n# extracts one sheet, a list with two elements: info and tables. \n    info <- sheet$info  \n# stores the metadata associated with this sheet \n    for (j in seq_along(sheet$tables)) { \n# loops through each table in the current sheet. \n        table <- sheet$tables[[j]]  \n# extracts the actual data frame. \n        tablename <- gsub(\" \", \"-\", gsub(\"[()]\", \"\", names(sheet$tables)[j])) \n        write.csv(table,  \n# writes the table as a .csv file \n                  file.path(outpath, paste0(names(yjitables)[i], \"-\", tablename, \"-\", info$type[j], \".csv\")), \n                  row.names=false) \n    } \n} \n \n \ncode development: diffing r code \nlibrary(\"diffobj\") \ncomparing two functions diffobj::diffprint(f1, f2) \ncomparing functions if r formats differently from original diffobj::difffile(\"f1.r\", \"f2.r\") \ncomparing two code chunks code1 <- quote(x <- 1) \ncode2 <- quote(x <- 1:10) \ndiffobj::diffprint(code1, code2) \ncomparing two code chunks with multiple expressions code1 <- quote({ \n                    x <- 1 \n                    y <- 2    }) \ncode2 <- quote({ \n                    x <- 1:10 \n                    y <- 2:20    }) \ndiffobj::diffprint(code1, code2) \n \n\ncode development: convert xlsx to csv \nthis topic covers how to convert excel workbooks into csv files as a code-based solution. there are \nfive excel workbooks to convert and we want to write out two sheets from each workbook. \n \n \nlibrary(readxl)  \n# loads in readxl package to read .xlsx files \noutpath <- \"convert\" \n# sets the output folder where the csvs will be saved. \n \nconvertxlsx <- function(xlfile) { \n    sheets <- excel_sheets(file.path(datapath, xlfile)) # lists all sheet names from datapath/xlfile \n    keepsheets <- grep(\"^yji 1.1\", sheets) # filters sheets that start with \"yji 1.1\", returning their index \n    exportcsv <- function(sheet) { \n        df <- read_xlsx(file.path(datapath, xlfile), sheet=sheet)  # reads excel sheet into a df \n        yearstart <- regexpr(\"[0-9]{4}\", xlfile) # finds position of first 4-digit number in the filename  \n        year <- substring(xlfile, yearstart, yearstart + 3) # extracts year from filename based on position \n        safesheet <- gsub(\" \", \"-\", gsub(\"yji |[()]\", \"\", sheet)) \n        csvname <- file.path(outpath, paste0(year, \"-\", safesheet, \".csv\")) # constructs full output path  \n        write.csv(df, csvname, row.names=false) # saves the df to csv, excluding row numbers. \n    } \n    lapply(sheets[keepsheets], exportcsv) # applies exportcsv() to each sheet in the excel file \n} \n \n \n\nall together \nintroduction \nwe use r to verify the claims between the discrepancies between the media and official data of \nyouth crime rates. starting with a manual approach and explaining why this approach is flawed. \n1. open the excel file, highlight the specific data, copy and paste this into a plain text file called \nmanual-rate.txt. this data represents the offending rate per 10,000 people for three ethnic \ngroups, over 11 years. \n2. reading the data in, read.table() reads data from a text file that is structured like a table \nratemanual <- read.table(\"data/manual-rate.txt\", header=true) \n3. plotting the data, matplot() plots columns of a matrix or df, each column becomes a \nseparate line or set of points. t(ratemanual) is used because rows represent ethnic groups, \nwhile columns represent years. matplot(), by default would plot each column as a separate \nseries, plotting 11 lines, one for each year, with only 3 points per line. if we want to plot the \nethnic group over time, we need to swap the rows and columns, hence, 3 lines (one for each \nethnic group), and 11 points (for each year). \nmatplot(t(ratemanual), type=\"l\") \nthe manual approach eliminates ant reproducibility, record, modification and is highly error-prone, \nhence we need to write r code that can automatically extract data. \n \nfunctions \nwe grab the offending rates by ethnic group by looking at the spreadsheet, we notice the data lives \nin rows 20-22 and columns 24-34. skip=19 tells r to completely ignore the first 19 rows of the csv \nfile. nrows=3 tells r that after skipping 19 rows of junk, only read the next 3 rows. [ratecols] is a filter. \nof the data we just grabbed, only keep columns specified in ratecols (columns 24 to 34). we then \ntidy the column names. \ncsvfile <- \"csv/2021-1.1-children.csv\" \nratecols <- 24:34 \nratebyethnicgroup <- read.csv(csvfile, skip=19, nrows=3)[ratecols] \nyearnames <- 2011:2021 \ncolnames(ratebyethnicgroup) <- yearnames \n \nthe data does not include column names to tell us what different rows represent. we should modify \nthe existing code. \n \nratebyethnicgroup <- read.csv(csvfile, skip=20, nrows=3,  header=false)[c(1, ratecols)] \ncolnames(ratebyethnicgroup) <- c(\"group\", yearnames) \n \nthe above modification skips the first 20 rows as opposed from the first 19, meaning we should set \nheader=false. the subsetting means keep the first column and the ratecols (24-34), effectively \nappending the ethnicity names to their rates. we label this as \"group\". \n \n \n\ndf is wide, we want a row to represent a unique observation. use reshape2::melt(). see figure 1. \nratebyethnicgrouplong <-reshape2::melt(ratebyethnicgroup, id.vars=\"group\", variable=\"year\", value.name=\"rate\") \n● id.vars=\"group\" the group columns should stay as is (identifier) \n● variable=\"year\" take all column names (2011, 2012, etc) and put them in new column \"year\" \n● value.name=\"rate\" take all values from the columns and put them in a new column \"rate\" \n \nwide format: 3 rows, 12 columns \nlong format: (unique identifier x total years) 33 rows, 3 columns \n \none problem with melting the df is that the resulting year is a factor. r stores factors as characters, \nhence, converting straight to numeric would return internal integer codes, not the actual numbers. \nclass(ratebyethnicgrouplong$year) \n# [1] \"factor\" \nratebyethnicgrouplong$year <- as.numeric(as.character(ratebyethnicgrouplong$year)) \n \n \n 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 \n1 489 446 396 332 399 269 249 263 222 205 176 \n2 147 133 105 97 78 83 77 46 37 40 34 \n3 97 84 75 64 57 46 43 43 33 32 28 \n \n \n \n \n \n \n \n \n→ \ngroup year rate \n1 2011 489 \n2 2011 147 \n3 2011 97 \n1 2012 446 \n. \n. \n. \n3 2021 28 \nfigure 1: reshaping the offenders rate from wide to long format \n \nwith small modifications, we can write similar code to extract another table, the count by ethnicity. \nnow, if we compare the table extraction for the rate vs count, we see they are very similar. \n \nratebyethnicgroup <- read.csv(csvfile, skip=20, nrows=3, header=false)[c(1, ratecol)] \ncountbyethnicgroup <- read.csv(csvfile, skip=20, nrows=3, header=false)[c(1, countcols)] \ncolnames(ratebyethnicgroup) <- c(\"group\", yearnames) \ncolnames(countbyethnicgroup) <- c(\"group\", yearnames) \nratebyethnicgrouplong <-  \n    reshape2::melt(ratebyethnicgroup, id.vars=\"group\", variable=\"year\",  value.name=\"rate\") \ncountbyethnicgrouplong <-  \n    reshape2::melt(countbyethnicgroup, id.vars=\"group\", variable=\"year\", value.name=\"count\") \nratebyethnicgrouplong$year <- as.numeric(as.character(ratebyethnicgrouplong$year)) \ncountbyethnicgrouplong$year <- as.numeric(as.character(countbyethnicgrouplong$year)) \n \ntherefore, this hints we should create a single function with parameter input. \ntablebyethnicgroup <- function(keep, name){ \n    cols <- c(1, keep) \n    df <- read.csv(csvfile, skip=20, nrows=3, header=false)[cols] \n    colnames(df) <- c(\"group\", yearnames) \n    dflong <-  reshape2::melt(df, id.vars=\"group\", variable=\"year\",  value.name=value.name) \n    dflong$year <- as.numeric(as.character(dflong$year))  \n} \n\nwe should check if the new function provides the same output as our singular functions. \nall.equal(ratebyethnicgrouplong, tablebyethnicgroup(ratecols, \"rate\"))  \n# [1] true \nall.equal(countbyethnicgrouplong, tablebyethnicgroup(countcols, \"count\")) \n# [1] true \n \nin excel, there is a third table, population by ethnic group, this works for our tablebyethnicgroup \nfunction as well.  \nratebyethnicgrouplong <- tablebyethnicgroup(ratecols, \"rate\") \ncountbyethnicgrouplong <- tablebyethnicgroup(countcols, \"count\") \npopbyethnicgrouplong <- tablebyethnicgroup(popcols, \"pop\") \nsince the data is in long format, we can actually combine all three, count, rate and population into \none table. we could combine count and rate using merge, allowing two df's to combine into a single \ndf. or, we could use cbind() to combine more than two. \nchildrenbyethnicgroup <- merge(ratebyethnicgrouplong, \n                                   countbyethnicgrouplong, by=c(\"year\", \"group\")) \nchildrenbyethnicgroup <- cbind(ratebyethnicgrouplong, \n                                 popbyethnicgrouplong[3], countbyethnicgrouplong[3] )\n \n \nfurther, there's granular breakdowns of ethnicities in another table in a different format. there's two \ntables, one for the number of offenders and one for the proportion per ethnicity, providing a \npromising pattern to work with.  \n \ngettable <- function(skip, nrows, keep, name, by){ \n    cols <- c(1, keep) \n    df <- read.csv(csvfile, skip=skip, nrows=nrows, header=false)[cols] \n    colnames(df) <- c(by, yearnames) \n    dflong <-  reshape2::melt(df, id.vars=by, variable=\"year\",  value.name=value.name) \n    dflong$year <- as.numeric(as.character(dflong$year))  \n} \npropbyethnicitylong <- gettable(61, 7, propcols, \"prop\", \"ethnicity\") \nnumberbyethnicitylong <- gettable(61, 7, numbercols, \"number\", \"ethnicity\") \nchildrenbyethnicity <- cbind(numberbyethnicitylong, propbyethnicitylong[3]) \n \nlooking further, we see more tables, this time by gender. gettable() works for this dataset too! \nratebygenderlong <- gettable(75, 2, ratecols, \"rate\", \"gender\") \ncountbygenderlong <- gettable(75, 2, countcols, \"count\", \"gender\") \npopbygenderlong <- gettable(75, 2, popcols, \"pop\", \"gender\") \nchildrenbygender <- cbind(countbygenderlong, popbygenderlong[3], ratebygenderlong[3]) \n \nchildrenbyethnicity childrenbyethnicgroup childrenbygender  \n \n \n \n\nsince we can now extract rates, counts and populations for ethnicities as well as for genders, we \nshould combine them into a single function to eliminate more repeatability.\n \ngetrateby <- function(skip, nrows, by) { \n    rate <- gettable(skip, nrows, ratecols, \"rate\", by) \n    count <- gettable(skip, nrows, countcols, \"count\", by) \n    pop <- gettable(skip, nrows, popcols, \"pop\", by) \n    cbind(count, pop[3], rate[3]) \n} \n \noffenders by ethnic group (20:23, a:ah) \ngetrateby(20, 3, \"group\") \n \noffenders by gender (75:77, a:ah) \ngetrateby(75, 2, \"gender\") \n \noffenders by age (85:89, a:ah) \ngetrateby(85, 4, \"age\") \n \n \nsince we can now extract the number and proportion of offenders for ethnicities, we should combine \nthem into a single function to eliminate more repeatability. \ngetpropby <- function(skip, nrows, by) { \n    number <- gettable(skip, nrows, numbercols, \"number\", by) \n    prop <- gettable(skip, nrows, propcols, \"prop\", by) \n    cbind(number, prop[3]) \n} \noffenders by ethnicity (61:67, a:w) \ngetpropby(61, 7, \"ethnicity\") \n \n \nstyle - checking assumptions \nbased on the gettable() function, we assume that the first three inputs are numbers and the last two \nare character values. we can check these by adding more code. \ngettable <- function(skip, nrows, keep, name, by) { \n    if (!(is.numeric(skip) && is.numeric(nrows) && is.numeric(keep) && \n          is.character(name) && is.character(by)))  \n        stop(\"'skip', 'nrows', and 'keep' must be numeric and 'name' and 'by' must be character values\") \n   ... \n} \n\nlet's say this input has been provided: ratebyethnicgroup <- gettable(20, 3, 24, \"rate\", \"group\") \n \ngettable <- function(20, 3, 24, \"rate\", \"group\"){ \n    cols <- c(1, keep) \n# [1] 1 24 \n    df <- read.csv(csvfile, skip=20, nrows=3, header=false)[1, 24\n]   \ndf = \n    colnames(df) <- c(by, yearnames) \n# [1] \"group\" \"2011\" \"2012\" ,..., \"2021\" \n    error in names(x) <- value: 'names' attribute [12] must be the same length as the vector [2] \ngettable() assumes the 'keep' input is the same length as yearnames. since 'keep' = 24, two columns \nget extracted. when renaming columns, gettable() tries to name columns v1 and v2 using this list: \n[\"group\", \"2011\", \"2012\", \"2013\", \"2014\",  \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\"] \n \ntherefore, assumption checking can be implemented. \nif (length(keep) != length(yearnames)) \n        stop(\"'keep' and 'yearnames' must have the same length\") \n \nrefactoring \nwe have refactored the gettable() function so csvfile and yearnames are no longer global variables. \ngettable <- function(csvfile, skip, nrows, keep, yearnames, name, by){ \n    cols <- c(1, keep) \n    df <- read.csv(csvfile, skip=skip, nrows=nrows, header=false)[cols] \n    colnames(df) <- c(by, yearnames) \n    dflong <-  reshape2::melt(df, id.vars=by, variable=\"year\",  value.name=value.name) \n    dflong$year <- as.numeric(as.character(dflong$year)) \n    dflong \n}\n \nwe have refactored the getrateby() to eliminate global variables \ngetrateby <- function(csv, skip, nrows, years, by) { \n    nyears <- length(years) \n    countcols <- 1:nyears + 1 \n    popcols <- countcols + nyears \n    ratecols <- popcols + nyears \n    rate <- gettable(csv, skip, nrows, ratecols, years, \"rate\", by) \n    count <- gettable(csv, skip, nrows, countcols, years, \"count\", by) \n    pop <- gettable(csv, skip, nrows, popcols, years, \"pop\", by) \n    cbind(count, pop[3], rate[3]) \n}\n \nwe have refactored the getpropby() to eliminate global variables \ngetpropby <- function(csv, skip, nrows, years, by) { \n    nyears <- length(years) \n    numbercols <- 1:nyears + 1 \n    propcols <- numbercols + nyears \n    number <- gettable(csv, skip, nrows, numbercols, years, \"number\", by) \n    prop <- gettable(csv, skip, nrows, propcols, years, \"prop\", by) \n    cbind(number, prop[3]) \n} \n\ndivide and conquer \ngetrateby() and getpropby() are refactored, although, we can still simplify by reducing the number of \nparameter inputs required. looking at the data, we see that a table starts when there are two \nnon-na values consecutively. \n \n1. write a simple loop to check for two consecutive  \nnon-na values in the dataset. \n \ncsvfile <- \"csv/2021-1.1-children.csv\" \nfirstcolumn <- read.csv(csvfile, header=false)[[1]] \nfor (i in 1:21) { \n    if (!is.na(firstcolumn[i])) { \n        if (!is.na(firstcolumn[i + 1])) { \n            cat(\"table starts on row\", i, \"\\n\") \n        }}} \n \n2. if \"ratio\" is in the row, treat this like an na value and determine if we are in a table already.  \nthis code checks if we aren't already in a table, the row we are at is non-na, does not contain 'ratio' \nand the following row is also non-na, we have found the start of a table and we are now in a table. if \nwe are in a table and the row we are on says \"total\", then we have reached the end of the table. \nintable <- false \nfor (i in 1:30) { \n    if (!intable && !is.na(firstcolumn[i]) && !grepl(\"ratio\", firstcolumn[i])) { \n        if (!is.na(firstcolumn[i + 1])) { \n            cat(\"table starts on row\", i, \"\\n\")\n # table starts on row 21 \n            intable <- true \n        } }  \n    if (intable && firstcolumn[i] == \"total\") { \n        cat(\"table ends on row\", i, \"\\n\") \n# table ends on row 26 \n    }}   \n \n3. now that the code is increasing in complexity, we should use modular functions to break down. \nintable <- false \nfor (i in 1:30) { \n    if (tablestart(firstcolumn, intable, i)) { \n        cat(\"table starts on row\", i, \"\\n\") \n        intable <- true \n    }  # table starts on row 21 \n    if (tableend(firstcolumn, intable, i)) { \n        cat(\"table ends on row\", i, \"\\n\") \n    } # table ends on row 26 \n} \ntableend <- function(column, intable, i) { \n    intable && column[i] == \"total\" \n} \ntablestart <- function(column, intable, i) { \n    !intable &&  \n    !is.na(column[i]) &&  \n    !grepl(\"^ratio\", column[i]) && \n    !is.na(column[i + 1]) \n} \n \n \n\n4. now working our way back to our gettable() function, we need to make some changes. \n1. rather than the row the table starts on, we need the number of rows of skip (i-1) \n2. rather than row of table end, we need the number of rows the table has (i - tableskip - 1) \n \ntableinfo <- function(csvfile) { \n    csv <- read.csv(csvfile, header=false) \n    column <- csv[,1] \n    intable <- false \n    tableskip <- numeric() \n    tablerows <- numeric() \n    numtables <- 0 \n    for (i in 1:length(column)) { \n        if (tablestart(column, intable, i)) { \n            numtables <- numtables + 1 \n            tableskip[numtables] <- i - 1 \n            intable <- true \n        }  \n        if (tableend(column, intable, i)) {  \n            tablerows[numtables] <- i - tableskip[numtables] - 1 \n            intable <- false \n        } \n    } \n    list(skip=tableskip, nrows=tablerows) \n} \ntableinfo(csvfile) \n$skip \n[1]  20  61  75  85  96 141 164 176 188 196 \n$nrows \n[1]  5  7  3  4 12 16  5  5  5 \n \ntableinfo() explanation \nfirst, we read the csv file then store all the rows for the first column in 'column'. intable is initialised \nto false as we haven't started traversing yet. we initialise two numeric vectors, tableskip and \ntablerows and counter numtables to 0. then we enter the loop to check every row in the first \ncolumn of the csv. traversing each row, if we are at the start of a table, increment the numtables \ncount, append the row number - 1 to tableskip to log how many rows it took to arrive at a table, then \nchange intable to true, because now we are in a table. then we check if the table has ended. if it \nhas, then append to tablerows the number of rows the table has. this is calculated by the row we \nare on subtracted by all the rows we skipped to arrive at the start of the table, subtracted by 1 \nbecause the end of a table is not a data entry. since we're no longer in a table, set intable to false. \nwe then return a list of two vectors. the first vector contains the numbers of rows to skip for each \ntable found, while the second vector contains their corresponding number of rows. \n \nchildrenbyethnicgroup <-  getrateby(csvfile, info$skip[1], info$nrows[1], yearnames, \"group\") \nchildrenbyethnicity <- getpropby(csvfile, info$skip[2], info$nrows[2], yearnames, \"ethnicity\") \n\nversion control \nin this chapter we will automate three things. first, stop explicitly calling getrateby() or getpropby() \nbased on whether the table contains rates,counts and populations or proportions and numbers. \nthen stop explicitly providing the by label, and finally, stop explicitly specifying yearnames. we can \ndo this by modifying the tableinfo() function to return by, type and years, along with skip and nrows.\n \n \nnow say we have modified tableinfo() but want to experiment with a vectorised approach rather \nthan loops. say we have successfully created a vectorised modification too. now, both the loop and \nvectorised version of tableinfo() are very different from each other, we need to control the version. \n \nlibrary(git2r) \nrepodir <- \"tableinfo\" \ndir.create(repodir) \nrepo <- init(repodir) \nconfig(repo, user.name=\"paul\", user.email=\"paul@stats.nz\") # or repo <- repository(repodir) \nstatus(repo) \n \nadd(repo, c(\"tableend.r\", \"tableinfo.r\", \"tablestart.r\")) \ncommit(repo, \"initial version\") \nstatus(repo) \n \nsummary(repo)  ----------------------------------------> \ncat(diff(repo, as_char=true)) \n \n# view the differences \nreflog(repo)   \n# view commit history --> \nrm_file(repo, c(\"tablestart.r\", \"tableend.r\")) \n# remove files in repository \n \nviewcommit <- function(repo, commit, filename) {# function to view commits (including deleted file) \n    cat(content(git2r::tree(commits(repo)[[commit]])[filename]), sep=\"\\n\") \n} \nlscommit <- function(repo, commit) { \n    ls_tree(git2r::tree(commits(repo)[[commit]]))[c(\"path\", \"name\")] \n} # find what files existed in project at the time of a specific historical commit \n \ndiffcommits <- function(repo, commit1, commit2) { \n   commits <- commits(repo) \n   cat(diff(git2r::tree(commits[[commit1]]),  \n            git2r::tree(commits[[commit2]]), as_char=true)) \n} # find the exact line-by-line changes between two different historical commits \n \n \n \n \n\n \n\n\n\nstats 380 mid-semester test notes and revision \n \nr basics \n7 / 3\n    # decimal division \n7 %/% 3 \n  # integer division (floor) \n7 %% 3   # modulus operation \nlist.files(\"output\")\n  #gives all the files in the directory \n\\n \n   # new line \n\\t \n   # tab \n\\\\ \n   # literal backslash \n \nvectors \nall values have to be of the same mode(). \nlength() \n  # returns the number of elements in a vector \nthe recycling rule \nthe shorter vector will be repeated to make it the length of the longer vector. \nvector_1 <- c(1, 1, 1, 1) \nvector_2 <- c(5, 6, 7) \nvector_1 + vector_2  \n[1] 6 7 8 6 \ngenerating vectors \nseq(1, 10, by=2) \n[1] 1 3 5 7 9 \nseq(1, by=2, length.out=10) \n[1] 1 3 5 7 9 11 13 15 17 19 \nrep(c(5, 10), c(6, 3)) \n[1] 5 5 5 5 5 5 10 10 10 \nrep(c(5, 10), each=10) \n[1] 5 5 5 5 5 5 5 5 5 5 10 10 10 10 10 10 10 10 10 10  \n \nminutes \nhours \n[1]  0 10 20 30 40 50 \n[1] 800 800 800 800 800 800 900 900 900 900 900 900 \nminutes + hours \n[1] 800 810 820 830 840 850 900 910 920 930 940 950 \n \nrep(100*8:12, each=2) + c(0, 30) \n[1] 800 830 900 930 1000 1030 1100 1130 1200 1230 \nlogical vectors \ngrades <- c(80, 32, 92, 72, 54, 88, 49, 41, 63, 47) \nlow <- grades > 50 \nlow  \n[1] true false true true true true false false true false \nhigh <- grades < 50 \nhigh  \n[1] false true false false false false true true false true \n \n%in%    \n# checks whether each item in a vector can be found in another vector \ngrades2 <- c(15, 53, 62, 76, 46, 83, 46, 64, 47, 80) \ngrades %in% grades2 \n[1] true false false false false false false false false true \n\nsubsetting  # (1) create a logical vector of days with moods > 5, (2) pull out those days \nmoods <- c(10, 8, 2, 5, 5, 4, 7) \ngooddays <- moods > 5 \ngooddays  \n[1] true true false false false false true \nmoods[gooddays] \n[1] 10 8 7 \n \ncustom ordering \nmyname <- c(\"n\", \"a\", \"i\", \"b\", \"r\", \"n\", \"a\") \no <- c(4, 5, 3, 2, 1, 6, 7) \nmyname[o] \n[1] \"b\" \"r\" \"i\" \"a\" \"n\" \"n\" \"a\" \n \nappending to vectors  # the after= parameter refers to the index not the element itself \nanimals <- c(\"wolf\", \"flamingo\", \"seal\", \"eagle\") \nappend(animals , \"penguin\", after=3) \n[1] \"wolf\" \"flamingo\" \"seal\" \"penguin\" \"eagle\"    \n \ncharacter functions \nsubstring() \nbrianna <- c(\"animal jam\", \"business analytics\", \"crochet\", \"deloitte\", \"egg tart\", \"fuck\", \"gymnastics\") \nsubstring(brianna, 1, 1) \n[1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \nstrsplit() \nstrsplit(brianna, \" \") \n[[1]] \n[1] \"animal\" \"jam\" \n[[2]] \n[1] \"business\" \"analytics\" \n[[3]] \n[1] \"crochet\" \n[[4]] \n[1] \"deloitte\" \n[[5]] \n[1] \"egg\" \"tart\" \n[[6]] \n[1] \"fuck\" \n[[7]] \n[1] \"gymnastics\" \npaste() \ncontext <- c(\"i play\", \"my major is\", \"i enjoy\", \"i work at\", \"i eat\", \"my favourite word is\", \"my sport is\") \npaste(context, brianna, sep=\" \") \n[1] \"i play animal jam\" \"my major is business analytics\" \"i enjoy crochet\" \"i work at deloitte\" \n[5] \"my favourite word is fuck\" \"my sport is gymnastics\" \n \npaste(context, brianna, sep=\", \", collapse=\" \") \n[1] \"i play, animal jam my major is, business analytics i enjoy, crochet i work at, deloitte i eat, egg tart my favourite word is, \nfuck my sport is, gymnastics\" \n \nnchar()    # returns the number of characters per string in a character vector \nnchar(brianna) \n\n[1] 10 18  7  8  8  4 10 \n \nitems <- c(\"worn blankets\", \"spiked collars\", \"fox hats\", \"cupid wings\") \nnchar(items) \n[1] 13 14 8 11 \nnchar(items)-1 \n[1] 12 13 7 10 \nsubstring(items , nchar(items)-1) # substring without a third parameter assumes to the end of the character value \n[1] \"ts\" \"rs\" \"ts\" \"gs\" \nsubstring(items , 1, nchar(items)-1)  \n[1] \"worn blanket\"  \"spiked collar\" \"fox hat\" \"cupid wing\"   \n \ntimes <- paste(rep(8:12, each=2), sprintf(\"%02d\", seq(0, 50, 10)), sep=\":\") \n[1] \"8:00\" \"8:10\" \"9:20\" \"9:30\" \"10:40\" \"10:50\" \"11:00\" \"11:10\" \"12:20\" \"12:30\" \n \ngrep()    # gets the indexes of character(s) in a vector if it contains the specified character \ngrep(\":00\", times, fixed=true) \n# fixed=true means the first argument is literal text \n[1] 1 7 \n \ngrepl()    # same as grep() but returns a logical vector \ngrepl(\":00\", times, fixed=true) \n[1] true false false false false false true false false false \n \ngsub()    # gsub() finds and removes (replaces) \ngsub(\"s\", \"z\", items, fixed=true) \n[1] \"worn blanketz\"  \"spiked collarz\" \"fox hatz\" \"cupid wingz\"   \n \nlists    # instead of storing variables separately, we can store them in a list \nname = \"brianna\" \nage = 21 \nbirthday = \"09-05-2003\" \nmyfriends <- c(\"mellisa\", \"aulia\", \"tanveer\") \nme <- list(name = \"brianna\", \n   age = 21, \n   birthday = \"09-05-2003\",  \n   friends = myfriends) \nme  \n$name \n[1] \"brianna\" \n \n$age \n[1] 21 \n \n$birthday \n[1] \"09-05-2003\" \n \n$friends \n[1] \"mellisa\" \"aulia\"   \"tanveer\" \n \nme$friends[2]   \n# extracting an element in a vector stored in a list \n[1] \"aulia\" \n \n\nr graphics \ncex()     # a size multiplier\n \ncol = hcl(240, 40, 40, alpha=0.5) # changing the transparency of colours using alpha() and the hsl triplets \n \nmanual plotting \nplot.new()  # start a new empty plot \nplot.window(x, y) # sets scale on axes \npoints(x, y)  # adds data points \nlines(x, y)  # adds the trend line; can make use of col, lty, pch \naxis(1)   # draws the x axis \naxis(2)   # draws the y axis \nbox()   # adds a border \n \ntwo plots in one \ngrades <- c(87, 74, 95, 63) \nattendance <- c(10, 9, 10, 5) \n \ngrades2 <- c(41, 56, 72, 77) \nattendance2 <- c(2, 4, 8, 8) \n \nplot.new() \nplot.window(range(grades, grades2), range(attendance, attendance2)) \npoints(grades, attendance, col=2) \nlines(grades, attendance) \npoints(grades2, attendance2, col=4) \nlines(grades2, attendance2) \naxis(1)   \naxis(2)   \nbox()   \n \nlegends \nlegend(\"topleft\", legend = c(\"2025\", \"2024\"), col=c(2, 4), pch=1, lwd=1) \n \nr data structures \nmatrices \nmatrix(1:4, nrow=2) \n1 3 \n2 4 \nmatrix(1:20, nrow=3, ncol=7) \n1 4 7 10 13 16 19 \n2 5 8 11 14 17 20 \n3 6 9 12 15 18 01   \n# recycling rule applies where the final element = 1 \n \nmatrix(1:9, nrow=3, byrow=true) \n1 2 3 \n4 5 6 \n7 8 9 \ncbind(1:3, 4:6, 7:9, 10:12) \n1 4 7 10 \n2 5 8 11 \n3 6 9 12 \n\nrbind(1:3, 4:6, 7:9, 10:12) \n  1   2   3 \n  4   5   6 \n  7   8   9 \n10 11 12 \nhours <- 0:11     \n# [1] 0 1 2 3 4 5 6 7 8 9 \nminutes <- seq(0, 50, 10)   \n# [1] 0 10 20 30 40 50 \nas.numeric(outer(minutes, hours*100, \"+\")) \n#hours*100 is 0 100 200 300 400 500 600 700 800 900 \n00 100 200 300 400 500 600 700 800 900 \n10 110 210 310 410 510 610 710 810 910  \n20 120 220 320 420 520 620 720 820 920 \n30 130 230 330 430 530 630 730 830 930 \n40 140 240 340 440 540 640 740 840 940 \n50 150 250 350 450 550 650 750 850 950 \n \nmymat <- matrix(1:9, nrow=3) \nmymat \n1 4 7 \n2 5 8 \n3 6 9 \nmymat * 2  \n# mathematical operations apply to all elements, similar to vector arithmetic \n1   8 14 \n4 10 16 \n6 12 18 \n \ntranspose \nt(mymat )\n  # flips rows and columns \n1 2 3 \n4 5 6 \n7 8 9 \n \nnaming matrices rows and columns \nanimaljam <- matrix(c(\"eagle\", \"flamingo\", \"seal\", \"spike\", \"worn\", \"necklace\", \"bear\", \"owl\", \"mouse\"), nrow=3) \n     [,1]         [,2]         [,3]    \n[1,] \"eagle\"     \"spike\"     \"bear\"  \n[2,]  \"flamingo\"  \"worn\"      \"owl\"   \n[3,]  \"seal\"      \"necklace\" \" mouse\" \n \ncolnames(animaljam) <- c(\"animals\", \"clothing\", \"pets\") \nrownames(animaljam) <- c(\"avatar 1\", \"avatar 2\", \"avatar 3\") \n          animals     clothing    pets    \navatar 1  \"eagle\"     \"spike\"     \"bear\"  \navatar 2  \"flamingo\"  \"worn\"      \"owl\"   \navatar 3  \"seal\"      \"necklace\"  \"mouse\" \n \nsubsetting matrices \nanimaljam[2, 3] \n[1] \"owl\" \nanimaljam[3, 1:3] \n[1] \"seal\" \"necklace\" \"mouse\" \nanimaljam[, \"animals\"] \n[1] \"eagle\" \"flamingo\" \"seal\" \n\nanimaljam[\"avatar 1\", \"animals\"] \n[1] \"eagle\" \n \nfactors \nlongspikes <- c(\"black\", \"red\", \"pink\", \"purple\",\"blue\", \"yellow\", \"green\", \"orange\") \nhavespikes <- c(1, 0, 0, 0, 0, 1, 0, 0) \nspikes_df <- data.frame(longspikes, havespikes) \nspikes_df <- data.frame(\"longspikes \" = longspikes, \"havespikes\" = havespikes) \n \nspikestate <- factor(havespikes, labels=c(\"has spike\", \"no spike\")) \n[1] no spike  has spike has spike has spike has spike no spike  has spike has spike \nlevels: has spike no spike \n \nin r, levels automatically use the unique character values in alphabetical order \ndaysofweek <- c(\"sa\", \"th\", \"tu\", \"mo\", \"we\", \"mo\", \"th\", \"sa\", \"we\", \"we\", \"fr\", \"su\") \ndaysfactor <- factor(daysofweek ) \n[1] sa th tu mo we mo th sa we we fr su \nlevels: fr mo sa su th tu we   \n# an unnatural order, sorted alphabetically \nso, we can specify a custom order using label  \ndaysfactor <- factor(daysofweek, label=c(\"mo\", \"tu\", \"we\", \"th\", \"fr\", \"sa\", \"su\")) \n[1] we fr sa tu su tu fr we su su mo th \nlevels: mo tu we th fr sa su   \n# factors no sorted in a natural order \n \nfactor functions: table() \ntable(daysfactor \n)    # table() returns a table of counts automatically \nmo tu we th fr sa su  \n 1     2     2     1   2    1   3 \n \ndata frames \nsubset() \nthe subset() function can be used to select rows and/or columns \ngrades <- c(95, 84, 76, 87, 80) \ncourses <- c(\"stats330\", \"compsci345\", \"compsci399\", \"busan300\", \"busan302\") \nsemone <- data.frame(grades=grades, courses = courses) \nsubset(semone , grades > 80) \n grades    courses \n1     95 stats330 \n2     84  compsci345 \n4     87 busn300 \n \nordering a data frame \no <- order(semone$grades, decreasing=true) \nsemone[o, ] \n grades    courses \n1     95   stats330 \n4     87   busan300 \n2     84  compsci345 \n5     80   busan302 \n3     76  compsci399 \n \n \n \n\ncombining data frames \ngrades2 <- c(72, 89, 76, 88) \ncourses2 <- c(\"compsci335\", \"stats220\", \"business350\", \"busan305\") \nsemtwo <- data.frame(grades=grades2 , courses = courses2 )  \n# new data frame \nto apply a id column so we can identify which rows are from which semester: \ns1 <- cbind(id=\"s1\", semone ) \ns2 <- cbind(id=\"s2\", semtwo ) \nwholeyear <- rbind(s1, s2) \nid  grades     courses \ns1      95     stats330 \ns1      84   compsci345 \ns1      76   compsci399 \ns1      87      busan300 \ns1      80     busan302 \ns2     72   compsci335 \ns2     89     stats220 \ns2      76  business350 \ns2     88      busan305 \n \nr programming \ncontrol flow \nfor loops \nwishlist<- c(\"headdress\", \"party hat\", \"snow leopard slippers\", \"furry hat\", \"glitched ring\") \nfor (item in wishlist){ \n print(item) \n} \n[1] \"headdress\" \n[1] \"party hat\" \n[1] \"snow leopard slippers\" \n[1] \"furry hat\" \n[1] \"glitched ring\" \n \nfor loops with conditions \nnumbers <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) \nfor (num in numbers){ \nif (num %% 2 == 0){ \n print(paste0(num, \" is even\")) \n}else{ \nprint(paste0(num, \" is odd\"))} \n} \n[1] \"1 is odd\" \n[1] \"2 is even\" \n[1] \"3 is odd\" \n[1] \"4 is even\" \n[1] \"5 is odd\" \n[1] \"6 is even\" \n[1] \"7 is odd\" \n[1] \"8 is even\" \n[1] \"9 is odd\" \n[1] \"10 is even\" \n \n\nwhile loops \ncount <- 1 \nwhile (count < 11){ \n if (numbers[count] %% 2 == 0){ \n  print(paste0(numbers[count], \" is even\")) \n  count <- count + 1 \n }else { \nprint(paste0(numbers[count], \" is odd\")) \n  count <- count + 1 \n }  \n} \n[1] \"1 is odd\" \n[1] \"2 is even\" \n[1] \"3 is odd\" \n[1] \"4 is even\" \n[1] \"5 is odd\" \n[1] \"6 is even\" \n[1] \"7 is odd\" \n[1] \"8 is even\" \n[1] \"9 is odd\" \n[1] \"10 is even\" \n \nthe ifelse() function \nnumbers <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) \noddeven <- ifelse(numbers %% 2 == 0, \"even\", \"odd\") \n[1] \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \n \nfunctions \ncountevenoddnumbers <- function(numbers){ \nevencount <- 0 \noddcount <- 0 \n for (num in numbers){ \n  if (num %% 2 == 1) { \n   oddcount <- oddcount + 1 \n} else { \nevencount <- evencount + 1 \n} \n} \nreturn(c(evencount, oddcount )) \n} \nnumbers <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) \ncountevenoddnumbers(numbers) \n[1] 5 5 \n \n \n \n \n \n \n \n\nstatistical computing \nsplit-apply-combine \nstudyhours <- c(840, 950, 1030, 850, 920, 750, 1940, 2030, 2310, 2330) \nmood <- c(5, 6, 3, 7, 3, 8, 9, 8, 7, 8) \nsay we want to plot the study hours by mood, but have a separate trendline for day and night time hours. \nlate <- studyhours > 1800 \nearly <- studyhours < 1800 \nnight <- ifelse(late, \"night\", \"day\") \nnight  \n[1] \"day\" \"day\" \"day\" \"day\" \"day\" \"day\" \"night\" \"night\" \"night\" \"night\" \nsplit() \nthe split() function takes a vector or data frame and breaks it into pieces, the first argument is a vector to split, and the \nsecond argument is a vector of values that indicate group membership. \nstudyhoursdaynight <- split(studyhours, night) \n$day \n[1] 840 950 1030 850 920 750 \n$night \n[1] 1940 2030 2310 2330 \n \nlapply() \nthe lapply() function helps to perform action on each \"group\" of a list more efficiently. for example, if we wanted to compute \nthe mean of day and night, we would simply call mean() on both groups, although if we have many groups, the code becomes \nvery long and repetitive. \nstudyhoursnightavg <- lapply(studyhoursdaynight , mean) \n$day \n[1] 890 \n$night \n[1] 2152.5 \n \nsapply() \nworks the same way as lapply() but attempts to simplify the result into a vector is possible \n \nunlist() \nthe unlist() function reduces a list into a vector. \nunlist(studyhoursnightavg ) \n  day    night  \n890.0 2152.5 \n \ndo.call() \nsay we want to use lapply() to derive the ranges. \nstudyhoursrange <- lapply(studyhoursdaynight, range) \nunlist(studyhoursrange ) \nday1  day2 night1 night2  \n750   1030   1940   2330 \nnow we end up with a meaningless vector. a better approach would be to use do.call(). \ndo.call(rbind, studyhoursrange) \n           [,1] [,2] \nday    750 1030 \nnight 1940 2330 \n \n \n\napply() \nwe can also use apply() using a matrix \nm <- matrix(1:12, nrow=3) \napply(m, 1, sum) \n[1] 22 26 30 \napply(m, 1, mean) \n[1] 2 5 8 11 \n \nanonymous functions \nlapply(studyhoursdaynight, function(x) mean(x, na.rm=true)) \n \nstatistical functions \ndistributions \ndnorm(x), pnrom(q), qnorm(p)   # normal distributions \ndt(x), pt(q), qt(p)   # t distributions \ndf(x), pf(q), qf(p)   # f distributions \ndbinom(x), pbinom(q), qbinom(p) # binomial distributions \ndpois(x), ppois(q), qpois(p)  # poisson distributions \n \nlecture notes \nt = true = 1 \nf = false = 0 \n \norder of types \nlogical → numerical → character \na <- c(8e2, '1e1') # the 8e2 is evaluated first before converting into a string \n[1] \"800\" \"1e1\" \n \nb <- c(1, 2, 3, na) \nlength(b) \n[1] 4 \n \nc <- c(1, 2, 3, null) \nlength(c) \n[1] 3 \nc[-1] \n[1] 2 3 \n \nd <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20) \nd[-1] \n[1] 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \nd[d[-1]] \n[1] 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 na \nd[-length(d)] \n[1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \n \nfruit <- c(\"apple\", \"banana\", \"orange\", \"lemon\", \"grape\") \ngrep(\"a\", fruit) \n[1] 1 2 3 5 \ngrepl(\"a\", fruit) \n[1] true true true false true \n\ngrep(\"^a\", fruit) \n[1] 1 \ngrepl(\"a$\", fruit) \n[1] false true false false \n \noperations \n0^0  # nan \n0/0   # nan \n1/0   # inf \n-1/0  # -inf \nlog(0)  # -inf \nlog(-1)  # nan \ninf - inf  # nan \ninf / inf   # nan \ninf + inf  # inf \ninf * 0   # nan \n1/inf  # 0 \ninf ^ 0  # 1 \n0 ^ inf  # 0 \nnan + 1  # nan \nna == na # na \nna + 1  # na \nna == null # na \ntrue + 1 # 2 \nfalse + 1 # 1 \nna > 1   # na \nsqrt(na) # na \nsqrt(-1)  # nan \n\n"
}